[
    {
        "paper_name": "Proceedings of the 29th International Conference on Computational Linguistics",
        "paper_url": "https://aclanthology.org/2022.coling-1.0/",
        "paper_authors": [
            "Nicoletta Calzolari",
            "Chu-Ren Huang",
            "Hansaem Kim",
            "James Pustejovsky",
            "Leo Wanner",
            "Key-Sun Choi",
            "Pum-Mo Ryu",
            "Hsin-Hsi Chen",
            "Lucia Donatelli",
            "Heng Ji",
            "Sadao Kurohashi",
            "Patrizia Paggio",
            "Nianwen Xue",
            "Seokhwan Kim",
            "Younggyun Hahm",
            "Zhong He",
            "Tony Kyungil Lee",
            "Enrico Santus",
            "Francis Bond",
            "Seung-Hoon Na"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Do Language Models Make Human-like Predictions about the Coreferents of Italian Anaphoric Zero Pronouns?",
        "paper_url": "https://aclanthology.org/2022.coling-1.1/",
        "paper_authors": [
            "James A. Michaelov",
            "Benjamin K. Bergen"
        ],
        "paper_abstract": "Some languages allow arguments to be omitted in certain contexts. Yet human language comprehenders reliably infer the intended referents of these zero pronouns, in part because they construct expectations about which referents are more likely. We ask whether Neural Language Models also extract the same expectations. We test whether 12 contemporary language models display expectations that reflect human behavior when exposed to sentences with zero pronouns from five behavioral experiments conducted in Italian by Carminati (2005). We find that three models - XGLM 2.9B, 4.5B, and 7.5B - capture the human behavior from all the experiments, with others successfully modeling some of the results. This result suggests that human expectations about coreference can be derived from exposure to language, and also indicates features of language models that allow them to better reflect human behavior.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Language Acquisition through Intention Reading and Pattern Finding",
        "paper_url": "https://aclanthology.org/2022.coling-1.2/",
        "paper_authors": [
            "Jens Nevens",
            "Jonas Doumen",
            "Paul Van Eecke",
            "Katrien Beuls"
        ],
        "paper_abstract": "One of AI\u2019s grand challenges consists in the development of autonomous agents with communication systems offering the robustness, flexibility and adaptivity found in human languages. While the processes through which children acquire language are by now relatively well understood, a faithful computational operationalisation of the underlying mechanisms is still lacking. Two main cognitive processes are involved in child language acquisition. First, children need to reconstruct the intended meaning of observed utterances, a process called intention reading. Then, they can gradually abstract away from concrete utterances in a process called pattern finding and acquire productive schemata that generalise over form and meaning. In this paper, we introduce a mechanistic model of the intention reading process and its integration with pattern finding capacities. Concretely, we present an agent-based simulation in which an agent learns a grammar that enables them to ask and answer questions about a scene. This involves the reconstruction of queries that correspond to observed questions based on the answer and scene alone, and the generalization of linguistic schemata based on these reconstructed question-query pairs. The result is a productive grammar which can be used to map between natural language questions and queries without ever having observed the queries.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Stability of Syntactic Dialect Classification over Space and Time",
        "paper_url": "https://aclanthology.org/2022.coling-1.3/",
        "paper_authors": [
            "Jonathan Dunn",
            "Sidney Wong"
        ],
        "paper_abstract": "This paper analyses the degree to which dialect classifiers based on syntactic representations remain stable over space and time. While previous work has shown that the combination of grammar induction and geospatial text classification produces robust dialect models, we do not know what influence both changing grammars and changing populations have on dialect models. This paper constructs a test set for 12 dialects of English that spans three years at monthly intervals with a fixed spatial distribution across 1,120 cities. Syntactic representations are formulated within the usage-based Construction Grammar paradigm (CxG). The decay rate of classification performance for each dialect over time allows us to identify regions undergoing syntactic change. And the distribution of classification accuracy within dialect regions allows us to identify the degree to which the grammar of a dialect is internally heterogeneous. The main contribution of this paper is to show that a rigorous evaluation of dialect classification models can be used to find both variation over space and change over time.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. BERT",
        "paper_url": "https://aclanthology.org/2022.coling-1.4/",
        "paper_authors": [
            "Karim Lasri",
            "Olga Seminck",
            "Alessandro Lenci",
            "Thierry Poibeau"
        ],
        "paper_abstract": "Both humans and neural language models are able to perform subject verb number agreement (SVA). In principle, semantics shouldn\u2019t interfere with this task, which only requires syntactic knowledge. In this work we test whether meaning interferes with this type of agreement in English in syntactic structures of various complexities. To do so, we generate both semantically well-formed and nonsensical items. We compare the performance of BERT-base to that of humans, obtained with a psycholinguistic online crowdsourcing experiment. We find that BERT and humans are both sensitive to our semantic manipulation: They fail more often when presented with nonsensical items, especially when their syntactic structure features an attractor (a noun phrase between the subject and the verb that has not the same number as the subject). We also find that the effect of meaningfulness on SVA errors is stronger for BERT than for humans, showing higher lexical sensitivity of the former on this task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Measuring Morphological Fusion Using Partial Information Decomposition",
        "paper_url": "https://aclanthology.org/2022.coling-1.5/",
        "paper_authors": [
            "Michaela Socolof",
            "Jacob Louis Hoover",
            "Richard Futrell",
            "Alessandro Sordoni",
            "Timothy J. O\u2019Donnell"
        ],
        "paper_abstract": "Morphological systems across languages vary when it comes to the relation between form and meaning. In some languages, a single meaning feature corresponds to a single morpheme, whereas in other languages, multiple meaning features are bundled together into one morpheme. The two types of languages have been called agglutinative and fusional, respectively, but this distinction does not capture the graded nature of the phenomenon. We provide a mathematically precise way of characterizing morphological systems using partial information decomposition, a framework for decomposing mutual information into three components: unique, redundant, and synergistic information. We show that highly fusional languages are characterized by high levels of synergy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Smells like Teen Spirit: An Exploration of Sensorial Style in Literary Genres",
        "paper_url": "https://aclanthology.org/2022.coling-1.6/",
        "paper_authors": [
            "Osama Khalid",
            "Padmini Srinivasan"
        ],
        "paper_abstract": "It is well recognized that sensory perceptions and language have interconnections through numerous studies in psychology, neuroscience, and sensorial linguistics. Set in this rich context we ask whether the use of sensorial language in writings is part of linguistic style? This question is important from the view of stylometrics research where a rich set of language features have been explored, but with insufficient attention given to features related to sensorial language. Taking this as the goal we explore several angles about sensorial language and style in collections of lyrics, novels, and poetry. We find, for example, that individual use of sensorial language is not a random phenomenon; choice is likely involved. Also, sensorial style is generally stable over time - the shifts are extremely small. Moreover, style can be extracted from just a few hundred sentences that have sensorial terms. We also identify representative and distinctive features within each genre. For example, we observe that 4 of the top 6 representative features in novels collection involved individuals using olfactory language where we expected them to use non-olfactory language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Metaphorical Polysemy Detection: Conventional Metaphor Meets Word Sense Disambiguation",
        "paper_url": "https://aclanthology.org/2022.coling-1.7/",
        "paper_authors": [
            "Rowan Hall Maudslay",
            "Simone Teufel"
        ],
        "paper_abstract": "Linguists distinguish between novel and conventional metaphor, a distinction which the metaphor detection task in NLP does not take into account. Instead, metaphoricity is formulated as a property of a token in a sentence, regardless of metaphor type. In this paper, we investigate the limitations of treating conventional metaphors in this way, and advocate for an alternative which we name \u2018metaphorical polysemy detection\u2019 (MPD). In MPD, only conventional metaphoricity is treated, and it is formulated as a property of word senses in a lexicon. We develop the first MPD model, which learns to identify conventional metaphors in the English WordNet. To train it, we present a novel training procedure that combines metaphor detection with \u2018word sense disambiguation\u2019 (WSD). For evaluation, we manually annotate metaphor in two subsets of WordNet. Our model significantly outperforms a strong baseline based on a state-of-the-art metaphor detection model, attaining an ROC-AUC score of .78 (compared to .65) on one of the sets. Additionally, when paired with a WSD model, our approach outperforms a state-of-the-art metaphor detection model at identifying conventional metaphors in text (.659 F1 compared to .626).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Machine Reading, Fast and Slow: When Do Models \u201cUnderstand\u201d Language?",
        "paper_url": "https://aclanthology.org/2022.coling-1.8/",
        "paper_authors": [
            "Sagnik Ray Choudhury",
            "Anna Rogers",
            "Isabelle Augenstein"
        ],
        "paper_abstract": "Two of the most fundamental issues in Natural Language Understanding (NLU) at present are: (a) how it can established whether deep learning-based models score highly on NLU benchmarks for the \u201dright\u201d reasons; and (b) what those reasons would even be. We investigate the behavior of reading comprehension models with respect to two linguistic \u201dskills\u201d: coreference resolution and comparison. We propose a definition for the reasoning steps expected from a system that would be \u201dreading slowly\u201d, and compare that with the behavior of five models of the BERT family of various sizes, observed through saliency scores and counterfactual explanations. We find that for comparison (but not coreference) the systems based on larger encoders are more likely to rely on the \u201dright\u201d information, but even they struggle with generalization, suggesting that they still learn specific lexical patterns rather than the general principles of comparison.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings",
        "paper_url": "https://aclanthology.org/2022.coling-1.9/",
        "paper_authors": [
            "Sooji Han",
            "Rui Mao",
            "Erik Cambria"
        ],
        "paper_abstract": "Automatic depression detection on Twitter can help individuals privately and conveniently understand their mental health status in the early stages before seeing mental health professionals. Most existing black-box-like deep learning methods for depression detection largely focused on improving classification performance. However, explaining model decisions is imperative in health research because decision-making can often be high-stakes and life-and-death. Reliable automatic diagnosis of mental health problems including depression should be supported by credible explanations justifying models\u2019 predictions. In this work, we propose a novel explainable model for depression detection on Twitter. It comprises a novel encoder combining hierarchical attention mechanisms and feed-forward neural networks. To support psycholinguistic studies, our model leverages metaphorical concept mappings as input. Thus, it not only detects depressed individuals, but also identifies features of such users\u2019 tweets and associated metaphor concept mappings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-view and Cross-view Brain Decoding",
        "paper_url": "https://aclanthology.org/2022.coling-1.10/",
        "paper_authors": [
            "Subba Reddy Oota",
            "Jashn Arora",
            "Manish Gupta",
            "Raju S. Bapi"
        ],
        "paper_abstract": "Can we build multi-view decoders that can decode concepts from brain recordings corresponding to any view (picture, sentence, word cloud) of stimuli? Can we build a system that can use brain recordings to automatically describe what a subject is watching using keywords or sentences? How about a system that can automatically extract important keywords from sentences that a subject is reading? Previous brain decoding efforts have focused only on single view analysis and hence cannot help us build such systems. As a first step toward building such systems, inspired by Natural Language Processing literature on multi-lingual and cross-lingual modeling, we propose two novel brain decoding setups: (1) multi-view decoding (MVD) and (2) cross-view decoding (CVD). In MVD, the goal is to build an MV decoder that can take brain recordings for any view as input and predict the concept. In CVD, the goal is to train a model which takes brain recordings for one view as input and decodes a semantic vector representation of another view. Specifically, we study practically useful CVD tasks like image captioning, image tagging, keyword extraction, and sentence formation. Our extensive experiments lead to MVD models with ~0.68 average pairwise accuracy across view pairs, and also CVD models with ~0.8 average pairwise accuracy across tasks. Analysis of the contribution of different brain networks reveals exciting cognitive insights: (1) Models trained on picture or sentence view of stimuli are better MV decoders than a model trained on word cloud view. (2) Our extensive analysis across 9 broad regions, 11 language sub-regions and 16 visual sub-regions of the brain help us localize, for the first time, the parts of the brain involved in cross-view tasks like image captioning, image tagging, sentence formation and keyword extraction. We make the code publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Visio-Linguistic Brain Encoding",
        "paper_url": "https://aclanthology.org/2022.coling-1.11/",
        "paper_authors": [
            "Subba Reddy Oota",
            "Jashn Arora",
            "Vijay Rowtula",
            "Manish Gupta",
            "Raju S. Bapi"
        ],
        "paper_abstract": "Brain encoding aims at reconstructing fMRI brain activity given a stimulus. There exists a plethora of neural encoding models which study brain encoding for single mode stimuli: visual (pretrained CNNs) or text (pretrained language models). Few recent papers have also obtained separate visual and text representation models and performed late-fusion using simple heuristics. However, previous work has failed to explore the co-attentive multi-modal modeling for visual and text reasoning. In this paper, we systematically explore the efficacy of image and multi-modal Transformers for brain encoding. Extensive experiments on two popular datasets, BOLD5000 and Pereira, provide the following insights. (1) We find that VisualBERT, a multi-modal Transformer, significantly outperforms previously proposed single-mode CNNs, image Transformers as well as other previously proposed multi-modal models, thereby establishing new state-of-the-art. (2) The regions such as LPTG, LMTG, LIFG, and STS which have dual functionalities for language and vision, have higher correlation with multi-modal models which reinforces the fact that these models are good at mimicing the human brain behavior. (3) The supremacy of visio-linguistic models raises the question of whether the responses elicited in the visual regions are affected implicitly by linguistic processing even when passively viewing images. Future fMRI tasks can verify this computational insight in an appropriate experimental setting. We make our code publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Gestures Are Used Rationally: Information Theoretic Evidence from Neural Sequential Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.12/",
        "paper_authors": [
            "Yang Xu",
            "Yang Cheng",
            "Riya Bhatia"
        ],
        "paper_abstract": "Verbal communication is companied by rich non-verbal signals. The usage of gestures, poses, and facial expressions facilitates the information transmission in verbal channel. However, few computational studies have explored the non-verbal channels with finer theoretical lens. We extract gesture representations from monologue video data and train neural sequential models, in order to study the degree to which non-verbal signals can effectively transmit information. We focus on examining whether the gestures demonstrate the similar pattern of entropy rate constancy (ERC) found in words, as predicted by Information Theory. Positive results are shown to support the assumption, which leads to the conclusion that speakers indeed use simple gestures to convey information that enhances verbal communication, and the production of non-verbal information is rationally organized.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Revisiting Statistical Laws of Semantic Shift in Romance Cognates",
        "paper_url": "https://aclanthology.org/2022.coling-1.13/",
        "paper_authors": [
            "Yoshifumi Kawasaki",
            "Ma\u00eblys Salingre",
            "Marzena Karpinska",
            "Hiroya Takamura",
            "Ryo Nagata"
        ],
        "paper_abstract": "This article revisits statistical relationships across Romance cognates between lexical semantic shift and six intra-linguistic variables, such as frequency and polysemy. Cognates are words that are derived from a common etymon, in this case, a Latin ancestor. Despite their shared etymology, some cognate pairs have experienced semantic shift. The degree of semantic shift is quantified using cosine distance between the cognates\u2019 corresponding word embeddings. In the previous literature, frequency and polysemy have been reported to be correlated with semantic shift; however, the understanding of their effects needs revision because of various methodological defects. In the present study, we perform regression analysis under improved experimental conditions, and demonstrate a genuine negative effect of frequency and positive effect of polysemy on semantic shift. Furthermore, we reveal that morphologically complex etyma are more resistant to semantic shift and that the cognates that have been in use over a longer timespan are prone to greater shift in meaning. These findings add to our understanding of the historical process of semantic change.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Character Jacobian: Modeling Chinese Character Meanings with Deep Learning Model",
        "paper_url": "https://aclanthology.org/2022.coling-1.14/",
        "paper_authors": [
            "Yu-Hsiang Tseng",
            "Shu-Kai Hsieh"
        ],
        "paper_abstract": "Compounding, a prevalent word-formation process, presents an interesting challenge for computational models. Indeed, the relations between compounds and their constituents are often complicated. It is particularly so in Chinese morphology, where each character is almost simultaneously bound and free when treated as a morpheme. To model such word-formation process, we propose the Notch (NOnlinear Transformation of CHaracter embeddings) model and the character Jacobians. The Notch model first learns the non-linear relations between the constituents and words, and the character Jacobians further describes the character\u2019s role in each word. In a series of experiments, we show that the Notch model predicts the embeddings of the real words from their constituents but helps account for the behavioral data of the pseudowords. Moreover, we also demonstrated that character Jacobians reflect the characters\u2019 meanings. Taken together, the Notch model and character Jacobians may provide a new perspective on studying the word-formation process and morphology with modern deep learning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "COMMA: Modeling Relationship among Motivations, Emotions and Actions in Language-based Human Activities",
        "paper_url": "https://aclanthology.org/2022.coling-1.15/",
        "paper_authors": [
            "Yuqiang Xie",
            "Yue Hu",
            "Wei Peng",
            "Guanqun Bi",
            "Luxi Xing"
        ],
        "paper_abstract": "Motivations, emotions, and actions are inter-related essential factors in human activities. While motivations and emotions have long been considered at the core of exploring how people take actions in human activities, there has been relatively little research supporting analyzing the relationship between human mental states and actions. We present the first study that investigates the viability of modeling motivations, emotions, and actions in language-based human activities, named COMMA (Cognitive Framework of Human Activities). Guided by COMMA, we define three natural language processing tasks (emotion understanding, motivation understanding and conditioned action generation), and build a challenging dataset Hail through automatically extracting samples from Story Commonsense. Experimental results on NLP applications prove the effectiveness of modeling the relationship. Furthermore, our models inspired by COMMA can better reveal the essential relationship among motivations, emotions and actions than existing methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Exploring Semantic Spaces for Detecting Clustering and Switching in Verbal Fluency",
        "paper_url": "https://aclanthology.org/2022.coling-1.16/",
        "paper_authors": [
            "\u00d6zge Alacam",
            "Simeon Sch\u00fcz",
            "Martin Wegrzyn",
            "Johanna Ki\u00dfler",
            "Sina Zarrie\u00df"
        ],
        "paper_abstract": "In this work, we explore the fitness of various word/concept representations in analyzing an experimental verbal fluency dataset providing human responses to 10 different category enumeration tasks. Based on human annotations of so-called clusters and switches between sub-categories in the verbal fluency sequences, we analyze whether lexical semantic knowledge represented in word embedding spaces (GloVe, fastText, ConceptNet, BERT) is suitable for detecting these conceptual clusters and switches within and across different categories. Our results indicate that ConceptNet embeddings, a distributional semantics method enriched with taxonomical relations, outperforms other semantic representations by a large margin. Moreover, category-specific analysis suggests that individual thresholds per category are more suited for the analysis of clustering and switching in particular embedding sub-space instead of a one-fits-all cross-category solution. The results point to interesting directions for future work on probing word embedding models on the verbal fluency task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neuro-Symbolic Visual Dialog",
        "paper_url": "https://aclanthology.org/2022.coling-1.17/",
        "paper_authors": [
            "Adnen Abdessaied",
            "Mihai B\u00e2ce",
            "Andreas Bulling"
        ],
        "paper_abstract": "We propose Neuro-Symbolic Visual Dialog (NSVD) \u2014the first method to combine deep learning and symbolic program execution for multi-round visually-grounded reasoning. NSVD significantly outperforms existing purely-connectionist methods on two key challenges inherent to visual dialog: long-distance co-reference resolution as well as vanishing question-answering performance. We demonstrate the latter by proposing a more realistic and stricter evaluation scheme in which we use predicted answers for the full dialog history when calculating accuracy. We describe two variants of our model and show that using this new scheme, our best model achieves an accuracy of 99.72% on CLEVR-Dialog\u2014a relative improvement of more than 10% over the state of the art\u2014while only requiring a fraction of training data. Moreover, we demonstrate that our neuro-symbolic models have a higher mean first failure round, are more robust against incomplete dialog histories, and generalise better not only to dialogs that are up to three times longer than those seen during training but also to unseen question types and scenes.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging",
        "paper_url": "https://aclanthology.org/2022.coling-1.18/",
        "paper_authors": [
            "Andy Rosenbaum",
            "Saleh Soltan",
            "Wael Hamza",
            "Yannick Versley",
            "Markus Boese"
        ],
        "paper_abstract": "We present LINGUIST, a method for generating annotated data for Intent Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and Example Extrapolation) by a wide margin, showing absolute improvement for the target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST out-performs a strong baseline of Machine Translation with Slot Alignment by +4.14 points absolute on ST F1 Score across 6 languages, while matching performance on IC. Finally, we verify our results on an internal large-scale multilingual dataset for conversational agent IC+ST and show significant improvements over a baseline which uses Back-Translation, Paraphrasing and Slot Catalog Resampling. To our knowledge, we are the first to demonstrate instruction fine-tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptive Natural Language Generation for Task-oriented Dialogue via Reinforcement Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.19/",
        "paper_authors": [
            "Atsumoto Ohashi",
            "Ryuichiro Higashinaka"
        ],
        "paper_abstract": "When a natural language generation (NLG) component is implemented in a real-world task-oriented dialogue system, it is necessary to generate not only natural utterances as learned on training data but also utterances adapted to the dialogue environment (e.g., noise from environmental sounds) and the user (e.g., users with low levels of understanding ability). Inspired by recent advances in reinforcement learning (RL) for language generation tasks, we propose ANTOR, a method for Adaptive Natural language generation for Task-Oriented dialogue via Reinforcement learning. In ANTOR, a natural language understanding (NLU) module, which corresponds to the user\u2019s understanding of system utterances, is incorporated into the objective function of RL. If the NLG\u2019s intentions are correctly conveyed to the NLU, which understands a system\u2019s utterances, the NLG is given a positive reward. We conducted experiments on the MultiWOZ dataset, and we confirmed that ANTOR could generate adaptive utterances against speech recognition errors and the different vocabulary levels of users.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TAKE: Topic-shift Aware Knowledge sElection for Dialogue Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.20/",
        "paper_authors": [
            "Chenxu Yang",
            "Zheng Lin",
            "Jiangnan Li",
            "Fandong Meng",
            "Weiping Wang",
            "Lanrui Wang",
            "Jie Zhou"
        ],
        "paper_abstract": "Knowledge-grounded dialogue generation consists of two subtasks: knowledge selection and response generation. The knowledge selector generally constructs a query based on the dialogue context and selects the most appropriate knowledge to help response generation. Recent work finds that realizing who (the user or the agent) holds the initiative and utilizing the role-initiative information to instruct the query construction can help select knowledge. It depends on whether the knowledge connection between two adjacent rounds is smooth to assign the role. However, whereby the user takes the initiative only when there is a strong semantic transition between two rounds, probably leading to initiative misjudgment. Therefore, it is necessary to seek a more sensitive reason beyond the initiative role for knowledge selection. To address the above problem, we propose a Topic-shift Aware Knowledge sElector(TAKE). Specifically, we first annotate the topic shift and topic inheritance labels in multi-round dialogues with distant supervision. Then, we alleviate the noise problem in pseudo labels through curriculum learning and knowledge distillation. Extensive experiments on WoW show that TAKE performs better than strong baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dynamic Dialogue Policy for Continual Reinforcement Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.21/",
        "paper_authors": [
            "Christian Geishauser",
            "Carel van Niekerk",
            "Hsien-chin Lin",
            "Nurul Lubis",
            "Michael Heck",
            "Shutong Feng",
            "Milica Ga\u0161i\u0107"
        ],
        "paper_abstract": "Continual learning is one of the key components of human learning and a necessary requirement of artificial intelligence. As dialogue can potentially span infinitely many topics and tasks, a task-oriented dialogue system must have the capability to continually learn, dynamically adapting to new challenges while preserving the knowledge it already acquired. Despite the importance, continual reinforcement learning of the dialogue policy has remained largely unaddressed. The lack of a framework with training protocols, baseline models and suitable metrics, has so far hindered research in this direction. In this work we fill precisely this gap, enabling research in dialogue policy optimisation to go from static to dynamic learning. We provide a continual learning algorithm, baseline architectures and metrics for assessing continual learning models. Moreover, we propose the dynamic dialogue policy transformer (DDPT), a novel dynamic architecture that can integrate new knowledge seamlessly, is capable of handling large state spaces and obtains significant zero-shot performance when being exposed to unseen domains, without any growth in network parameter size. We validate the strengths of DDPT in simulation with two user simulators as well as with humans.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GRAVL-BERT: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution",
        "paper_url": "https://aclanthology.org/2022.coling-1.22/",
        "paper_authors": [
            "Danfeng Guo",
            "Arpit Gupta",
            "Sanchit Agarwal",
            "Jiun-Yu Kao",
            "Shuyang Gao",
            "Arijit Biswas",
            "Chien-Wei Lin",
            "Tagyoung Chung",
            "Mohit Bansal"
        ],
        "paper_abstract": "Learning from multimodal data has become a popular research topic in recent years. Multimodal coreference resolution (MCR) is an important task in this area. MCR involves resolving the references across different modalities, e.g., text and images, which is a crucial capability for building next-generation conversational agents. MCR is challenging as it requires encoding information from different modalities and modeling associations between them. Although significant progress has been made for visual-linguistic tasks such as visual grounding, most of the current works involve single turn utterances and focus on simple coreference resolutions. In this work, we propose an MCR model that resolves coreferences made in multi-turn dialogues with scene images. We present GRAVL-BERT, a unified MCR framework which combines visual relationships between objects, background scenes, dialogue, and metadata by integrating Graph Neural Networks with VL-BERT. We present results on the SIMMC 2.0 multimodal conversational dataset, achieving the rank-1 on the DSTC-10 SIMMC 2.0 MCR challenge with F1 score 0.783. Our code is available at https://github.com/alexa/gravl-bert.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Improve Persona Consistency in Multi-party Dialogue Generation via Text Knowledge Enhancement",
        "paper_url": "https://aclanthology.org/2022.coling-1.23/",
        "paper_authors": [
            "Dongshi Ju",
            "Shi Feng",
            "Pengcheng Lv",
            "Daling Wang",
            "Yifei Zhang"
        ],
        "paper_abstract": "In an open-domain dialogue system, the consistent persona is a key factor to generate real and coherent dialogues. Existing methods suffer from the incomprehensive persona tags that have unique and obscure meanings to describe human\u2019s personality. Besides, the addressee information, which is closely related to express personality in multi-party dialogues, has been neglected. In this paper, we construct a multi-party personalized dialogue dataset and propose a graph convolution network model (PersonaTKG) with addressee selecting mechanism that integrates personas, dialogue utterances, and external text knowledge in a unified graph. Extensive experiments have shown that PersonaTKG outperforms the baselines by large margins and effectively improves persona consistency in the generated responses.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via Intent Conditioning",
        "paper_url": "https://aclanthology.org/2022.coling-1.24/",
        "paper_authors": [
            "Geunseob Oh",
            "Rahul Goel",
            "Chris Hidey",
            "Shachi Paul",
            "Aditya Gupta",
            "Pararth Shah",
            "Rushin Shah"
        ],
        "paper_abstract": "Semantic parsing (SP) is a core component of modern virtual assistants like Google Assistant and Amazon Alexa. While sequence-to-sequence based auto-regressive (AR) approaches are common for conversational SP, recent studies employ non-autoregressive (NAR) decoders and reduce inference latency while maintaining competitive parsing quality. However, a major drawback of NAR decoders is the difficulty of generating top-k (i.e., k-best) outputs with approaches such as beam search. To address this challenge, we propose a novel NAR semantic parser that introduces intent conditioning on the decoder. Inspired by the traditional intent and slot tagging parsers, we decouple the top-level intent prediction from the rest of a parse. As the top-level intent largely governs the syntax and semantics of a parse, the intent conditioning allows the model to better control beam search and improves the quality and diversity of top-k outputs. We introduce a hybrid teacher-forcing approach to avoid training and inference mismatch. We evaluate the proposed NAR on conversational SP datasets, TOP & TOPv2. Like the existing NAR models, we maintain the O(1) decoding time complexity while generating more diverse outputs and improving top-3 exact match (EM) by 2.4 points. In comparison with AR models, our model speeds up beam search inference by 6.7 times on CPU with competitive top-k EM.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Autoregressive Entity Generation for End-to-End Task-Oriented Dialog",
        "paper_url": "https://aclanthology.org/2022.coling-1.25/",
        "paper_authors": [
            "Guanhuan Huang",
            "Xiaojun Quan",
            "Qifan Wang"
        ],
        "paper_abstract": "Task-oriented dialog (TOD) systems are often required to interact with an external knowledge base (KB) to retrieve necessary entity (e.g., restaurants) information to support their response generation. Most current end-to-end TOD systems either retrieve the KB information explicitly or embed it into model parameters for implicit access. While the first approach demands scanning the KB at each turn of response generation, which is inefficient when the KB scales up, the second approach shows higher flexibility and efficiency. In either approach, the response shall contain attributes of the same entity, however the systems may generate a response with conflicting entities. To address this, we propose to generate the entity autoregressively before leveraging it to guide the response generation in an end-to-end system. To ensure entity consistency, we impose a trie constraint on the decoding of an entity. We also introduce a logit concatenation strategy to facilitate gradient backpropagation for end-to-end training. Experiments on MultiWOZ 2.1 single and CAMREST show that our system can generate more high-quality and entity-consistent responses in an end-to-end manner.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Continual Few-shot Intent Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.26/",
        "paper_authors": [
            "Guodun Li",
            "Yuchen Zhai",
            "Qianglong Chen",
            "Xing Gao",
            "Ji Zhang",
            "Yin Zhang"
        ],
        "paper_abstract": "Intent detection is at the core of task-oriented dialogue systems. Existing intent detection systems are typically trained with a large amount of data over a predefined set of intent classes. However, newly emerged intents in multiple domains are commonplace in the real world. And it is time-consuming and impractical for dialogue systems to re-collect enough annotated data and re-train the model. These limitations call for an intent detection system that could continually recognize new intents with very few labeled examples. In this work, we study the Continual Few-shot Intent Detection (CFID) problem and construct a benchmark consisting of nine tasks with multiple domains and imbalanced classes. To address the key challenges of (a) catastrophic forgetting during continuous learning and (b) negative knowledge transfer across tasks, we propose the Prefix-guided Lightweight Encoder (PLE) with three auxiliary strategies, namely Pseudo Samples Replay (PSR), Teacher Knowledge Transfer (TKT) and Dynamic Weighting Replay (DWR). Extensive experiments demonstrate the effectiveness and efficiency of our method in preventing catastrophic forgetting and encouraging positive knowledge transfer across tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "\u201cMama Always Had a Way of Explaining Things So I Could Understand\u201d: A Dialogue Corpus for Learning to Construct Explanations",
        "paper_url": "https://aclanthology.org/2022.coling-1.27/",
        "paper_authors": [
            "Henning Wachsmuth",
            "Milad Alshomary"
        ],
        "paper_abstract": "As AI is more and more pervasive in everyday life, humans have an increasing demand to understand its behavior and decisions. Most research on explainable AI builds on the premise that there is one ideal explanation to be found. In fact, however, everyday explanations are co-constructed in a dialogue between the person explaining (the explainer) and the specific person being explained to (the explainee). In this paper, we introduce a first corpus of dialogical explanations to enable NLP research on how humans explain as well as on how AI can learn to imitate this process. The corpus consists of 65 transcribed English dialogues from the Wired video series 5 Levels, explaining 13 topics to five explainees of different proficiency. All 1550 dialogue turns have been manually labeled by five independent professionals for the topic discussed as well as for the dialogue act and the explanation move performed. We analyze linguistic patterns of explainers and explainees, and we explore differences across proficiency levels. BERT-based baseline results indicate that sequence information helps predicting topics, acts, and moves effectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Schema Encoding for Transferable Dialogue State Tracking",
        "paper_url": "https://aclanthology.org/2022.coling-1.28/",
        "paper_authors": [
            "Hyunmin Jeon",
            "Gary Geunbae Lee"
        ],
        "paper_abstract": "Dialogue state tracking (DST) is an essential sub-task for task-oriented dialogue systems. Recent work has focused on deep neural models for DST. However, the neural models require a large dataset for training. Furthermore, applying them to another domain needs a new dataset because the neural models are generally trained to imitate the given dataset. In this paper, we propose Schema Encoding for Transferable Dialogue State Tracking (SET-DST), which is a neural DST method for effective transfer to new domains. Transferable DST could assist developments of dialogue systems even with few dataset on target domains. We use a schema encoder not just to imitate the dataset but to comprehend the schema of the dataset. We aim to transfer the model to new domains by encoding new schemas and using them for DST on multi-domain settings. As a result, SET-DST improved the joint accuracy by 1.46 points on MultiWOZ 2.1.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Personalized Dialogue Generator with Implicit User Persona Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.29/",
        "paper_authors": [
            "Itsugun Cho",
            "Dongyang Wang",
            "Ryota Takahashi",
            "Hiroaki Saito"
        ],
        "paper_abstract": "Current works in the generation of personalized dialogue primarily contribute to the agent presenting a consistent personality and driving a more informative response. However, we found that the generated responses from most previous models tend to be self-centered, with little care for the user in the dialogue. Moreover, we consider that human-like conversation is essentially built based on inferring information about the persona of the other party. Motivated by this, we propose a novel personalized dialogue generator by detecting an implicit user persona. Because it is hard to collect a large number of detailed personas for each user, we attempted to model the user\u2019s potential persona and its representation from dialogue history, with no external knowledge. The perception and fader variables were conceived using conditional variational inference. The two latent variables simulate the process of people being aware of each other\u2019s persona and producing a corresponding expression in conversation. Finally, posterior-discriminated regularization was presented to enhance the training procedure. Empirical studies demonstrate that, compared to state-of-the-art methods, our approach is more concerned with the user\u2019s persona and achieves a considerable boost across both automatic metrics and human evaluations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Incorporating Causal Analysis into Diversified and Logical Response Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.30/",
        "paper_authors": [
            "Jiayi Liu",
            "Wei Wei",
            "Zhixuan Chu",
            "Xing Gao",
            "Ji Zhang",
            "Tan Yan",
            "Yulin Kang"
        ],
        "paper_abstract": "Although the Conditional Variational Auto-Encoder (CVAE) model can generate more diversified responses than the traditional Seq2Seq model, the responses often have low relevance with the input words or are illogical with the question. A causal analysis is carried out to study the reasons behind, and a methodology of searching for the mediators and mitigating the confounding bias in dialogues is provided. Specifically, we propose to predict the mediators to preserve relevant information and auto-regressively incorporate the mediators into generating process. Besides, a dynamic topic graph guided conditional variational auto-encoder (TGG-CVAE) model is utilized to complement the semantic space and reduce the confounding bias in responses. Extensive experiments demonstrate that the proposed model is able to generate both relevant and informative responses, and outperforms the state-of-the-art in terms of automatic metrics and human evaluations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Reciprocal Learning of Knowledge Retriever and Response Ranker for Knowledge-Grounded Conversations",
        "paper_url": "https://aclanthology.org/2022.coling-1.31/",
        "paper_authors": [
            "Jiazhan Feng",
            "Chongyang Tao",
            "Zhen Li",
            "Chang Liu",
            "Tao Shen",
            "Dongyan Zhao"
        ],
        "paper_abstract": "Grounding dialogue agents with knowledge documents has sparked increased attention in both academia and industry. Recently, a growing body of work is trying to build retrieval-based knowledge-grounded dialogue systems. While promising, these approaches require collecting pairs of dialogue context and the corresponding ground-truth knowledge sentences that contain the information regarding the dialogue context. Unfortunately, hand-labeling data to that end is time-consuming, and many datasets and applications lack such knowledge annotations. In this paper, we propose a reciprocal learning approach to jointly optimize a knowledge retriever and a response ranker for knowledge-grounded response retrieval without ground-truth knowledge labels. Specifically, the knowledge retriever uses the feedback from the response ranker as pseudo supervised signals of knowledge retrieval for updating its parameters, while the response ranker also receives the top-ranked knowledge sentences from knowledge retriever for optimization. Evaluation results on two public benchmarks show that our model can significantly outperform previous state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CR-GIS: Improving Conversational Recommendation via Goal-aware Interest Sequence Modeling",
        "paper_url": "https://aclanthology.org/2022.coling-1.32/",
        "paper_authors": [
            "Jinfeng Zhou",
            "Bo Wang",
            "Zhitong Yang",
            "Dongming Zhao",
            "Kun Huang",
            "Ruifang He",
            "Yuexian Hou"
        ],
        "paper_abstract": "Conversational recommendation systems (CRS) aim to determine a goal item by sequentially tracking users\u2019 interests through multi-turn conversation. In CRS, implicit patterns of user interest sequence guide the smooth transition of dialog utterances to the goal item. However, with the convenient explicit knowledge of knowledge graph (KG), existing KG-based CRS methods over-rely on the explicit separate KG links to model the user interests but ignore the rich goal-aware implicit interest sequence patterns in a dialog. In addition, interest sequence is also not fully used to generate smooth transited utterances. We propose CR-GIS with a parallel star framework. First, an interest-level star graph is designed to model the goal-aware implicit user interest sequence. Second, a hierarchical Star Transformer is designed to guide the multi-turn utterances generation with the interest-level star graph. Extensive experiments verify the effectiveness of CR-GIS in achieving more accurate recommended items with more fluent and coherent dialog utterances.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GRASP: Guiding Model with RelAtional Semantics Using Prompt for Dialogue Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.33/",
        "paper_authors": [
            "Junyoung Son",
            "Jinsung Kim",
            "Jungwoo Lim",
            "Heuiseok Lim"
        ],
        "paper_abstract": "The dialogue-based relation extraction (DialogRE) task aims to predict the relations between argument pairs that appear in dialogue. Most previous studies utilize fine-tuning pre-trained language models (PLMs) only with extensive features to supplement the low information density of the dialogue by multiple speakers. To effectively exploit inherent knowledge of PLMs without extra layers and consider scattered semantic cues on the relation between the arguments, we propose a Guiding model with RelAtional Semantics using Prompt (GRASP). We adopt a prompt-based fine-tuning approach and capture relational semantic clues of a given dialogue with 1) an argument-aware prompt marker strategy and 2) the relational clue detection task. In the experiments, GRASP achieves state-of-the-art performance in terms of both F1 and F1c scores on a DialogRE dataset even though our method only leverages PLMs without adding any extra layers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PEPDS: A Polite and Empathetic Persuasive Dialogue System for Charity Donation",
        "paper_url": "https://aclanthology.org/2022.coling-1.34/",
        "paper_authors": [
            "Kshitij Mishra",
            "Azlaan Mustafa Samad",
            "Palak Totala",
            "Asif Ekbal"
        ],
        "paper_abstract": "Persuasive conversations for a social cause often require influencing other person\u2019s attitude or intention that may fail even with compelling arguments. The use of emotions and different types of polite tones as needed with facts may enhance the persuasiveness of a message. To incorporate these two aspects, we propose a polite, empathetic persuasive dialogue system (PEPDS). First, in a Reinforcement Learning setting, a Maximum Likelihood Estimation loss based model is fine-tuned by designing an efficient reward function consisting of five different sub rewards viz. Persuasion, Emotion, Politeness-Strategy Consistency, Dialogue-Coherence and Non-repetitiveness. Then, to generate empathetic utterances for non-empathetic ones, an Empathetic transfer model is built upon the RL fine-tuned model. Due to the unavailability of an appropriate dataset, by utilizing the PERSUASIONFORGOOD dataset, we create two datasets, viz. EPP4G and ETP4G. EPP4G is used to train three transformer-based classification models as per persuasiveness, emotion and politeness strategy to achieve respective reward feedbacks. The ETP4G dataset is used to train an empathetic transfer model. Our experimental results demonstrate that PEPDS increases the rate of persuasive responses with emotion and politeness acknowledgement compared to the current state-of-the-art dialogue models, while also enhancing the dialogue\u2019s engagement and maintaining the linguistic quality.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling",
        "paper_url": "https://aclanthology.org/2022.coling-1.35/",
        "paper_authors": [
            "Lahari Poddar",
            "Peiyao Wang",
            "Julia Reinspach"
        ],
        "paper_abstract": "Retrieval-based conversational systems learn to rank response candidates for a given dialogue context by computing the similarity between their vector representations. However, training on a single textual form of the multi-turn context limits the ability of a model to learn representations that generalize to natural perturbations seen during inference. In this paper we propose a framework that incorporates augmented versions of a dialogue context into the learning objective. We utilize contrastive learning as an auxiliary objective to learn robust dialogue context representations that are invariant to perturbations injected through the augmentation method. We experiment with four benchmark dialogue datasets and demonstrate that our framework combines well with existing augmentation methods and can significantly improve over baseline BERT-based ranking architectures. Furthermore, we propose a novel data augmentation method, ConMix, that adds token level perturbations through stochastic mixing of tokens from other contexts in the batch. We show that our proposed augmentation method outperforms previous data augmentation approaches, and provides dialogue representations that are more robust to common perturbations seen during inference.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Closer Look at Few-Shot Out-of-Distribution Intent Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.36/",
        "paper_authors": [
            "Li-Ming Zhan",
            "Haowen Liang",
            "Lu Fan",
            "Xiao-Ming Wu",
            "Albert Y.S. Lam"
        ],
        "paper_abstract": "We consider few-shot out-of-distribution (OOD) intent detection, a practical and important problem for the development of task-oriented dialogue systems. Despite its importance, this problem is seldom studied in the literature, let alone examined in a systematic way. In this work, we take a closer look at this problem and identify key issues for research. In our pilot study, we reveal the reason why existing OOD intent detection methods are not adequate in dealing with this problem. Based on the observation, we propose a promising approach to tackle this problem based on latent representation generation and self-supervision. Comprehensive experiments on three real-world intent detection benchmark datasets demonstrate the high effectiveness of our proposed approach and its great potential in improving state-of-the-art methods for few-shot OOD intent detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CGIM: A Cycle Guided Interactive Learning Model for Consistency Identification in Task-oriented Dialogue",
        "paper_url": "https://aclanthology.org/2022.coling-1.37/",
        "paper_authors": [
            "Libo Qin",
            "Qiguang Chen",
            "Tianbao Xie",
            "Qian Liu",
            "Shijue Huang",
            "Wanxiang Che",
            "Zhou Yu"
        ],
        "paper_abstract": "Consistency identification in task-oriented dialog (CI-ToD) usually consists of three subtasks, aiming to identify inconsistency between current system response and current user response, dialog history and the corresponding knowledge base. This work aims to solve CI-ToD task by introducing an explicit interaction paradigm, Cycle Guided Interactive learning Model (CGIM), which achieves to make information exchange explicitly from all the three tasks. Specifically, CGIM relies on two core insights, referred to as guided multi-head attention module and cycle interactive mechanism, that collaborate from each other. On the one hand, each two tasks are linked with the guided multi-head attention module, aiming to explicitly model the interaction across two related tasks. On the other hand, we further introduce cycle interactive mechanism that focuses on facilitating model to exchange information among the three correlated sub-tasks via a cycle interaction manner. Experimental results on CI-ToD benchmark show that our model achieves the state-of-the-art performance, pushing the overall score to 56.3% (5.0% point absolute improvement). In addition, we find that CGIM is robust to the initial task flow order.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations",
        "paper_url": "https://aclanthology.org/2022.coling-1.38/",
        "paper_authors": [
            "Lin Xu",
            "Qixian Zhou",
            "Jinlan Fu",
            "Min-Yen Kan",
            "See-Kiong Ng"
        ],
        "paper_abstract": "Knowledge-grounded dialog systems need to incorporate smooth transitions among knowledge selected for generating responses, to ensure that dialog flows naturally. For document-grounded dialog systems, the inter- and intra-document knowledge relations can be used to model such conversational flows. We develop a novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture the inter-document relationships based on commonsense and similarity and the intra-document co-referential structures of knowledge segments within the grounding documents. We propose CorefDiffs, a Co-referential and Differential flow management method, to linearize the static Coref-MDG into conversational sequence logic. CorefDiffs performs knowledge selection by accounting for contextual graph structures and the knowledge difference sequences. CorefDiffs significantly outperforms the state-of-the-art by 9.5%, 7.4% and 8.2% on three public benchmarks. This demonstrates that the effective modeling of co-reference and knowledge difference for dialog flows are critical for transitions in document-grounded conversation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation",
        "paper_url": "https://aclanthology.org/2022.coling-1.39/",
        "paper_authors": [
            "Longxuan Ma",
            "Ziyu Zhuang",
            "Weinan Zhang",
            "Mingda Li",
            "Ting Liu"
        ],
        "paper_abstract": "This paper introduces a novel Self-supervised Fine-grained Dialogue Evaluation framework (SelF-Eval). The core idea is to model the correlation between turn quality and the entire dialogue quality. We first propose a novel automatic data construction method that can automatically assign fine-grained scores for arbitrarily dialogue data. Then we train SelF-Eval with a multi-level contrastive learning schema which helps to distinguish different score levels. Experimental results on multiple benchmarks show that SelF-Eval is highly consistent with human evaluations and better than the state-of-the-art models. We give a detailed analysis of the experiments in this paper. Our code is available on GitHub.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Open-Domain Dialog Evaluation Using Follow-Ups Likelihood",
        "paper_url": "https://aclanthology.org/2022.coling-1.40/",
        "paper_authors": [
            "Maxime De Bruyn",
            "Ehsan Lotfi",
            "Jeska Buhmann",
            "Walter Daelemans"
        ],
        "paper_abstract": "Automatic evaluation of open-domain dialogs remains an unsolved problem. Existing methods do not correlate strongly with human annotations. In this paper, we present a new automated evaluation method based on the use of follow-ups. We measure the probability that a language model will continue the conversation with a fixed set of follow-ups (e.g. not really relevant here, what are you trying to say?). When compared against twelve existing methods, our new evaluation achieves the highest correlation with human evaluations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Joint Goal Segmentation and Goal Success Prediction on Multi-Domain Conversations",
        "paper_url": "https://aclanthology.org/2022.coling-1.41/",
        "paper_authors": [
            "Meiguo Wang",
            "Benjamin Yao",
            "Bin Guo",
            "Xiaohu Liu",
            "Yu Zhang",
            "Tuan-Hung Pham",
            "Chenlei Guo"
        ],
        "paper_abstract": "To evaluate the performance of a multi-domain goal-oriented Dialogue System (DS), it is important to understand what the users\u2019 goals are for the conversations and whether those goals are successfully achieved. The success rate of goals directly correlates with user satisfaction and perceived usefulness of the DS. In this paper, we propose a novel automatic dialogue evaluation framework that jointly performs two tasks: goal segmentation and goal success prediction. We extend the RoBERTa-IQ model (Gupta et al., 2021) by adding multi-task learning heads for goal segmentation and success prediction. Using an annotated dataset from a commercial DS, we demonstrate that our proposed model reaches an accuracy that is on-par with single-pass human annotation comparing to a three-pass gold annotation benchmark.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Slot Dependency Modeling for Zero-Shot Cross-Domain Dialogue State Tracking",
        "paper_url": "https://aclanthology.org/2022.coling-1.42/",
        "paper_authors": [
            "Qingyue Wang",
            "Yanan Cao",
            "Piji Li",
            "Yanhe Fu",
            "Zheng Lin",
            "Li Guo"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model",
        "paper_url": "https://aclanthology.org/2022.coling-1.43/",
        "paper_authors": [
            "Sixing Wu",
            "Ying Li",
            "Ping Xue",
            "Dawei Zhang",
            "Zhonghai Wu"
        ],
        "paper_abstract": "In knowledge-grounded dialogue generation, pre-trained language models (PLMs) can be expected to deepen the fusing of dialogue context and knowledge because of their superior ability of semantic understanding. Unlike adopting the plain text knowledge, it is thorny to leverage the structural commonsense knowledge when using PLMs because most PLMs can only operate plain texts. Thus, linearizing commonsense knowledge facts into plan text is a compulsory trick. However, a dialogue is always aligned to a lot of retrieved fact candidates; as a result, the linearized text is always lengthy and then significantly increases the burden of using PLMs. To address this issue, we propose a novel two-stage framework SAKDP. In the first pre-screening stage, we use a ranking network PriorRanking to estimate the relevance of a retrieved knowledge fact. Thus, facts can be clustered into three sections of different priorities. As priority decreases, the relevance decreases, and the number of included facts increases. In the next dialogue generation stage, we use section-aware strategies to encode the linearized knowledge. The powerful but expensive PLM is only used for a few facts in the higher priority sections, reaching the performance-efficiency balance. Both the automatic and human evaluation demonstrate the superior performance of this work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection",
        "paper_url": "https://aclanthology.org/2022.coling-1.44/",
        "paper_authors": [
            "Souvik Das",
            "Sougata Saha",
            "Rohini K. Srihari"
        ],
        "paper_abstract": "Personalized response selection systems are generally grounded on persona. However, a correlation exists between persona and empathy, which these systems do not explore well. Also, when a contradictory or off-topic response is selected, faithfulness to the conversation context plunges. This paper attempts to address these issues by proposing a suite of fusion strategies that capture the interaction between persona, emotion, and entailment information of the utterances. Ablation studies on the Persona-Chat dataset show that incorporating emotion and entailment improves the accuracy of response selection. We combine our fusion strategies and concept-flow encoding to train a BERT-based model which outperforms the previous methods by margins larger than 2.3% on original personas and 1.9% on revised personas in terms of hits@1 (top-1 accuracy), achieving a new state-of-the-art performance on the Persona-Chat dataset",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Multi-Dimensional, Cross-Domain and Hierarchy-Aware Neural Architecture for ISO-Standard Dialogue Act Tagging",
        "paper_url": "https://aclanthology.org/2022.coling-1.45/",
        "paper_authors": [
            "Stefano Mezza",
            "Wayne Wobcke",
            "Alan Blair"
        ],
        "paper_abstract": "Dialogue Act tagging with the ISO 24617-2 standard is a difficult task that involves multi-label text classification across a diverse set of labels covering semantic, syntactic and pragmatic aspects of dialogue. The lack of an adequately sized training set annotated with this standard is a major problem when using the standard in practice. In this work we propose a neural architecture to increase classification accuracy, especially on low-frequency fine-grained tags. Our model takes advantage of the hierarchical structure of the ISO taxonomy and utilises syntactic information in the form of Part-Of-Speech and dependency tags, in addition to contextual information from previous turns. We train our architecture on an aggregated corpus of conversations from different domains, which provides a variety of dialogue interactions and linguistic registers. Our approach achieves state-of-the-art tagging results on the DialogBank benchmark data set, providing empirical evidence that this architecture can successfully generalise to different domains.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding",
        "paper_url": "https://aclanthology.org/2022.coling-1.46/",
        "paper_authors": [
            "Wanwei He",
            "Yinpei Dai",
            "Binyuan Hui",
            "Min Yang",
            "Zheng Cao",
            "Jianbo Dong",
            "Fei Huang",
            "Luo Si",
            "Yongbin Li"
        ],
        "paper_abstract": "Pre-training methods with contrastive learning objectives have shown remarkable success in dialog understanding tasks. However, current contrastive learning solely considers the self-augmented dialog samples as positive samples and treats all other dialog samples as negative ones, which enforces dissimilar representations even for dialogs that are semantically related. In this paper, we propose SPACE-2, a tree-structured pre-trained conversation model, which learns dialog representations from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised contrastive pre-training. Concretely, we first define a general semantic tree structure (STS) to unify the inconsistent annotation schema across different dialog datasets, so that the rich structural information stored in all labeled data can be exploited. Then we propose a novel multi-view score function to increase the relevance of all possible dialogs that share similar STSs and only push away other completely different dialogs during supervised contrastive pre-training. To fully exploit unlabeled dialogs, a basic self-supervised contrastive loss is also added to refine the learned representations. Experiments show that our method can achieve new state-of-the-art results on the DialoGLUE benchmark consisting of seven datasets and four popular dialog understanding tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ET5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension",
        "paper_url": "https://aclanthology.org/2022.coling-1.47/",
        "paper_authors": [
            "Xiao Zhang",
            "Heyan Huang",
            "Zewen Chi",
            "Xian-Ling Mao"
        ],
        "paper_abstract": "Conversational machine reading comprehension (CMRC) aims to assist computers to understand an natural language text and thereafter engage in a multi-turn conversation to answer questions related to the text. Existing methods typically require three steps: (1) decision making based on entailment reasoning; (2) span extraction if required by the above decision; (3) question rephrasing based on the extracted span. However, for nearly all these methods, the span extraction and question rephrasing steps cannot fully exploit the fine-grained entailment reasoning information in decision making step because of their relative independence, which will further enlarge the information gap between decision making and question phrasing. Thus, to tackle this problem, we propose a novel end-to-end framework for conversational machine reading comprehension based on shared parameter mechanism, called entailment reasoning T5 (ET5). Despite the lightweight of our proposed framework, experimental results show that the proposed ET5 achieves new state-of-the-art results on the ShARC leaderboard with the BLEU-4 score of 55.2. Our model and code are publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CoHS-CQG: Context and History Selection for Conversational Question Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.48/",
        "paper_authors": [
            "Xuan Long Do",
            "Bowei Zou",
            "Liangming Pan",
            "Nancy F. Chen",
            "Shafiq Joty",
            "Ai Ti Aw"
        ],
        "paper_abstract": "Conversational question generation (CQG) serves as a vital task for machines to assist humans, such as interactive reading comprehension, through conversations. Compared to traditional single-turn question generation (SQG), CQG is more challenging in the sense that the generated question is required not only to be meaningful, but also to align with the provided conversation. Previous studies mainly focus on how to model the flow and alignment of the conversation, but do not thoroughly study which parts of the context and history are necessary for the model. We believe that shortening the context and history is crucial as it can help the model to optimise more on the conversational alignment property. To this end, we propose CoHS-CQG, a two-stage CQG framework, which adopts a novel CoHS module to shorten the context and history of the input. In particular, it selects the top-p sentences and history turns by calculating the relevance scores of them. Our model achieves state-of-the-art performances on CoQA in both the answer-aware and answer-unaware settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantic-based Pre-training for Dialogue Understanding",
        "paper_url": "https://aclanthology.org/2022.coling-1.49/",
        "paper_authors": [
            "Xuefeng Bai",
            "Linfeng Song",
            "Yue Zhang"
        ],
        "paper_abstract": "Pre-trained language models have made great progress on dialogue tasks. However, these models are typically trained on surface dialogue text, thus are proven to be weak in understanding the main semantic meaning of a dialogue context. We investigate Abstract Meaning Representation (AMR) as explicit semantic knowledge for pre-training models to capture the core semantic information in dialogues during pre-training. In particular, we propose a semantic-based pre-training framework that extends the standard pre-training framework (Devlin et al.,2019) by three tasks for learning 1) core semantic units, 2) semantic relations and 3) the overall semantic representation according to AMR graphs. Experiments on the understanding of both chit-chats and task-oriented dialogues show the superiority of our model. To our knowledge, we are the first to leverage a deep semantic representation for dialogue pre-training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation",
        "paper_url": "https://aclanthology.org/2022.coling-1.50/",
        "paper_authors": [
            "Yanan Wu",
            "Zhiyuan Zeng",
            "Keqing He",
            "Yutao Mou",
            "Pei Wang",
            "Weiran Xu"
        ],
        "paper_abstract": "Out-of-Domain (OOD) detection is a key component in a task-oriented dialog system, which aims to identify whether a query falls outside the predefined supported intent set. Previous softmax-based detection algorithms are proved to be overconfident for OOD samples. In this paper, we analyze overconfident OOD comes from distribution uncertainty due to the mismatch between the training and test distributions, which makes the model can\u2019t confidently make predictions thus probably causes abnormal softmax scores. We propose a Bayesian OOD detection framework to calibrate distribution uncertainty using Monte-Carlo Dropout. Our method is flexible and easily pluggable to existing softmax-based baselines and gains 33.33% OOD F1 improvements with increasing only 0.41% inference time compared to MSP. Further analyses show the effectiveness of Bayesian learning for OOD detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Tracking Satisfaction States for Customer Satisfaction Prediction in E-commerce Service Chatbots",
        "paper_url": "https://aclanthology.org/2022.coling-1.51/",
        "paper_authors": [
            "Yang Sun",
            "Liangqing Wu",
            "Shuangyong Song",
            "Xiaoguang Yu",
            "Xiaodong He",
            "Guohong Fu"
        ],
        "paper_abstract": "Due to the increasing use of service chatbots in E-commerce platforms in recent years, customer satisfaction prediction (CSP) is gaining more and more attention. CSP is dedicated to evaluating subjective customer satisfaction in conversational service and thus helps improve customer service experience. However, previous methods focus on modeling customer-chatbot interaction across different turns, which are hard to represent the important dynamic satisfaction states throughout the customer journey. In this work, we investigate the problem of satisfaction states tracking and its effects on CSP in E-commerce service chatbots. To this end, we propose a dialogue-level classification model named DialogueCSP to track satisfaction states for CSP. In particular, we explore a novel two-step interaction module to represent the dynamic satisfaction states at each turn. In order to capture dialogue-level satisfaction states for CSP, we further introduce dialogue-aware attentions to integrate historical informative cues into the interaction module. To evaluate the proposed approach, we also build a Chinese E-commerce dataset for CSP. Experiment results demonstrate that our model significantly outperforms multiple baselines, illustrating the benefits of satisfaction states tracking on CSP.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Multi-label Unknown Intent Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.52/",
        "paper_authors": [
            "Yawen Ouyang",
            "Zhen Wu",
            "Xinyu Dai",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "paper_abstract": "Multi-class unknown intent detection has made remarkable progress recently. However, it has a strong assumption that each utterance has only one intent, which does not conform to reality because utterances often have multiple intents. In this paper, we propose a more desirable task, multi-label unknown intent detection, to detect whether the utterance contains the unknown intent, in which each utterance may contain multiple intents. In this task, the unique utterances simultaneously containing known and unknown intents make existing multi-class methods easy to fail. To address this issue, we propose an intuitive and effective method to recognize whether All Intents contained in the utterance are Known (AIK). Our high-level idea is to predict the utterance\u2019s intent number, then check whether the utterance contains the same number of known intents. If the number of known intents is less than the number of intents, it implies that the utterance also contains unknown intents. We benchmark AIK over existing methods, and empirical results suggest that our method obtains state-of-the-art performances. For example, on the MultiWOZ 2.3 dataset, AIK significantly reduces the FPR95 by 12.25% compared to the best baseline.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.53/",
        "paper_authors": [
            "Yihe Wang",
            "Yitong Li",
            "Yasheng Wang",
            "Fei Mi",
            "Pingyi Zhou",
            "Xin Wang",
            "Jin Liu",
            "Xin Jiang",
            "Qun Liu"
        ],
        "paper_abstract": "Real human conversation data are complicated, heterogeneous, and noisy, from which building open-domain dialogue systems remains a challenging task. In fact, such dialogue data still contains a wealth of information and knowledge, however, they are not fully explored. In this paper, we show existing open-domain dialogue generation methods that memorize context-response paired data with autoregressive or encode-decode language models underutilize the training data. Different from current approaches, using external knowledge, we explore a retrieval-generation training framework that can take advantage of the heterogeneous and noisy training data by considering them as \u201cevidence\u201d. In particular, we use BERTScore for retrieval, which gives better qualities of the evidence and generation. Experiments over publicly available datasets demonstrate that our method can help models generate better responses, even such training data are usually impressed as low-quality data. Such performance gain is comparable with those improved by enlarging the training set, even better. We also found that the model performance has a positive correlation with the relevance of the retrieved evidence. Moreover, our method performed well on zero-shot experiments, which indicates that our method can be more robust to real-world data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.54/",
        "paper_authors": [
            "Yongkang Liu",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang"
        ],
        "paper_abstract": "Building dialogue generation systems in a zero-shot scenario remains a huge challenge, since the typical zero-shot approaches in dialogue generation rely heavily on large-scale pre-trained language generation models such as GPT-3 and T5. The research on zero-shot dialogue generation without cumbersome language models is limited due to lacking corresponding parallel dialogue corpora. In this paper, we propose a simple but effective Multilingual learning framework for Zero-shot Dialogue Generation (dubbed as MulZDG) that can effectively transfer knowledge from an English corpus with large-scale training samples to a non-English corpus with zero samples. Besides, MulZDG can be viewed as a multilingual data augmentation method to improve the performance of the resource-rich language. First, we construct multilingual code-switching dialogue datasets via translation utterances randomly selected from monolingual English datasets. Then we employ MulZDG to train a unified multilingual dialogue model based on the code-switching datasets. The MulZDG can conduct implicit semantic alignment between different languages. Experiments on DailyDialog and DSTC7 datasets demonstrate that MulZDG not only achieve competitive performance under zero-shot case compared to training with sufficient examples but also greatly improve the performance of the source language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Target-Guided Open-Domain Conversation Planning",
        "paper_url": "https://aclanthology.org/2022.coling-1.55/",
        "paper_authors": [
            "Yosuke Kishinami",
            "Reina Akama",
            "Shiki Sato",
            "Ryoko Tokuhisa",
            "Jun Suzuki",
            "Kentaro Inui"
        ],
        "paper_abstract": "Prior studies addressing target-oriented conversational tasks lack a crucial notion that has been intensively studied in the context of goal-oriented artificial intelligence agents, namely, planning. In this study, we propose the task of Target-Guided Open-Domain Conversation Planning (TGCP) task to evaluate whether neural conversational agents have goal-oriented conversation planning abilities. Using the TGCP task, we investigate the conversation planning abilities of existing retrieval models and recent strong generative models. The experimental results reveal the challenges facing current technology.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.56/",
        "paper_authors": [
            "Young-Jun Lee",
            "Chae-Gyun Lim",
            "Ho-Jin Choi"
        ],
        "paper_abstract": "Since empathy plays a crucial role in increasing social bonding between people, many studies have designed their own dialogue agents to be empathetic using the well-established method of fine-tuning. However, they do not use prompt-based in-context learning, which has shown powerful performance in various natural language processing (NLP) tasks, for empathetic dialogue generation. Although several studies have investigated few-shot in-context learning for empathetic dialogue generation, an in-depth analysis of the generation of empathetic dialogue with in-context learning remains unclear, especially in GPT-3 (Brown et al., 2020). In this study, we explore whether GPT-3 can generate empathetic dialogues through prompt-based in-context learning in both zero-shot and few-shot settings. To enhance performance, we propose two new in-context example selection methods, called SITSM and EMOSITSM, that utilize emotion and situational information. We also introduce a new automatic evaluation method, DIFF-EPITOME, which reflects the human tendency to express empathy. From the analysis, we reveal that our DIFF-EPITOME is effective in measuring the degree of human empathy. We show that GPT-3 achieves competitive performance with Blender 90M, a state-of-the-art dialogue generative model, on both automatic and human evaluation. Our code is available at https://github.com/passing2961/EmpGPT-3.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DialogueEIN: Emotion Interaction Network for Dialogue Affective Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.57/",
        "paper_authors": [
            "Yuchen Liu",
            "Jinming Zhao",
            "Jingwen Hu",
            "Ruichen Li",
            "Qin Jin"
        ],
        "paper_abstract": "Emotion Recognition in Conversation (ERC) has attracted increasing attention in the affective computing research field. Previous works have mainly focused on modeling the semantic interactions in the dialogue and implicitly inferring the evolution of the speakers\u2019 emotional states. Few works have considered the emotional interactions, which directly reflect the emotional evolution of speakers in the dialogue. According to psychological and behavioral studies, the emotional inertia and emotional stimulus are important factors that affect the speaker\u2019s emotional state in conversations. In this work, we propose a novel Dialogue Emotion Interaction Network, DialogueEIN, to explicitly model the intra-speaker, inter-speaker, global and local emotional interactions to respectively simulate the emotional inertia, emotional stimulus, global and local emotional evolution in dialogues. Extensive experiments on four ERC benchmark datasets, IEMOCAP, MELD, EmoryNLP and DailyDialog, show that our proposed DialogueEIN considering emotional interaction factors can achieve superior or competitive performance compared to state-of-the-art methods. Our codes and models are released.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Enhancing Health Coaching Dialogue in Low-Resource Settings",
        "paper_url": "https://aclanthology.org/2022.coling-1.58/",
        "paper_authors": [
            "Yue Zhou",
            "Barbara Di Eugenio",
            "Brian Ziebart",
            "Lisa Sharp",
            "Bing Liu",
            "Ben Gerber",
            "Nikolaos Agadakos",
            "Shweta Yadav"
        ],
        "paper_abstract": "Health coaching helps patients identify and accomplish lifestyle-related goals, effectively improving the control of chronic diseases and mitigating mental health conditions. However, health coaching is cost-prohibitive due to its highly personalized and labor-intensive nature. In this paper, we propose to build a dialogue system that converses with the patients, helps them create and accomplish specific goals, and can address their emotions with empathy. However, building such a system is challenging since real-world health coaching datasets are limited and empathy is subtle. Thus, we propose a modularized health coaching dialogue with simplified NLU and NLG frameworks combined with mechanism-conditioned empathetic response generation. Through automatic and human evaluation, we show that our system generates more empathetic, fluent, and coherent responses and outperforms the state-of-the-art in NLU tasks while requiring less annotation. We view our approach as a key step towards building automated and more accessible health coaching systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generalized Intent Discovery: Learning from Open World Dialogue System",
        "paper_url": "https://aclanthology.org/2022.coling-1.59/",
        "paper_authors": [
            "Yutao Mou",
            "Keqing He",
            "Yanan Wu",
            "Pei Wang",
            "Jingang Wang",
            "Wei Wu",
            "Yi Huang",
            "Junlan Feng",
            "Weiran Xu"
        ],
        "paper_abstract": "Traditional intent classification models are based on a pre-defined intent set and only recognize limited in-domain (IND) intent classes. But users may input out-of-domain (OOD) queries in a practical dialogue system. Such OOD queries can provide directions for future improvement. In this paper, we define a new task, Generalized Intent Discovery (GID), which aims to extend an IND intent classifier to an open-world intent set including IND and OOD intents. We hope to simultaneously classify a set of labeled IND intent classes while discovering and recognizing new unlabeled OOD types incrementally. We construct three public datasets for different application scenarios and propose two kinds of frameworks, pipeline-based and end-to-end for future work. Further, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future GID research.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DialMed: A Dataset for Dialogue-based Medication Recommendation",
        "paper_url": "https://aclanthology.org/2022.coling-1.60/",
        "paper_authors": [
            "Zhenfeng He",
            "Yuqiang Han",
            "Zhenqiu Ouyang",
            "Wei Gao",
            "Hongxu Chen",
            "Guandong Xu",
            "Jian Wu"
        ],
        "paper_abstract": "Medication recommendation is a crucial task for intelligent healthcare systems. Previous studies mainly recommend medications with electronic health records (EHRs). However, some details of interactions between doctors and patients may be ignored or omitted in EHRs, which are essential for automatic medication recommendation. Therefore, we make the first attempt to recommend medications with the conversations between doctors and patients. In this work, we construct DIALMED, the first high-quality dataset for medical dialogue-based medication recommendation task. It contains 11, 996 medical dialogues related to 16 common diseases from 3 departments and 70 corresponding common medications. Furthermore, we propose a Dialogue structure and Disease knowledge aware Network (DDN), where a QA Dialogue Graph mechanism is designed to model the dialogue structure and the knowledge graph is used to introduce external disease knowledge. The extensive experimental results demonstrate that the proposed method is a promising solution to recommend medications with medical dialogues. The dataset and code are available at https://github.com/f-window/DialMed.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Speaker Clustering in Textual Dialogue with Pairwise Utterance Relation and Cross-corpus Dialogue Act Supervision",
        "paper_url": "https://aclanthology.org/2022.coling-1.61/",
        "paper_authors": [
            "Zhihua Su",
            "Qiang Zhou"
        ],
        "paper_abstract": "We propose a speaker clustering model for textual dialogues, which groups the utterances of a multi-party dialogue without speaker annotations, so that the actual speakers are identical inside each cluster. We find that, without knowing the speakers, the interactions between utterances are still implied in the text, which suggest the relations between speakers. In this work, we model the semantic content of utterance with a pre-trained language model, and the relations between speakers with an utterance-level pairwise matrix. The semantic content representation can be further instructed by cross-corpus dialogue act modeling. The speaker labels are finally generated by spectral clustering. Experiments show that our model outperforms the sequence classification baseline, and benefits from the auxiliary dialogue act classification task. We also discuss the detail of determining the number of speakers (clusters), eliminating the interference caused by semantic similarity, and the impact of utterance distance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TopKG: Target-oriented Dialog via Global Planning on Knowledge Graph",
        "paper_url": "https://aclanthology.org/2022.coling-1.62/",
        "paper_authors": [
            "Zhitong Yang",
            "Bo Wang",
            "Jinfeng Zhou",
            "Yue Tan",
            "Dongming Zhao",
            "Kun Huang",
            "Ruifang He",
            "Yuexian Hou"
        ],
        "paper_abstract": "Target-oriented dialog aims to reach a global target through multi-turn conversation. The key to the task is the global planning towards the target, which flexibly guides the dialog concerning the context. However, existing target-oriented dialog works take a local and greedy strategy for response generation, where global planning is absent. In this work, we propose global planning for target-oriented dialog on a commonsense knowledge graph (KG). We design a global reinforcement learning with the planned paths to flexibly adjust the local response generation model towards the global target. We also propose a KG-based method to collect target-oriented samples automatically from the chit-chat corpus for model training. Experiments show that our method can reach the target with a higher success rate, fewer turns, and more coherent responses.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Extractive Summarisation for German-language Data: A Text-level Approach with Discourse Features",
        "paper_url": "https://aclanthology.org/2022.coling-1.63/",
        "paper_authors": [
            "Freya Hewett",
            "Manfred Stede"
        ],
        "paper_abstract": "We examine the link between facets of Rhetorical Structure Theory (RST) and the selection of content for extractive summarisation, for German-language texts. For this purpose, we produce a set of extractive summaries for a dataset of German-language newspaper commentaries, a corpus which already has several layers of annotation. We provide an in-depth analysis of the connection between summary sentences and several RST-based features and transfer these insights to various automated summarisation models. Our results show that RST features are informative for the task of extractive summarisation, particularly nuclearity and relations at sentence-level.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "End-to-End Neural Bridging Resolution",
        "paper_url": "https://aclanthology.org/2022.coling-1.64/",
        "paper_authors": [
            "Hideo Kobayashi",
            "Yufang Hou",
            "Vincent Ng"
        ],
        "paper_abstract": "The state of bridging resolution research is rather unsatisfactory: not only are state-of-the-art resolvers evaluated in unrealistic settings, but the neural models underlying these resolvers are weaker than those used for entity coreference resolution. In light of these problems, we evaluate bridging resolvers in an end-to-end setting, strengthen them with better encoders, and attempt to gain a better understanding of them via perturbation experiments and a manual analysis of their outputs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Investigating the Performance of Transformer-Based NLI Models on Presuppositional Inferences",
        "paper_url": "https://aclanthology.org/2022.coling-1.65/",
        "paper_authors": [
            "Jad Kabbara",
            "Jackie Chi Kit Cheung"
        ],
        "paper_abstract": "Presuppositions are assumptions that are taken for granted by an utterance, and identifying them is key to a pragmatic interpretation of language. In this paper, we investigate the capabilities of transformer models to perform NLI on cases involving presupposition. First, we present simple heuristics to create alternative \u201ccontrastive\u201d test cases based on the ImpPres dataset and investigate the model performance on those test cases. Second, to better understand how the model is making its predictions, we analyze samples from sub-datasets of ImpPres and examine model performance on them. Overall, our findings suggest that NLI-trained transformer models seem to be exploiting specific structural and lexical cues as opposed to performing some kind of pragmatic reasoning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Re-Examining FactBank: Predicting the Author\u2019s Presentation of Factuality",
        "paper_url": "https://aclanthology.org/2022.coling-1.66/",
        "paper_authors": [
            "John Murzaku",
            "Peter Zeng",
            "Magdalena Markowska",
            "Owen Rambow"
        ],
        "paper_abstract": "We present a corrected version of a subset of the FactBank data set. Previously published results on FactBank are no longer valid. We perform experiments on FactBank using multiple training paradigms, data smoothing techniques, and polarity classifiers. We argue that f-measure is an important alternative evaluation metric for factuality. We provide new state-of-the-art results for four corpora including FactBank. We perform an error analysis on Factbank combined with two similar corpora.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The Role of Context and Uncertainty in Shallow Discourse Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.67/",
        "paper_authors": [
            "Katherine Atwell",
            "Remi Choi",
            "Junyi Jessy Li",
            "Malihe Alikhani"
        ],
        "paper_abstract": "Discourse parsing has proven to be useful for a number of NLP tasks that require complex reasoning. However, over a decade since the advent of the Penn Discourse Treebank, predicting implicit discourse relations in text remains challenging. There are several possible reasons for this, and we hypothesize that models should be exposed to more context as it plays an important role in accurate human annotation; meanwhile adding uncertainty measures can improve model accuracy and calibration. To thoroughly investigate this phenomenon, we perform a series of experiments to determine 1) the effects of context on human judgments, and 2) the effect of quantifying uncertainty with annotator confidence ratings on model accuracy and calibration (which we measure using the Brier score (Brier et al, 1950)). We find that including annotator accuracy and confidence improves model accuracy, and incorporating confidence in the model\u2019s temperature function can lead to models with significantly better-calibrated confidence measures. We also find some insightful qualitative results regarding human and model behavior on these datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Commonsense Contingent Reasoning by Pseudo-data and Its Application to the Related Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.68/",
        "paper_authors": [
            "Kazumasa Omura",
            "Sadao Kurohashi"
        ],
        "paper_abstract": "Contingent reasoning is one of the essential abilities in natural language understanding, and many language resources annotated with contingent relations have been constructed. However, despite the recent advances in deep learning, the task of contingent reasoning is still difficult for computers. In this study, we focus on the reasoning of contingent relation between basic events. Based on the existing data construction method, we automatically generate large-scale pseudo-problems and incorporate the generated data into training. We also investigate the generality of contingent knowledge through quantitative evaluation by performing transfer learning on the related tasks: discourse relation analysis, the Japanese Winograd Schema Challenge, and the JCommonsenseQA. The experimental results show the effectiveness of utilizing pseudo-problems for both the commonsense contingent reasoning task and the related tasks, which suggests the importance of contingent reasoning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",
        "paper_url": "https://aclanthology.org/2022.coling-1.69/",
        "paper_authors": [
            "Qingcheng Zeng",
            "An-Ran Li"
        ],
        "paper_abstract": "Irony is a ubiquitous figurative language in daily communication. Previously, many researchers have approached irony from linguistic, cognitive science, and computational aspects. Recently, some progress have been witnessed in automatic irony processing due to the rapid development in deep neural models in natural language processing (NLP). In this paper, we will provide a comprehensive overview of computational irony, insights from linguisic theory and cognitive science, as well as its interactions with downstream NLP tasks and newly proposed multi-X irony processing perspectives.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Identifying Alternative-Lexicalization Signals of Discourse Relations",
        "paper_url": "https://aclanthology.org/2022.coling-1.70/",
        "paper_authors": [
            "Ren\u00e9 Knaebel",
            "Manfred Stede"
        ],
        "paper_abstract": "The task of shallow discourse parsing in the Penn Discourse Treebank (PDTB) framework has traditionally been restricted to identifying those relations that are signaled by a discourse connective (\u201cexplicit\u201d) and those that have no signal at all (\u201cimplicit\u201d). The third type, the more flexible group of \u201cAltLex\u201d realizations has been neglected because of its small amount of occurrences in the PDTB2 corpus. Their number has grown significantly in the recent PDTB3, and in this paper, we present the first approaches for recognizing these \u201calternative lexicalizations\u201d. We compare the performance of a pattern-based approach and a sequence labeling model, add an experiment on the pre-classification of candidate sentences, and provide an initial qualitative analysis of the error cases made by both models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Topicalization in Language Models: A Case Study on Japanese",
        "paper_url": "https://aclanthology.org/2022.coling-1.71/",
        "paper_authors": [
            "Riki Fujihara",
            "Tatsuki Kuribayashi",
            "Kaori Abe",
            "Ryoko Tokuhisa",
            "Kentaro Inui"
        ],
        "paper_abstract": "Humans use different wordings depending on the context to facilitate efficient communication. For example, instead of completely new information, information related to the preceding context is typically placed at the sentence-initial position. In this study, we analyze whether neural language models (LMs) can capture such discourse-level preferences in text generation. Specifically, we focus on a particular aspect of discourse, namely the topic-comment structure. To analyze the linguistic knowledge of LMs separately, we chose the Japanese language, a topic-prominent language, for designing probing tasks, and we created human topicalization judgment data by crowdsourcing. Our experimental results suggest that LMs have different generalizations from humans; LMs exhibited less context-dependent behaviors toward topicalization judgment. These results highlight the need for the additional inductive biases to guide LMs to achieve successful discourse-level generalization.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "\u201cNo, They Did Not\u201d: Dialogue Response Dynamics in Pre-trained Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.72/",
        "paper_authors": [
            "Sanghee J. Kim",
            "Lang Yu",
            "Allyson Ettinger"
        ],
        "paper_abstract": "A critical component of competence in language is being able to identify relevant components of an utterance and reply appropriately. In this paper we examine the extent of such dialogue response sensitivity in pre-trained language models, conducting a series of experiments with a particular focus on sensitivity to dynamics involving phenomena of at-issueness and ellipsis. We find that models show clear sensitivity to a distinctive role of embedded clauses, and a general preference for responses that target main clause content of prior utterances. However, the results indicate mixed and generally weak trends with respect to capturing the full range of dynamics involved in targeting at-issue versus not-at-issue content. Additionally, models show fundamental limitations in grasp of the dynamics governing ellipsis, and response selections show clear interference from superficial factors that outweigh the influence of principled discourse constraints.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "New or Old? Exploring How Pre-Trained Language Models Represent Discourse Entities",
        "paper_url": "https://aclanthology.org/2022.coling-1.73/",
        "paper_authors": [
            "Sharid Lo\u00e1iciga",
            "Anne Beyer",
            "David Schlangen"
        ],
        "paper_abstract": "Recent research shows that pre-trained language models, built to generate text conditioned on some context, learn to encode syntactic knowledge to a certain degree. This has motivated researchers to move beyond the sentence-level and look into their ability to encode less studied discourse-level phenomena. In this paper, we add to the body of probing research by investigating discourse entity representations in large pre-trained language models in English. Motivated by early theories of discourse and key pieces of previous work, we focus on the information-status of entities as discourse-new or discourse-old. We present two probing models, one based on binary classification and another one on sequence labeling. The results of our experiments show that pre-trained language models do encode information on whether an entity has been introduced before or not in the discourse. However, this information alone is not sufficient to find the entities in a discourse, opening up interesting questions about the definition of entities for future work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dialo-AP: A Dependency Parsing Based Argument Parser for Dialogues",
        "paper_url": "https://aclanthology.org/2022.coling-1.74/",
        "paper_authors": [
            "Sougata Saha",
            "Souvik Das",
            "Rohini K. Srihari"
        ],
        "paper_abstract": "While neural approaches to argument mining (AM) have advanced considerably, most of the recent work has been limited to parsing monologues. With an urgent interest in the use of conversational agents for broader societal applications, there is a need to advance the state-of-the-art in argument parsers for dialogues. This enables progress towards more purposeful conversations involving persuasion, debate and deliberation. This paper discusses Dialo-AP, an end-to-end argument parser that constructs argument graphs from dialogues. We formulate AM as dependency parsing of elementary and argumentative discourse units; the system is trained using extensive pre-training and curriculum learning comprising nine diverse corpora. Dialo-AP is capable of generating argument graphs from dialogues by performing all sub-tasks of AM. Compared to existing state-of-the-art baselines, Dialo-AP achieves significant improvements across all tasks, which is further validated through rigorous human evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ConnPrompt: Connective-cloze Prompt Learning for Implicit Discourse Relation Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.75/",
        "paper_authors": [
            "Wei Xiang",
            "Zhenglin Wang",
            "Lu Dai",
            "Bang Wang"
        ],
        "paper_abstract": "Implicit Discourse Relation Recognition (IDRR) is to detect and classify relation sense between two text segments without an explicit connective. Vanilla pre-train and fine-tuning paradigm builds upon a Pre-trained Language Model (PLM) with a task-specific neural network. However, the task objective functions are often not in accordance with that of the PLM. Furthermore, this paradigm cannot well exploit some linguistic evidence embedded in the pre-training process. The recent pre-train, prompt, and predict paradigm selects appropriate prompts to reformulate downstream tasks, so as to utilizing the PLM itself for prediction. However, for its success applications, prompts, verbalizer as well as model training should still be carefully designed for different tasks. As the first trial of using this new paradigm for IDRR, this paper develops a Connective-cloze Prompt (ConnPrompt) to transform the relation prediction task as a connective-cloze task. Specifically, we design two styles of ConnPrompt template: Insert-cloze Prompt (ICP) and Prefix-cloze Prompt (PCP) and construct an answer space mapping to the relation senses based on the hierarchy sense tags and implicit connectives. Furthermore, we use a multi-prompt ensemble to fuse predictions from different prompting results. Experiments on the PDTB corpus show that our method significantly outperforms the state-of-the-art algorithms, even with fewer training data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Distance-Aware Multi-Task Framework for Conversational Discourse Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.76/",
        "paper_authors": [
            "Yaxin Fan",
            "Peifeng Li",
            "Fang Kong",
            "Qiaoming Zhu"
        ],
        "paper_abstract": "Conversational discourse parsing aims to construct an implicit utterance dependency tree to reflect the turn-taking in a multi-party conversation. Existing works are generally divided into two lines: graph-based and transition-based paradigms, which perform well for short-distance and long-distance dependency links, respectively. However, there is no study to consider the advantages of both paradigms to facilitate conversational discourse parsing. As a result, we propose a distance-aware multi-task framework DAMT that incorporates the strengths of transition-based paradigm to facilitate the graph-based paradigm from the encoding and decoding process. To promote multi-task learning on two paradigms, we first introduce an Encoding Interactive Module (EIM) to enhance the flow of semantic information between both two paradigms during the encoding step. And then we apply a Distance-Aware Graph Convolutional Network (DAGCN) in the decoding process, which can incorporate the different-distance dependency links predicted by the transition-based paradigm to facilitate the decoding of the graph-based paradigm. The experimental results on the datasets STAC and Molweni show that our method can significantly improve the performance of the SOTA graph-based paradigm on long-distance dependency links.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Linguistically Motivated Features for Classifying Shorter Text into Fiction and Non-Fiction Genre",
        "paper_url": "https://aclanthology.org/2022.coling-1.77/",
        "paper_authors": [
            "Arman Kazmi",
            "Sidharth Ranjan",
            "Arpit Sharma",
            "Rajakrishnan Rajkumar"
        ],
        "paper_abstract": "This work deploys linguistically motivated features to classify paragraph-level text into fiction and non-fiction genre using a logistic regression model and infers lexical and syntactic properties that distinguish the two genres. Previous works have focused on classifying document-level text into fiction and non-fiction genres, while in this work, we deal with shorter texts which are closer to real-world applications like sentiment analysis of tweets. Going beyond simple POS tag ratios proposed in Qureshi et al.(2019) for document-level classification, we extracted multiple linguistically motivated features belonging to four categories: Lexical features, POS ratio features, Syntactic features and Raw features. For the task of short-text classification, a model containing 28 best-features (selected via Recursive feature elimination with cross-validation; RFECV) confers an accuracy jump of 15.56 % over a baseline model consisting of 2 POS-ratio features found effective in previous work (cited above). The efficacy of the above model containing a linguistically motivated feature set also transfers over to another dataset viz, Baby BNC corpus. We also compared the classification accuracy of the logistic regression model with two deep-learning models. A 1D CNN model gives an increase of 2% accuracy over the logistic Regression classifier on both corpora. And the BERT-base-uncased model gives the best classification accuracy of 97% on Brown corpus and 98% on Baby BNC corpus. Although both the deep learning models give better results in terms of classification accuracy, the problem of interpreting these models remains unsolved. In contrast, regression model coefficients revealed that fiction texts tend to have more character-level diversity and have lower lexical density (quantified using content-function word ratios) compared to non-fiction texts. Moreover, subtle differences in word order exist between the two genres, i.e., in fiction texts Verbs precede Adverbs (inter-alia).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantic Sentence Matching via Interacting Syntax Graphs",
        "paper_url": "https://aclanthology.org/2022.coling-1.78/",
        "paper_authors": [
            "Chen Xu",
            "Jun Xu",
            "Zhenhua Dong",
            "Ji-Rong Wen"
        ],
        "paper_abstract": "Studies have shown that the sentence\u2019s syntactic structures are important for semantic sentence matching. A typical approach is encoding each sentence\u2019s syntactic structure into an embedding vector, which can be combined with other features to predict the final matching scores. Though successes have been observed, embedding the whole syntactic structures as one vector inevitably overlooks the fine-grained syntax matching patterns, e.g. the alignment of specific term dependencies relations in the two inputted sentences. In this paper, we formalize the task of semantic sentence matching as a problem of graph matching in which each sentence is represented as a directed graph according to its syntactic structures. The syntax matching patterns (i.e. similar syntactic structures) between two sentences, therefore, can be extracted as the sub-graph structure alignments. The proposed method, referred to as Interacted Syntax Graphs (ISG), represents two sentences\u2019 syntactic alignments as well as their semantic matching signals into one association graph. After that, the neural quadratic assignment programming (QAP) is adapted to extract syntactic matching patterns from the association graph. In this way, the syntactic structures fully interact in a fine granularity during the matching process. Experimental results on three public datasets demonstrated that ISG can outperform the state-of-the-art baselines effectively and efficiently. The empirical analysis also showed that ISG can match sentences in an interpretable way.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Hierarchical Information Matters: Text Classification via Tree Based Graph Neural Network",
        "paper_url": "https://aclanthology.org/2022.coling-1.79/",
        "paper_authors": [
            "Chong Zhang",
            "He Zhu",
            "Xingyu Peng",
            "Junran Wu",
            "Ke Xu"
        ],
        "paper_abstract": "Text classification is a primary task in natural language processing (NLP). Recently, graph neural networks (GNNs) have developed rapidly and been applied to text classification tasks. As a special kind of graph data, the tree has a simpler data structure and can provide rich hierarchical information for text classification. Inspired by the structural entropy, we construct the coding tree of the graph by minimizing the structural entropy and propose HINT, which aims to make full use of the hierarchical information contained in the text for the task of text classification. Specifically, we first establish a dependency parsing graph for each text. Then we designed a structural entropy minimization algorithm to decode the key information in the graph and convert each graph to its corresponding coding tree. Based on the hierarchical structure of the coding tree, the representation of the entire graph is obtained by updating the representation of non-leaf nodes in the coding tree layer by layer. Finally, we present the effectiveness of hierarchical information in text classification. Experimental results show that HINT outperforms the state-of-the-art methods on popular benchmarks while having a simple structure and few parameters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SelfMix: Robust Learning against Textual Label Noise with Self-Mixup Training",
        "paper_url": "https://aclanthology.org/2022.coling-1.80/",
        "paper_authors": [
            "Dan Qiao",
            "Chenchen Dai",
            "Yuyang Ding",
            "Juntao Li",
            "Qiang Chen",
            "Wenliang Chen",
            "Min Zhang"
        ],
        "paper_abstract": "The conventional success of textual classification relies on annotated data, and the new paradigm of pre-trained language models (PLMs) still requires a few labeled data for downstream tasks. However, in real-world applications, label noise inevitably exists in training data, damaging the effectiveness, robustness, and generalization of the models constructed on such data. Recently, remarkable achievements have been made to mitigate this dilemma in visual data, while only a few explore textual data. To fill this gap, we present SelfMix, a simple yet effective method, to handle label noise in text classification tasks. SelfMix uses the Gaussian Mixture Model to separate samples and leverages semi-supervised learning. Unlike previous works requiring multiple models, our method utilizes the dropout mechanism on a single model to reduce the confirmation bias in self-training and introduces a textual level mixup training strategy. Experimental results on three text classification benchmarks with different types of text show that the performance of our proposed method outperforms these strong baselines designed for both textual and visual data under different noise ratios and noise types. Our anonymous code is available at https://github.com/noise-learning/SelfMix.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Community Topic: Topic Model Inference by Consecutive Word Community Discovery",
        "paper_url": "https://aclanthology.org/2022.coling-1.81/",
        "paper_authors": [
            "Eric Austin",
            "Osmar R. Za\u00efane",
            "Christine Largeron"
        ],
        "paper_abstract": "We present our novel, hyperparameter-free topic modelling algorithm, Community Topic. Our algorithm is based on mining communities from term co-occurrence networks. We empirically evaluate and compare Community Topic with Latent Dirichlet Allocation and the recently developed top2vec algorithm. We find that Community Topic runs faster than the competitors and produces topics that achieve higher coherence scores. Community Topic can discover coherent topics at various scales. The network representation used by Community Topic results in a natural relationship between topics and a topic hierarchy. This allows sub- and super-topics to be found on demand. These features make Community Topic the ideal tool for downstream applications such as applied research and conversational agents.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications",
        "paper_url": "https://aclanthology.org/2022.coling-1.82/",
        "paper_authors": [
            "Heng-yang Lu",
            "Chenyou Fan",
            "Jun Yang",
            "Cong Hu",
            "Wei Fang",
            "Xiao-jun Wu"
        ],
        "paper_abstract": "Nowadays, deep-learning based NLP models are usually trained with large-scale third-party data which can be easily injected with malicious backdoors. Thus, BackDoor Attack (BDA) study has become a trending research to help promote the robustness of an NLP system. Text-based BDA aims to train a poisoned model with both clean and poisoned texts to perform normally on clean inputs while being misled to predict those trigger-embedded texts as target labels set by attackers. Previous works usually choose fixed Positions-to-Poison (P2P) first, then add triggers upon those positions such as letter insertion or deletion. However, considering the positions of words with important semantics may vary in different contexts, fixed P2P models are severely limited in flexibility and performance. We study the text-based BDA from the perspective of automatically and dynamically selecting P2P from contexts. We design a novel Locator model which can predict P2P dynamically without human intervention. Based on the predicted P2P, four effective strategies are introduced to show the BDA performance. Experiments on two public datasets show both tinier test accuracy gap on clean data and higher attack success rate on poisoned ones. Human evaluation with volunteers also shows the P2P predicted by our model are important for classification. Source code is available at https://github.com/jncsnlp/LocatorModel",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Locally Distributed Activation Vectors for Guided Feature Attribution",
        "paper_url": "https://aclanthology.org/2022.coling-1.83/",
        "paper_authors": [
            "Housam K. B. Bashier",
            "Mi-Young Kim",
            "Randy Goebel"
        ],
        "paper_abstract": "Explaining the predictions of a deep neural network (DNN) is a challenging problem. Many attempts at interpreting those predictions have focused on attribution-based methods, which assess the contributions of individual features to each model prediction. However, attribution-based explanations do not always provide faithful explanations to the target model, e.g., noisy gradients can result in unfaithful feature attribution for back-propagation methods. We present a method to learn explanations-specific representations while constructing deep network models for text classification. These representations can be used to faithfully interpret black-box predictions, i.e., highlighting the most important input features and their role in any particular prediction. We show that learning specific representations improves model interpretability across various tasks, for both qualitative and quantitative evaluations, while preserving predictive performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Addressing Leakage in Self-Supervised Contextualized Code Retrieval",
        "paper_url": "https://aclanthology.org/2022.coling-1.84/",
        "paper_authors": [
            "Johannes Villmow",
            "Viola Campos",
            "Adrian Ulges",
            "Ulrich Schwanecke"
        ],
        "paper_abstract": "We address contextualized code retrieval, the search for code snippets helpful to fill gaps in a partial input program. Our approach facilitates a large-scale self-supervised contrastive training by splitting source code randomly into contexts and targets. To combat leakage between the two, we suggest a novel approach based on mutual identifier masking, dedentation, and the selection of syntax-aligned targets. Our second contribution is a new dataset for direct evaluation of contextualized code retrieval, based on a dataset of manually aligned subpassages of code clones. Our experiments demonstrate that the proposed approach improves retrieval substantially, and yields new state-of-the-art results for code clone and defect detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Domain Knowledge Enhanced Pre-Trained Language Model for Vertical Search: Case Study on Medicinal Products",
        "paper_url": "https://aclanthology.org/2022.coling-1.85/",
        "paper_authors": [
            "Kesong Liu",
            "Jianhui Jiang",
            "Feifei Lyu"
        ],
        "paper_abstract": "We present a biomedical knowledge enhanced pre-trained language model for medicinal product vertical search. Following ELECTRA\u2019s replaced token detection (RTD) pre-training, we leverage biomedical entity masking (EM) strategy to learn better contextual word representations. Furthermore, we propose a novel pre-training task, product attribute prediction (PAP), to inject product knowledge into the pre-trained language model efficiently by leveraging medicinal product databases directly. By sharing the parameters of PAP\u2019s transformer encoder with that of RTD\u2019s main transformer, these two pre-training tasks are jointly learned. Experiments demonstrate the effectiveness of PAP task for pre-trained language model on medicinal product vertical search scenario, which includes query-title relevance, query intent classification, and named entity recognition in query.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval",
        "paper_url": "https://aclanthology.org/2022.coling-1.86/",
        "paper_authors": [
            "Kung-Hsiang Huang",
            "ChengXiang Zhai",
            "Heng Ji"
        ],
        "paper_abstract": "Fact-checking has gained increasing attention due to the widespread of falsified information. Most fact-checking approaches focus on claims made in English only due to the data scarcity issue in other languages. The lack of fact-checking datasets in low-resource languages calls for an effective cross-lingual transfer technique for fact-checking. Additionally, trustworthy information in different languages can be complementary and helpful in verifying facts. To this end, we present the first fact-checking framework augmented with cross-lingual retrieval that aggregates evidence retrieved from multiple languages through a cross-lingual retriever. Given the absence of cross-lingual information retrieval datasets with claim-like queries, we train the retriever with our proposed Cross-lingual Inverse Cloze Task (X-ICT), a self-supervised algorithm that creates training instances by translating the title of a passage. The goal for X-ICT is to learn cross-lingual retrieval in which the model learns to identify the passage corresponding to a given translated title. On the X-Fact dataset, our approach achieves 2.23% absolute F1 improvement in the zero-shot cross-lingual setup over prior systems. The source code and data are publicly available at https://github.com/khuangaf/CONCRETE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "E-VarM: Enhanced Variational Word Masks to Improve the Interpretability of Text Classification Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.87/",
        "paper_authors": [
            "Ling Ge",
            "ChunMing Hu",
            "Guanghui Ma",
            "Junshuang Wu",
            "Junfan Chen",
            "JiHong Liu",
            "Hong Zhang",
            "Wenyi Qin",
            "Richong Zhang"
        ],
        "paper_abstract": "Enhancing the interpretability of text classification models can help increase the reliability of these models in real-world applications. Currently, most researchers focus on extracting task-specific words from inputs to improve the interpretability of the model. The competitive approaches exploit the Variational Information Bottleneck (VIB) to improve the performance of word masking at the word embedding layer to obtain task-specific words. However, these approaches ignore the multi-level semantics of the text, which can impair the interpretability of the model, and do not consider the risk of representation overlap caused by the VIB, which can impair the classification performance. In this paper, we propose an enhanced variational word masks approach, named E-VarM, to solve these two issues effectively. The E-VarM combines multi-level semantics from all hidden layers of the model to mask out task-irrelevant words and uses contrastive learning to readjust the distances between representations. Empirical studies on ten benchmark text classification datasets demonstrate that our approach outperforms the SOTA methods in simultaneously improving the interpretability and accuracy of the model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Attribute Injection for Pretrained Language Models: A New Benchmark and an Efficient Method",
        "paper_url": "https://aclanthology.org/2022.coling-1.88/",
        "paper_authors": [
            "Reinald Kim Amplayo",
            "Kang Min Yoo",
            "Sang-Woo Lee"
        ],
        "paper_abstract": "Metadata attributes (e.g., user and product IDs from reviews) can be incorporated as additional inputs to neural-based NLP models, by expanding the architecture of the models to improve performance. However, recent models rely on pretrained language models (PLMs), in which previously used techniques for attribute injection are either nontrivial or cost-ineffective. In this paper, we introduce a benchmark for evaluating attribute injection models, which comprises eight datasets across a diverse range of tasks and domains and six synthetically sparsified ones. We also propose a lightweight and memory-efficient method to inject attributes into PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules, to include attributes both independently of or jointly with the text. We use approximation techniques to parameterize the model efficiently for domains with large attribute vocabularies, and training mechanisms to handle multi-labeled and sparse attributes. Extensive experiments and analyses show that our method outperforms previous attribute injection methods and achieves state-of-the-art performance on all datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Robust Neural Retrieval with Source Domain Synthetic Pre-Finetuning",
        "paper_url": "https://aclanthology.org/2022.coling-1.89/",
        "paper_authors": [
            "Revanth Gangi Reddy",
            "Vikas Yadav",
            "Md Arafat Sultan",
            "Martin Franz",
            "Vittorio Castelli",
            "Heng Ji",
            "Avirup Sil"
        ],
        "paper_abstract": "Research on neural IR has so far been focused primarily on standard supervised learning settings, where it outperforms traditional term matching baselines. Many practical use cases of such models, however, may involve previously unseen target domains. In this paper, we propose to improve the out-of-domain generalization of Dense Passage Retrieval (DPR) - a popular choice for neural IR - through synthetic data augmentation only in the source domain. We empirically show that pre-finetuning DPR with additional synthetic data in its source domain (Wikipedia), which we generate using a fine-tuned sequence-to-sequence generator, can be a low-cost yet effective first step towards its generalization. Across five different test sets, our augmented model shows more robust performance than DPR in both in-domain and zero-shot out-of-domain evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
        "paper_url": "https://aclanthology.org/2022.coling-1.90/",
        "paper_authors": [
            "Robert Litschko",
            "Ivan Vuli\u0107",
            "Goran Glava\u0161"
        ],
        "paper_abstract": "State-of-the-art neural (re)rankers are notoriously data-hungry which \u2013 given the lack of large-scale training data in languages other than English \u2013 makes them rarely used in multilingual and cross-lingual retrieval settings. Current approaches therefore commonly transfer rankers trained on English data to other languages and cross-lingual setups by means of multilingual encoders: they fine-tune all parameters of pretrained massively multilingual Transformers (MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy them in the target language(s). In this work, we show that two parameter-efficient approaches to cross-lingual transfer, namely Sparse Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more effective zero-shot transfer to multilingual and cross-lingual retrieval tasks. We first train language adapters (or SFTMs) via Masked Language Modelling and then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping all other parameters fixed. At inference, this modular design allows us to compose the ranker by applying the (re)ranking adapter (or SFTM) trained with source language data together with the language adapter (or SFTM) of a target language. We carry out a large scale evaluation on the CLEF-2003 and HC4 benchmarks and additionally, as another contribution, extend the former with queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed parameter-efficient methods outperform standard zero-shot transfer with full MMT fine-tuning, while being more modular and reducing training times. The gains are particularly pronounced for low-resource languages, where our approaches also substantially outperform the competitive machine translation-based rankers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LIME: Weakly-Supervised Text Classification without Seeds",
        "paper_url": "https://aclanthology.org/2022.coling-1.91/",
        "paper_authors": [
            "Seongmin Park",
            "Jihwa Lee"
        ],
        "paper_abstract": "In weakly-supervised text classification, only label names act as sources of supervision. Predominant approaches to weakly-supervised text classification utilize a two-phase framework, where test samples are first assigned pseudo-labels and are then used to train a neural text classifier. In most previous work, the pseudo-labeling step is dependent on obtaining seed words that best capture the relevance of each class label. We present LIME, a framework for weakly-supervised text classification that entirely replaces the brittle seed-word generation process with entailment-based pseudo-classification. We find that combining weakly-supervised classification and textual entailment mitigates shortcomings of both, resulting in a more streamlined and effective classification pipeline. With just an off-the-shelf textual entailment model, LIME outperforms recent baselines in weakly-supervised text classification and achieves state-of-the-art in 4 benchmarks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Stage Framework with Refinement Based Point Set Registration for Unsupervised Bi-Lingual Word Alignment",
        "paper_url": "https://aclanthology.org/2022.coling-1.92/",
        "paper_authors": [
            "Silviu Vlad Oprea",
            "Sourav Dutta",
            "Haytham Assem"
        ],
        "paper_abstract": "Cross-lingual alignment of word embeddings are important in knowledge transfer across languages, for improving machine translation and other multi-lingual applications. Current unsupervised approaches relying on learning structure-preserving transformations, using adversarial networks and refinement strategies, suffer from instability and convergence issues. This paper proposes BioSpere, a novel multi-stage framework for unsupervised mapping of bi-lingual word embeddings onto a shared vector space, by combining adversarial initialization, refinement procedure and point set registration. Experiments for parallel dictionary induction and word similarity demonstrate state-of-the-art unsupervised results for BioSpere on diverse languages \u2013 showcasing robustness against variable adversarial performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "EM-PERSONA: EMotion-assisted Deep Neural Framework for PERSONAlity Subtyping from Suicide Notes",
        "paper_url": "https://aclanthology.org/2022.coling-1.93/",
        "paper_authors": [
            "Soumitra Ghosh",
            "Dhirendra Kumar Maurya",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "The World Health Organization has emphasised the need of stepping up suicide prevention efforts to meet the United Nation\u2019s Sustainable Development Goal target of 2030 (Goal 3: Good health and well-being). We address the challenging task of personality subtyping from suicide notes. Most research on personality subtyping has relied on statistical analysis and feature engineering. Moreover, state-of-the-art transformer models in the automated personality subtyping problem have received relatively less attention. We develop a novel EMotion-assisted PERSONAlity Detection Framework (EM-PERSONA). We annotate the benchmark CEASE-v2.0 suicide notes dataset with personality traits across four dichotomies: Introversion (I)-Extraversion (E), Intuition (N)-Sensing (S), Thinking (T)-Feeling (F), Judging (J)\u2013Perceiving (P). Our proposed method outperforms all baselines on comprehensive evaluation using multiple state-of-the-art systems. Across the four dichotomies, EM-PERSONA improved accuracy by 2.04%, 3.69%, 4.52%, and 3.42%, respectively, over the highest-performing single-task systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dense Template Retrieval for Customer Support",
        "paper_url": "https://aclanthology.org/2022.coling-1.94/",
        "paper_authors": [
            "Tiago Mesquita",
            "Bruno Martins",
            "Mariana Almeida"
        ],
        "paper_abstract": "Templated answers are used extensively in customer support scenarios, providing an efficient way to cover a plethora of topics, with an easily maintainable collection of templates. However, the number of templates is often too high for an agent to manually search. Automatically suggesting the correct template for a given question can thus improve the service efficiency, reducing costs and leading to a better customer satisfaction. In this work, we propose a dense retrieval framework for the customer support scenario, adapting a standard in-batch negatives technique to support unpaired sampling of queries and templates. We also propose a novel loss that extends the typical query-centric similarity, exploiting other similarity relations in the training data. Experiments show that our approach achieves considerable improvements, in terms of performance and training speed, over more standard dense retrieval methods. This includes methods such as DPR, and also ablated versions of the proposed approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Exploring Label Hierarchy in a Generative Way for Hierarchical Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.95/",
        "paper_authors": [
            "Wei Huang",
            "Chen Liu",
            "Bo Xiao",
            "Yihua Zhao",
            "Zhaoming Pan",
            "Zhimin Zhang",
            "Xinyun Yang",
            "Guiquan Liu"
        ],
        "paper_abstract": "Hierarchical Text Classification (HTC), which aims to predict text labels organized in hierarchical space, is a significant task lacking in investigation in natural language processing. Existing methods usually encode the entire hierarchical structure and fail to construct a robust label-dependent model, making it hard to make accurate predictions on sparse lower-level labels and achieving low Macro-F1. In this paper, we explore the level dependency and path dependency of the label hierarchy in a generative way for building the knowledge of upper-level labels of current path into lower-level ones, and thus propose a novel PAAM-HiA-T5 model for HTC: a hierarchy-aware T5 model with path-adaptive attention mechanism. Specifically, we generate a multi-level sequential label structure to exploit hierarchical dependency across different levels with Breadth-First Search (BFS) and T5 model. To further improve label dependency prediction within each path, we then propose an original path-adaptive attention mechanism (PAAM) to lead the model to adaptively focus on the path where the currently generated label is located, shielding the noise from other paths. Comprehensive experiments on three benchmark datasets show that PAAM-HiA-T5 greatly outperforms all state-of-the-art HTC approaches especially in Macro-F1.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MuSeCLIR: A Multiple Senses and Cross-lingual Information Retrieval Dataset",
        "paper_url": "https://aclanthology.org/2022.coling-1.96/",
        "paper_authors": [
            "Wing Yan Li",
            "Julie Weeds",
            "David Weir"
        ],
        "paper_abstract": "This paper addresses a deficiency in existing cross-lingual information retrieval (CLIR) datasets and provides a robust evaluation of CLIR systems\u2019 disambiguation ability. CLIR is commonly tackled by combining translation and traditional IR. Due to translation ambiguity, the problem of ambiguity is worse in CLIR than in monolingual IR. But existing auto-generated CLIR datasets are dominated by searches for named entity mentions, which does not provide a good measure for disambiguation performance, as named entity mentions can often be transliterated across languages and tend not to have multiple translations. Therefore, we introduce a new evaluation dataset (MuSeCLIR) to address this inadequacy. The dataset focusses on polysemous common nouns with multiple possible translations. MuSeCLIR is constructed from multilingual Wikipedia and supports searches on documents written in European (French, German, Italian) and Asian (Chinese, Japanese) languages. We provide baseline statistical and neural model results on MuSeCLIR which show that MuSeCLIR has a higher requirement on the ability of systems to disambiguate query terms.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Complicate Then Simplify: A Novel Way to Explore Pre-trained Models for Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.97/",
        "paper_authors": [
            "Xu Zhang",
            "Zejie Liu",
            "Yanzheng Xiang",
            "Deyu Zhou"
        ],
        "paper_abstract": "With the development of pre-trained models (PTMs), the performance of text classification has been continuously improved by directly employing the features generated by PTMs. However such way might not fully explore the knowledge in PTMs as it is constrained by the difficulty of the task. Compared to difficult task, the learning algorithms tend to saturate early on the simple task. Moreover, the native sentence representations derived from BERT are prone to be collapsed and directly employing such representation for text classification might fail to fully capture discriminative features. In order to address these issues, in this paper we propose a novel framework for text classification which implements a two-stage training strategy. In the pre-training stage, auxiliary labels are introduced to increase the task difficulties and to fully exploit the knowledge in the pre-trained model. In the fine-tuning stage, the textual representation learned in the pre-training stage is employed and the classifier is fine-tuned to obtain better classification performance. Experiments were conducted on six text classification corpora and the results showed that the proposed framework outperformed several state-of-the-art baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptive Feature Discrimination and Denoising for Asymmetric Text Matching",
        "paper_url": "https://aclanthology.org/2022.coling-1.98/",
        "paper_authors": [
            "Yan Li",
            "Chenliang Li",
            "Junjun Guo"
        ],
        "paper_abstract": "Asymmetric text matching has becoming increasingly indispensable for many downstream tasks (e.g., IR and NLP). Here, asymmetry means that the documents involved for matching hold different amounts of information, e.g., a short query against a relatively longer document. The existing solutions mainly focus on modeling the feature interactions between asymmetric texts, but rarely go one step further to recognize discriminative features and perform feature denoising to enhance relevance learning. In this paper, we propose a novel adaptive feature discrimination and denoising model for asymmetric text matching, called ADDAX. For each asymmetric text pair, ADDAX is devised to explicitly distinguish discriminative features and filter out irrelevant features in a context-aware fashion. Concretely, a matching-adapted gating siamese cell (MAGS) is firstly devised to identify discriminative features and produce the corresponding hybrid representations for a text pair. Afterwards, we introduce a locality-constrained hashing denoiser to perform feature-level denoising by learning a discriminative low-dimensional binary codes for redundantly longer text. Extensive experiments on four real-world datasets from different downstream tasks demostrate that the proposed ADDAX obtains substantial performance gain over 36 up-to-date state-of-the-art alternatives.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Rethinking Data Augmentation in Text-to-text Paradigm",
        "paper_url": "https://aclanthology.org/2022.coling-1.99/",
        "paper_authors": [
            "Yanan Chen",
            "Yang Liu"
        ],
        "paper_abstract": "As manually labelling data can be costly, some recent studies tend to augment the training data for improving the generalization power of machine learning models, known as data augmentation (DA). With the arise of pre-trained language models (PLMs), some recent works on DA try to synthesize new samples benefiting from the knowledge learned from PLM\u2019s pre-training. Along the same direction, we in this paper propose to integrate text-to-text language models and construct a new two-phase framework for augmentation: 1) a fine-tuning phase where PLMs are well adapted to downstream classification with the help of two novel schemes, and 2) a generation phase where the fine-tuned models are leveraged to create new samples for performance lifting. This paradigm opens up a new way of designing fine-tuning scheme to better serve DA in an easy-to-implement manner, and can be easily extended to other desired tasks. We evaluate our proposal on two public classification datasets and demonstrate its effectiveness with remarkable gains.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ConTextING: Granting Document-Wise Contextual Embeddings to Graph Neural Networks for Inductive Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.100/",
        "paper_authors": [
            "Yen-Hao Huang",
            "Yi-Hsin Chen",
            "Yi-Shin Chen"
        ],
        "paper_abstract": "Graph neural networks (GNNs) have been recently applied in natural language processing. Various GNN research studies are proposed to learn node interactions within the local graph of each document that contains words, sentences, or topics for inductive text classification. However, most inductive GNNs that are built on a word graph generally take global word embeddings as node features, without referring to document-wise contextual information. Consequently, we find that BERT models can perform better than inductive GNNs. An intuitive follow-up approach is used to enrich GNNs with contextual embeddings from BERT, yet there is a lack of related research. In this work, we propose a simple yet effective unified model, coined ConTextING, with a joint training mechanism to learn from both document embeddings and contextual word interactions simultaneously. Our experiments show that ConTextING outperforms pure inductive GNNs and BERT-style models. The analyses also highlight the benefits of the sub-word graph and joint training with separated classifiers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval",
        "paper_url": "https://aclanthology.org/2022.coling-1.101/",
        "paper_authors": [
            "Yeon Seonwoo",
            "Seunghyun Yoon",
            "Franck Dernoncourt",
            "Trung Bui",
            "Alice Oh"
        ],
        "paper_abstract": "Domain-specific documents cover terminologies and specialized knowledge. This has been the main challenge of domain-specific document retrieval systems. Previous approaches propose domain-adaptation and transfer learning methods to alleviate this problem. However, these approaches still follow the same document representation method in previous approaches; a document is embedded into a single vector. In this study, we propose VKGDR. VKGDR represents a given corpus into a graph of entities and their relations (known as a virtual knowledge graph) and computes the relevance between queries and documents based on the graph representation. We conduct three experiments 1) domain-specific document retrieval, 2) comparison of our virtual knowledge graph construction method with previous approaches, and 3) ablation study on each component of our virtual knowledge graph. From the results, we see that unsupervised VKGDR outperforms baselines in a zero-shot setting and even outperforms fully-supervised bi-encoder. We also verify that our virtual knowledge graph construction method results in better retrieval performance than previous approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MICO: Selective Search with Mutual Information Co-training",
        "paper_url": "https://aclanthology.org/2022.coling-1.102/",
        "paper_authors": [
            "Zhanyu Wang",
            "Xiao Zhang",
            "Hyokun Yun",
            "Choon Hui Teo",
            "Trishul Chilimbi"
        ],
        "paper_abstract": "In contrast to traditional exhaustive search, selective search first clusters documents into several groups before all the documents are searched exhaustively by a query, to limit the search executed within one group or only a few groups. Selective search is designed to reduce the latency and computation in modern large-scale search systems. In this study, we propose MICO, a Mutual Information CO-training framework for selective search with minimal supervision using the search logs. After training, MICO does not only cluster the documents, but also routes unseen queries to the relevant clusters for efficient retrieval. In our empirical experiments, MICO significantly improves the performance on multiple metrics of selective search and outperforms a number of existing competitive baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval",
        "paper_url": "https://aclanthology.org/2022.coling-1.103/",
        "paper_authors": [
            "Zhengyang Tang",
            "Benyou Wang",
            "Ting Yao"
        ],
        "paper_abstract": "Deep prompt tuning (DPT) has gained great success in most natural language processing (NLP) tasks. However, it is not well-investigated in dense retrieval where fine-tuning (FT) still dominates. When deploying multiple retrieval tasks using the same backbone model (e.g., RoBERTa), FT-based methods are unfriendly in terms of deployment cost: each new retrieval model needs to repeatedly deploy the backbone model without reuse. To reduce the deployment cost in such a scenario, this work investigates applying DPT in dense retrieval. The challenge is that directly applying DPT in dense retrieval largely underperforms FT methods. To compensate for the performance drop, we propose two model-agnostic and task-agnostic strategies for DPT-based retrievers, namely retrieval-oriented intermediate pretraining and unified negative mining, as a general approach that could be compatible with any pre-trained language model and retrieval task. The experimental results show that the proposed method (called DPTDR) outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct ablation studies to examine the effectiveness of each strategy in DPTDR. We believe this work facilitates the industry, as it saves enormous efforts and costs of deployment and increases the utility of computing resources. Our code is available at https://github.com/tangzhy/DPTDR.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.104/",
        "paper_authors": [
            "Ziwen Liu",
            "Josep Grau-Bove",
            "Scott Allan Orr"
        ],
        "paper_abstract": "Multi-label Text Classification (MLTC) is the task of categorizing documents into one or more topics. Considering the large volumes of data and varying domains of such tasks, fully supervised learning requires manually fully annotated datasets which is costly and time-consuming. In this paper, we propose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text Classification (WSMLTC) model that reduces the need for full supervision. This new model (1) produces BERT sentence embeddings and calibrates them using a flow model, (2) generates an initial topic-document matrix by averaging results of a seeded sparse topic model and a textual entailment model which only require surface name of topics and 4-6 seed words per topic, and (3) adopts a VAE framework to reconstruct the embeddings under the guidance of the topic-document matrix. Finally, (4) it uses the means produced by the encoder model in the VAE architecture as predictions for MLTC. Experimental results on 6 multi-label datasets show that BFV can substantially outperform other baseline WSMLTC models in key metrics and achieve approximately 84% performance of a fully-supervised model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender",
        "paper_url": "https://aclanthology.org/2022.coling-1.105/",
        "paper_authors": [
            "Anne Lauscher",
            "Archie Crowley",
            "Dirk Hovy"
        ],
        "paper_abstract": "The world of pronouns is changing \u2013 from a closed word class with few members to an open set of terms to reflect identities. However, Natural Language Processing (NLP) barely reflects this linguistic shift, resulting in the possible exclusion of non-binary users, even though recent work outlined the harms of gender-exclusive language technology. The current modeling of 3rd person pronouns is particularly problematic. It largely ignores various phenomena like neopronouns, i.e., novel pronoun sets that are not (yet) widely established. This omission contributes to the discrimination of marginalized and underrepresented groups, e.g., non-binary individuals. It thus prevents gender equality, one of the UN\u2019s sustainable development goals (goal 5). Further, other identity-expressions beyond gender are ignored by current NLP technology. This paper provides an overview of 3rd person pronoun issues for NLP. Based on our observations and ethical considerations, we define a series of five desiderata for modeling pronouns in language technology, which we validate through a survey. We evaluate existing and novel modeling approaches w.r.t. these desiderata qualitatively and quantify the impact of a more discrimination-free approach on an established benchmark dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Threat Scenarios and Best Practices to Detect Neural Fake News",
        "paper_url": "https://aclanthology.org/2022.coling-1.106/",
        "paper_authors": [
            "Artidoro Pagnoni",
            "Martin Graciarena",
            "Yulia Tsvetkov"
        ],
        "paper_abstract": "In this work, we discuss different threat scenarios from neural fake news generated by state-of-the-art language models. Through our experiments, we assess the performance of generated text detection systems under these threat scenarios. For each scenario, we also identify the minimax strategy for the detector that minimizes its worst-case performance. This constitutes a set of best practices that practitioners can rely on. In our analysis, we find that detectors are prone to shortcut learning (lack of out-of-distribution generalization) and discuss approaches to mitigate this problem and improve detectors more broadly. Finally, we argue that strong detectors should be released along with new generators.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "From Polarity to Intensity: Mining Morality from Semantic Space",
        "paper_url": "https://aclanthology.org/2022.coling-1.107/",
        "paper_authors": [
            "Chunxu Zhao",
            "Pengyuan Liu",
            "Dong Yu"
        ],
        "paper_abstract": "Most works on computational morality focus on moral polarity recognition, i.e., distinguishing right from wrong. However, a discrete polarity label is not informative enough to reflect morality as it does not contain any degree or intensity information. Existing approaches to compute moral intensity are limited to word-level measurement and heavily rely on human labelling. In this paper, we propose MoralScore, a weakly-supervised framework that can automatically measure moral intensity from text. It only needs moral polarity labels, which are more robust and easier to acquire. Besides, the framework can capture latent moral information not only from words but also from sentence-level semantics which can provide a more comprehensive measurement. To evaluate the performance of our method, we introduce a set of evaluation metrics and conduct extensive experiments. Results show that our method achieves good performance on both automatic and human evaluations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SOS: Systematic Offensive Stereotyping Bias in Word Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.108/",
        "paper_authors": [
            "Fatma Elsafoury",
            "Steve R. Wilson",
            "Stamos Katsigiannis",
            "Naeem Ramzan"
        ],
        "paper_abstract": "Systematic Offensive stereotyping (SOS) in word embeddings could lead to associating marginalised groups with hate speech and profanity, which might lead to blocking and silencing those groups, especially on social media platforms. In this [id=stk]work, we introduce a quantitative measure of the SOS bias, [id=stk]validate it in the most commonly used word embeddings, and investigate if it explains the performance of different word embeddings on the task of hate speech detection. Results show that SOS bias exists in almost all examined word embeddings and that [id=stk]the proposed SOS bias metric correlates positively with the statistics of published surveys on online extremism. We also show that the [id=stk]proposed metric reveals distinct information [id=stk]compared to established social bias metrics. However, we do not find evidence that SOS bias explains the performance of hate speech detection models based on the different word embeddings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bigger Data or Fairer Data? Augmenting BERT via Active Sampling for Educational Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.109/",
        "paper_authors": [
            "Lele Sha",
            "Yuheng Li",
            "Dragan Gasevic",
            "Guanliang Chen"
        ],
        "paper_abstract": "Pretrained Language Models (PLMs), though popular, have been diagnosed to encode bias against protected groups in the representations they learn, which may harm the prediction fairness of downstream models. Given that such bias is believed to be related to the amount of demographic information carried in the learned representations, this study aimed to quantify the awareness that a PLM (i.e., BERT) has regarding people\u2019s protected attributes and augment BERT to improve prediction fairness of downstream models by inhibiting this awareness. Specifically, we developed a method to dynamically sample data to continue the pretraining of BERT and enable it to generate representations carrying minimal demographic information, which can be directly used as input to downstream models for fairer predictions. By experimenting on the task of classifying educational forum posts and measuring fairness between students of different gender or first-language backgrounds, we showed that, compared to a baseline without any additional pretraining, our method improved not only fairness (with a maximum improvement of 52.33%) but also accuracy (with a maximum improvement of 2.53%). Our method can be generalized to any PLM and demographic attributes. All the codes used in this study can be accessed via https://github.com/lsha49/FairBERT_deploy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Debiasing Word Embeddings with Nonlinear Geometry",
        "paper_url": "https://aclanthology.org/2022.coling-1.110/",
        "paper_authors": [
            "Lu Cheng",
            "Nayoung Kim",
            "Huan Liu"
        ],
        "paper_abstract": "Debiasing word embeddings has been largely limited to individual and independent social categories. However, real-world corpora typically present multiple social categories that possibly correlate or intersect with each other. For instance, \u201chair weaves\u201d is stereotypically associated with African American females, but neither African American nor females alone. Therefore, this work studies biases associated with multiple social categories: joint biases induced by the union of different categories and intersectional biases that do not overlap with the biases of the constituent categories. We first empirically observe that individual biases intersect non-trivially (i.e., over a one-dimensional subspace). Drawing from the intersectional theory in social science and the linguistic theory, we then construct an intersectional subspace to debias for multiple social categories using the nonlinear geometry of individual biases. Empirical evaluations corroborate the efficacy of our approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Debiasing Isn\u2019t Enough! \u2013 on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.111/",
        "paper_authors": [
            "Masahiro Kaneko",
            "Danushka Bollegala",
            "Naoaki Okazaki"
        ],
        "paper_abstract": "We study the relationship between task-agnostic intrinsic and task-specific extrinsic social bias evaluation measures for MLMs, and find that there exists only a weak correlation between these two types of evaluation measures. Moreover, we find that MLMs debiased using different methods still re-learn social biases during fine-tuning on downstream tasks. We identify the social biases in both training instances as well as their assigned labels as reasons for the discrepancy between intrinsic and extrinsic bias evaluation measurements. Overall, our findings highlight the limitations of existing MLM bias evaluation measures and raise concerns on the deployment of MLMs in downstream applications using those measures.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Quantifying Bias from Decoding Techniques in Natural Language Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.112/",
        "paper_authors": [
            "Mayukh Das",
            "Wolf Tilo Balke"
        ],
        "paper_abstract": "Natural language generation (NLG) models can propagate social bias towards particular demography. Though several studies investigated bias from data and model, NLG task distinctively uses stochastic decoder that can positively or negatively impact the bias-sensitive tokens initially predicted by the model. To address this gap in research, we present an extensive analysis of bias from decoding techniques for open-domain language generation considering the entire decoding space. We analyze to what extent bias metrics like toxicity and sentiment are impacted by the individual components of decoder algorithms. To this extent, we also analyze the trade-off between bias scores and human-annotated generation quality throughout the decoder space. Together, these methods reveal the imperative of testing inference time bias and provide evidence on the usefulness of inspecting the entire decoding spectrum.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Study of Implicit Bias in Pretrained Language Models against People with Disabilities",
        "paper_url": "https://aclanthology.org/2022.coling-1.113/",
        "paper_authors": [
            "Pranav Narayanan Venkit",
            "Mukund Srinath",
            "Shomir Wilson"
        ],
        "paper_abstract": "Pretrained language models (PLMs) have been shown to exhibit sociodemographic biases, such as against gender and race, raising concerns of downstream biases in language technologies. However, PLMs\u2019 biases against people with disabilities (PWDs) have received little attention, in spite of their potential to cause similar harms. Using perturbation sensitivity analysis, we test an assortment of popular word embedding-based and transformer-based PLMs and show significant biases against PWDs in all of them. The results demonstrate how models trained on large corpora widely favor ableist language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Social Norms-Grounded Machine Ethics in Complex Narrative Situation",
        "paper_url": "https://aclanthology.org/2022.coling-1.114/",
        "paper_authors": [
            "Tao Shen",
            "Xiubo Geng",
            "Daxin Jiang"
        ],
        "paper_abstract": "Ethical judgment aims to determine if a person in a narrative situation acts under people\u2019s social norms under a culture, so it is crucial to understand actions in narratives and achieve machine ethics. Recent works depend on data-driven methods to directly judge the ethics of complex real-world narratives but face two major challenges. First, they cannot well handle dilemma situations due to a lack of basic knowledge about social norms. Second, they focus merely on sparse situation-level judgment regardless of the social norms involved during the judgment, leading to a black box. In this work, inspired by previous knowledge-grounded and -augmented paradigms, we propose to complement a complex situation with grounded social norms. Besides a norm-grounding knowledge model, we present a novel norm-supported ethical judgment model in line with neural module networks to alleviate dilemma situations and improve norm-level explainability. Empirically, our model improves state-of-the-art performance on two narrative judgment benchmarks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling",
        "paper_url": "https://aclanthology.org/2022.coling-1.115/",
        "paper_authors": [
            "Thiemo Wambsganss",
            "Vinitra Swamy",
            "Roman Rietsche",
            "Tanja K\u00e4ser"
        ],
        "paper_abstract": "Natural Language Processing (NLP) has become increasingly utilized to provide adaptivity in educational applications. However, recent research has highlighted a variety of biases in pre-trained language models. While existing studies investigate bias in different domains, they are limited in addressing fine-grained analysis on educational corpora and text that is not English. In this work, we analyze bias across text and through multiple architectures on a corpus of 9,165 German peer-reviews collected from university students over five years. Notably, our corpus includes labels such as helpfulness, quality, and critical aspect ratings from the peer-review recipient as well as demographic attributes. We conduct a Word Embedding Association Test (WEAT) analysis on (1) our collected corpus in connection with the clustered labels, (2) the most common pre-trained German language models (T5, BERT, and GPT-2) and GloVe embeddings, and (3) the language models after fine-tuning on our collected data-set. In contrast to our initial expectations, we found that our collected corpus does not reveal many biases in the co-occurrence analysis or in the GloVe embeddings. However, the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data. With our research, we aim to contribute to the fourth UN sustainability goal (quality education) with a novel dataset, an understanding of biases in natural language education data, and the potential harms of not counteracting biases in language models for educational tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dynamic Relevance Graph Network for Knowledge-Aware Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.116/",
        "paper_authors": [
            "Chen Zheng",
            "Parisa Kordjamshidi"
        ],
        "paper_abstract": "This work investigates the challenge of learning and reasoning for Commonsense Question Answering given an external source of knowledge in the form of a knowledge graph (KG). We propose a novel graph neural network architecture, called Dynamic Relevance Graph Network (DRGN). DRGN operates on a given KG subgraph based on the question and answers entities and uses the relevance scores between the nodes to establish new edges dynamically for learning node representations in the graph network. This explicit usage of relevance as graph edges has the following advantages, a) the model can exploit the existing relationships, re-scale the node weights, and influence the way the neighborhood nodes\u2019 representations are aggregated in the KG subgraph, b) It potentially recovers the missing edges in KG that are needed for reasoning. Moreover, as a byproduct, our model improves handling the negative questions due to considering the relevance between the question node and the graph entities. Our proposed approach shows competitive performance on two QA benchmarks, CommonsenseQA and OpenbookQA, compared to the state-of-the-art published results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SISER: Semantic-Infused Selective Graph Reasoning for Fact Verification",
        "paper_url": "https://aclanthology.org/2022.coling-1.117/",
        "paper_authors": [
            "Eunhwan Park",
            "Jong-Hyeon Lee",
            "DongHyeon Jeon",
            "Seonhoon Kim",
            "Inho Kang",
            "Seung-Hoon Na"
        ],
        "paper_abstract": "This study proposes Semantic-Infused SElective Graph Reasoning (SISER) for fact verification, which newly presents semantic-level graph reasoning and injects its reasoning-enhanced representation into other types of graph-based and sequence-based reasoning methods. SISER combines three reasoning types: 1) semantic-level graph reasoning, which uses a semantic graph from evidence sentences, whose nodes are elements of a triple \u2013 <Subject, Verb, Object>, 2) \u201csemantic-infused\u201d sentence-level \u201cselective\u201d graph reasoning, which combine semantic-level and sentence-level representations and perform graph reasoning in a selective manner using the node selection mechanism, and 3) sequence reasoning, which concatenates all evidence sentences and performs attention-based reasoning. Experiment results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that SISER outperforms the previous graph-based approaches and achieves state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder",
        "paper_url": "https://aclanthology.org/2022.coling-1.118/",
        "paper_authors": [
            "Fangyu Lei",
            "Shizhu He",
            "Xiang Li",
            "Jun Zhao",
            "Kang Liu"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Perform like an Engine: A Closed-Loop Neural-Symbolic Learning Framework for Knowledge Graph Inference",
        "paper_url": "https://aclanthology.org/2022.coling-1.119/",
        "paper_authors": [
            "Guanglin Niu",
            "Bo Li",
            "Yongfei Zhang",
            "Shiliang Pu"
        ],
        "paper_abstract": "Knowledge graph (KG) inference aims to address the natural incompleteness of KGs, including rule learning-based and KG embedding (KGE) models. However, the rule learning-based models suffer from low efficiency and generalization while KGE models lack interpretability. To address these challenges, we propose a novel and effective closed-loop neural-symbolic learning framework EngineKG via incorporating our developed KGE and rule learning modules. KGE module exploits symbolic rules and paths to enhance the semantic association between entities and relations for improving KG embeddings and interpretability. A novel rule pruning mechanism is proposed in the rule learning module by leveraging paths as initial candidate rules and employing KG embeddings together with concepts for extracting more high-quality rules. Experimental results on four real-world datasets show that our model outperforms the relevant baselines on link prediction tasks, demonstrating the superiority of our KG inference model in a neural-symbolic learning fashion. The source code and datasets of this paper are available at https://github.com/ngl567/EngineKG.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Table-based Fact Verification with Self-labeled Keypoint Alignment",
        "paper_url": "https://aclanthology.org/2022.coling-1.120/",
        "paper_authors": [
            "Guangzhen Zhao",
            "Peng Yang"
        ],
        "paper_abstract": "Table-based fact verification aims to verify whether a statement sentence is trusted or fake. Most existing methods rely on graph feature or data augmentation but fail to investigate evidence correlation between the statement and table effectively. In this paper, we propose a self-Labeled Keypoint Alignment model, named LKA, to explore the correlation between the two. Specifically, a dual-view alignment module based on the statement and table views is designed to discriminate the salient words through multiple interactions, where one regular and one adversarial alignment network cooperatively character the alignment discrepancy. Considering the interaction characteristic inherent in the alignment module, we introduce a novel mixture-of experts block to elaborately integrate the interacted information for supporting the alignment and final classification. Furthermore, a contrastive learning loss is utilized to learn the precise representation of the structure-involved words, encouraging the words closer to words with the same table attribute and farther from the words with the unrelated attribute. Experimental results on three widely-studied datasets show that our model can outperform the state-of-the-art baselines and capture interpretable evidence words.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification",
        "paper_url": "https://aclanthology.org/2022.coling-1.121/",
        "paper_authors": [
            "Hao Wang",
            "Yangguang Li",
            "Zhen Huang",
            "Yong Dou"
        ],
        "paper_abstract": "With the rapid development of automatic fake news detection technology, fact extraction and verification (FEVER) has been attracting more attention. The task aims to extract the most related fact evidences from millions of open-domain Wikipedia documents and then verify the credibility of corresponding claims. Although several strong models have been proposed for the task and they have made great process, we argue that they fail to utilize multi-view contextual information and thus cannot obtain better performance. In this paper, we propose to integrate multi-view contextual information (IMCI) for fact extraction and verification. For each evidence sentence, we define two kinds of context, i.e. intra-document context and inter-document context. Intra-document context consists of the document title and all the other sentences from the same document. Inter-document context consists of all other evidences which may come from different documents. Then we integrate the multi-view contextual information to encode the evidence sentences to handle the task. Our experimental results on FEVER 1.0 shared task show that our IMCI framework makes great progress on both fact extraction and verification, and achieves state-of-the-art performance with a winning FEVER score of 73.96% and label accuracy of 77.25% on the online blind test set. We also conduct ablation study to detect the impact of multi-view contextual information.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words",
        "paper_url": "https://aclanthology.org/2022.coling-1.122/",
        "paper_authors": [
            "Haochun Wang",
            "Chi Liu",
            "Nuwa Xi",
            "Sendong Zhao",
            "Meizhi Ju",
            "Shiwei Zhang",
            "Ziheng Zhang",
            "Yefeng Zheng",
            "Bing Qin",
            "Ting Liu"
        ],
        "paper_abstract": "Prompt-based fine-tuning for pre-trained models has proven effective for many natural language processing tasks under few-shot settings in general domain. However, tuning with prompt in biomedical domain has not been investigated thoroughly. Biomedical words are often rare in general domain, but quite ubiquitous in biomedical contexts, which dramatically deteriorates the performance of pre-trained models on downstream biomedical applications even after fine-tuning, especially in low-resource scenarios. We propose a simple yet effective approach to helping models learn rare biomedical words during tuning with prompt. Experimental results show that our method can achieve up to 6% improvement in biomedical natural language inference task without any extra parameters or training steps using few-shot vanilla prompt settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Self-Supervised Intermediate Fine-Tuning of Biomedical Language Models for Interpreting Patient Case Descriptions",
        "paper_url": "https://aclanthology.org/2022.coling-1.123/",
        "paper_authors": [
            "Israa Alghanmi",
            "Luis Espinosa-Anke",
            "Steven Schockaert"
        ],
        "paper_abstract": "Interpreting patient case descriptions has emerged as a challenging problem for biomedical NLP, where the aim is typically to predict diagnoses, to recommended treatments, or to answer questions about cases more generally. Previous work has found that biomedical language models often lack the knowledge that is needed for such tasks. In this paper, we aim to improve their performance through a self-supervised intermediate fine-tuning strategy based on PubMed abstracts. Our solution builds on the observation that many of these abstracts are case reports, and thus essentially patient case descriptions. As a general strategy, we propose to fine-tune biomedical language models on the task of predicting masked medical concepts from such abstracts. We find that the success of this strategy crucially depends on the selection of the medical concepts to be masked. By ensuring that these concepts are sufficiently salient, we can substantially boost the performance of biomedical language models, achieving state-of-the-art results on two benchmarks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating and Mitigating Inherent Linguistic Bias of African American English through Inference",
        "paper_url": "https://aclanthology.org/2022.coling-1.124/",
        "paper_authors": [
            "Jamell Dacon",
            "Haochen Liu",
            "Jiliang Tang"
        ],
        "paper_abstract": "Recent studies show that NLP models trained on standard English texts tend to produce biased outcomes against underrepresented English varieties. In this work, we conduct a pioneering study of the English variety use of African American English (AAE) in NLI task. First, we propose CodeSwitch, a greedy unidirectional morphosyntactically-informed rule-based translation method for data augmentation. Next, we use CodeSwitch to present a preliminary study to determine if demographic language features do in fact influence models to produce false predictions. Then, we conduct experiments on two popular datasets and propose two simple, yet effective and generalizable debiasing methods. Our findings show that NLI models (e.g. BERT) trained under our proposed frameworks outperform traditional large language models while maintaining or even improving the prediction performance. In addition, we intend to release CodeSwitch, in hopes of promoting dialectal language diversity in training data to both reduce the discriminatory societal impacts and improve model robustness of downstream NLP tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?",
        "paper_url": "https://aclanthology.org/2022.coling-1.125/",
        "paper_authors": [
            "Jesus Lovon-Melgarejo",
            "Jose G. Moreno",
            "Romaric Besan\u00e7on",
            "Olivier Ferret",
            "Lynda Tamine"
        ],
        "paper_abstract": "Despite the success of state-of-the-art pre-trained language models (PLMs) on a series of multi-hop reasoning tasks, they still suffer from their limited abilities to transfer learning from simple to complex tasks and vice-versa. We argue that one step forward to overcome this limitation is to better understand the behavioral trend of PLMs at each hop over the inference chain. Our critical underlying idea is to mimic human-style reasoning: we envision the multi-hop reasoning process as a sequence of explicit single-hop reasoning steps. To endow PLMs with incremental reasoning skills, we propose a set of inference strategies on relevant facts and distractors allowing us to build automatically generated training datasets. Using the SHINRA and ConceptNet resources jointly, we empirically show the effectiveness of our proposal on multiple-choice question answering and reading comprehension, with a relative improvement in terms of accuracy of 68.4% and 16.0% w.r.t. classic PLMs, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension",
        "paper_url": "https://aclanthology.org/2022.coling-1.126/",
        "paper_authors": [
            "Jialin Chen",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "paper_abstract": "Machine reading comprehension (MRC) poses new challenges to logical reasoning, which aims to understand the implicit logical relations entailed in the given contexts and perform inference over them. Due to the complexity of logic, logical connections exist at different granularity levels. However, most existing methods of logical reasoning individually focus on either entity-aware or discourse-based information but ignore the hierarchical relations that may even have mutual effects. This paper proposes a holistic graph network (HGN) that deals with context at both discourse-level and word-level as the basis for logical reasoning to provide a more fine-grained relation extraction. Specifically, node-level and type-level relations, which can be interpreted as bridges in the reasoning process, are modeled by a hierarchical interaction mechanism to improve the interpretation of MRC systems. Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and natural language inference datasets (SNLI and ANLI) show the effectiveness and generalization of our method, and in-depth analysis verifies its capability to understand complex logical relations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.127/",
        "paper_authors": [
            "Jianguo Mao",
            "Jiyuan Zhang",
            "Zengfeng Zeng",
            "Weihua Peng",
            "Wenbin Jiang",
            "Xiangdong Wang",
            "Hong Liu",
            "Yajuan Lyu"
        ],
        "paper_abstract": "Recently, Biomedical Question Answering (BQA) has attracted growing attention due to its application value and technical challenges. Most existing works treat it as a semantic matching task that predicts answers by computing confidence among questions, options and evidence sentences, which is insufficient for scenarios that require complex reasoning based on a deep understanding of biomedical evidences. We propose a novel model termed Hierarchical Representation-based Dynamic Reasoning Network (HDRN) to tackle this problem. It first constructs the hierarchical representations for biomedical evidences to learn semantics within and among evidences. It then performs dynamic reasoning based on the hierarchical representations of evidences to solve complex biomedical problems. Against the existing state-of-the-art model, the proposed model significantly improves more than 4.5%, 3% and 1.3% on three mainstream BQA datasets, PubMedQA, MedQA-USMLE and NLPEC. The ablation study demonstrates the superiority of each improvement of our model. The code will be released after the paper is published.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ArT: All-round Thinker for Unsupervised Commonsense Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.128/",
        "paper_authors": [
            "Jiawei Wang",
            "Hai Zhao"
        ],
        "paper_abstract": "Without labeled question-answer pairs for necessary training, unsupervised commonsense question-answering (QA) appears to be extremely challenging due to its indispensable unique prerequisite on commonsense source like knowledge bases (KBs), which are usually highly resource consuming in construction. Recently pre-trained language models (PLMs) show effectiveness as an alternative for commonsense clues when they play a role of knowledge generator. However, existing work either relies on large-scale in-domain or out-of-domain labeled data, or fails to generate knowledge of high quality in a general way. Motivated by human thinking experience, we propose an approach of All-round Thinker (ArT) by fully taking association during knowledge generating. In detail, our model first focuses on key parts in the given context, and then generates highly related knowledge on such a basis in an association way like human thinking. Besides, for casual reasoning, a reverse thinking mechanism is especially added to further enhance bidirectional inferring between cause and effect. ArT is totally unsupervised and KBs-free. We evaluate it on three commonsense QA benchmarks: COPA, SocialIQA and SCT. On all scales of PLM backbones, ArT shows its brilliant performance and outperforms previous advanced unsupervised models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Teaching Neural Module Networks to Do Arithmetic",
        "paper_url": "https://aclanthology.org/2022.coling-1.129/",
        "paper_authors": [
            "Jiayi Chen",
            "Xiao-Yu Guo",
            "Yuan-Fang Li",
            "Gholamreza Haffari"
        ],
        "paper_abstract": "Answering complex questions that require multi-step multi-type reasoning over raw text is challenging, especially when conducting numerical reasoning. Neural Module Networks (NMNs), follow the programmer-interpreter framework and design trainable modules to learn different reasoning skills. However, NMNs only have limited reasoning abilities, and lack numerical reasoning capability. We upgrade NMNs by: (a) bridging the gap between its interpreter and the complex questions; (b) introducing addition and subtraction modules that perform numerical reasoning over numbers. On a subset of DROP, experimental results show that our proposed methods enhance NMNs\u2019 numerical reasoning skills by 17.7% improvement of F1 score and significantly outperform previous state-of-the-art models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding",
        "paper_url": "https://aclanthology.org/2022.coling-1.130/",
        "paper_authors": [
            "Jie Cao",
            "Jing Xiao"
        ],
        "paper_abstract": "Automatic math problem solving has attracted much attention of NLP researchers recently. However, most of the works focus on the solving of Math Word Problems (MWPs). In this paper, we study on the Geometric Problem Solving based on neural networks. Solving geometric problems requires the integration of text and diagram information as well as the knowledge of the relevant theorems. The lack of high-quality datasets and efficient neural geometric solvers impedes the development of automatic geometric problems solving. Based on GeoQA, we newly annotate 2,518 geometric problems with richer types and greater difficulty to form an augmented benchmark dataset GeoQA+, containing 6,027 problems in training set and 7,528 totally. We further perform data augmentation method to expand the training set to 12,054. Besides, we design a Dual Parallel text Encoder DPE to efficiently encode long and medium-length problem text. The experimental results validate the effectiveness of GeoQA+ and DPE module, and the accuracy of automatic geometric problem solving is improved to 66.09%.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Competence-based Question Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.131/",
        "paper_authors": [
            "Jingxuan Tu",
            "Kyeongmin Rim",
            "James Pustejovsky"
        ],
        "paper_abstract": "Models of natural language understanding often rely on question answering and logical inference benchmark challenges to evaluate the performance of a system. While informative, such task-oriented evaluations do not assess the broader semantic abilities that humans have as part of their linguistic competence when speaking and interpreting language. We define competence-based (CB) question generation, and focus on queries over lexical semantic knowledge involving implicit argument and subevent structure of verbs. We present a method to generate such questions and a dataset of English cooking recipes we use for implementing the generation method. Our primary experiment shows that even large pretrained language models perform poorly on CB questions until they are provided with additional contextualized semantic information. The data and the source code is available at: https: //github.com/brandeis-llc/CompQG.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Coalescing Global and Local Information for Procedural Text Understanding",
        "paper_url": "https://aclanthology.org/2022.coling-1.132/",
        "paper_authors": [
            "Kaixin Ma",
            "Filip Ilievski",
            "Jonathan Francis",
            "Eric Nyberg",
            "Alessandro Oltramari"
        ],
        "paper_abstract": "Procedural text understanding is a challenging language reasoning task that requires models to track entity states across the development of a narrative. We identify three core aspects required for modeling this task, namely the local and global view of the inputs, as well as the global view of outputs. Prior methods have considered a subset of these aspects, which leads to either low precision or low recall. In this paper, we propose a new model Coalescing Global and Local Information (CGLI), which builds entity- and timestep-aware input representations (local input) considering the whole context (global input), and we jointly model the entity states with a structured prediction objective (global output). Thus, CGLI simultaneously optimizes for both precision and recall. Moreover, we extend CGLI with additional output layers and integrate it into a story reasoning framework. Extensive experiments on a popular procedural text understanding dataset show that our model achieves state-of-the-art results, while experiments on a story reasoning benchmark show the positive impact of our model on downstream reasoning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Original Content Is All You Need! an Empirical Study on Leveraging Answer Summary for WikiHowQA Answer Selection Task",
        "paper_url": "https://aclanthology.org/2022.coling-1.133/",
        "paper_authors": [
            "Liang Wen",
            "Juan Li",
            "Houfeng Wang",
            "Yingwei Luo",
            "Xiaolin Wang",
            "Xiaodong Zhang",
            "Zhicong Cheng",
            "Dawei Yin"
        ],
        "paper_abstract": "Answer selection task requires finding appropriate answers to questions from informative but crowdsourced candidates. A key factor impeding its solution by current answer selection approaches is the redundancy and lengthiness issues of crowdsourced answers. Recently, Deng et al. (2020) constructed a new dataset, WikiHowQA, which contains a corresponding reference summary for each original lengthy answer. And their experiments show that leveraging the answer summaries helps to attend the essential information in original lengthy answers and improve the answer selection performance under certain circumstances. However, when given a question and a set of long candidate answers, human beings could effortlessly identify the correct answer without the aid of additional answer summaries since the original answers contain all the information volume that answer summaries contain. In addition, pretrained language models have been shown superior or comparable to human beings on many natural language processing tasks. Motivated by those, we design a series of neural models, either pretraining-based or non-pretraining-based, to check wether the additional answer summaries are helpful for ranking the relevancy degrees of question-answer pairs on WikiHowQA dataset. Extensive automated experiments and hand analysis show that the additional answer summaries are not useful for achieving the best performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Case-Based Abductive Natural Language Inference",
        "paper_url": "https://aclanthology.org/2022.coling-1.134/",
        "paper_authors": [
            "Marco Valentino",
            "Mokanarangan Thayaparan",
            "Andr\u00e9 Freitas"
        ],
        "paper_abstract": "Most of the contemporary approaches for multi-hop Natural Language Inference (NLI) construct explanations considering each test case in isolation. However, this paradigm is known to suffer from semantic drift, a phenomenon that causes the construction of spurious explanations leading to wrong conclusions. In contrast, this paper proposes an abductive framework for multi-hop NLI exploring the retrieve-reuse-refine paradigm in Case-Based Reasoning (CBR). Specifically, we present Case-Based Abductive Natural Language Inference (CB-ANLI), a model that addresses unseen inference problems by analogical transfer of prior explanations from similar examples. We empirically evaluate the abductive framework on commonsense and scientific question answering tasks, demonstrating that CB-ANLI can be effectively integrated with sparse and dense pre-trained encoders to improve multi-hop inference, or adopted as an evidence retriever for Transformers. Moreover, an empirical analysis of semantic drift reveals that the CBR paradigm boosts the quality of the most challenging explanations, a feature that has a direct impact on robustness and accuracy in downstream inference tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantic Structure Based Query Graph Prediction for Question Answering over Knowledge Graph",
        "paper_url": "https://aclanthology.org/2022.coling-1.135/",
        "paper_authors": [
            "Mingchen Li",
            "Shihao Ji"
        ],
        "paper_abstract": "Building query graphs from natural language questions is an important step in complex question answering over knowledge graph (Complex KGQA). In general, a question can be correctly answered if its query graph is built correctly and the right answer is then retrieved by issuing the query graph against the KG. Therefore, this paper focuses on query graph generation from natural language questions. Existing approaches for query graph generation ignore the semantic structure of a question, resulting in a large number of noisy query graph candidates that undermine prediction accuracies. In this paper, we define six semantic structures from common questions in KGQA and develop a novel Structure-BERT to predict the semantic structure of a question. By doing so, we can first filter out noisy candidate query graphs by the predicted semantic structures, and then rank the remaining candidates with a BERT-based ranking model. Extensive experiments on two popular benchmarks MetaQA and WebQuestionsSP (WSP) demonstrate the effectiveness of our method as compared to state-of-the-arts.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Repo4QA: Answering Coding Questions via Dense Retrieval on GitHub Repositories",
        "paper_url": "https://aclanthology.org/2022.coling-1.136/",
        "paper_authors": [
            "Minyu Chen",
            "Guoqiang Li",
            "Chen Ma",
            "Jingyang Li",
            "Hongfei Fu"
        ],
        "paper_abstract": "Open-source platforms such as GitHub and Stack Overflow both play significant roles in current software ecosystems. It is crucial but time-consuming for developers to raise programming questions in coding forums such as Stack Overflow and be navigated to actual solutions on GitHub repositories. In this paper, we dedicate to accelerating this activity. We find that traditional information retrieval-based methods fail to handle the long and complex questions in coding forums, and thus cannot find suitable coding repositories. To effectively and efficiently bridge the semantic gap between repositories and real-world coding questions, we introduce a specialized dataset named Repo4QA, which includes over 12,000 question-repository pairs constructed from Stack Overflow and GitHub. Furthermore, we propose QuRep, a CodeBERT-based model that jointly learns the representation of both questions and repositories. Experimental results demonstrate that our model simultaneously captures the semantic features in both questions and repositories through supervised contrastive loss and hard negative sampling. We report that our approach outperforms existing state-of-art methods by 3%-8% on MRR and 5%-8% on P@1.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Addressing Limitations of Encoder-Decoder Based Approach to Text-to-SQL",
        "paper_url": "https://aclanthology.org/2022.coling-1.137/",
        "paper_authors": [
            "Octavian Popescu",
            "Irene Manotas",
            "Ngoc Phuoc An Vo",
            "Hangu Yeo",
            "Elahe Khorashani",
            "Vadim Sheinin"
        ],
        "paper_abstract": "Most attempts on Text-to-SQL task using encoder-decoder approach show a big problem of dramatic decline in performance for new databases. For the popular Spider dataset, despite models achieving 70% accuracy on its development or test sets, the same models show a huge decline below 20% accuracy for unseen databases. The root causes for this problem are complex and they cannot be easily fixed by adding more manually created training. In this paper we address the problem and propose a solution that is a hybrid system using automated training-data augmentation technique. Our system consists of a rule-based and a deep learning components that interact to understand crucial information in a given query and produce correct SQL as a result. It achieves double-digit percentage improvement for databases that are not part of the Spider corpus.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.138/",
        "paper_authors": [
            "Priyanka Sen",
            "Alham Fikri Aji",
            "Amir Saffari"
        ],
        "paper_abstract": "We introduce Mintaka, a complex, natural, and multilingual dataset designed for experimenting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questions, including superlative, intersection, and multi-hop questions, which were naturally elicited from crowd workers. We run baselines over Mintaka, the best of which achieves 38% hits@1 in English and 31% hits@1 multilingually, showing that existing models have room for improvement. We release Mintaka at https://github.com/amazon-research/mintaka.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?",
        "paper_url": "https://aclanthology.org/2022.coling-1.139/",
        "paper_authors": [
            "Sagnik Ray Choudhury",
            "Nikita Bhutani",
            "Isabelle Augenstein"
        ],
        "paper_abstract": "There have been many efforts to try to understand what grammatical knowledge (e.g., ability to understand the part of speech of a token) is encoded in large pre-trained language models (LM). This is done through \u2018Edge Probing\u2019 (EP) tests: supervised classification tasks to predict the grammatical properties of a span (whether it has a particular part of speech) using only the token representations coming from the LM encoder. However, most NLP applications fine-tune these LM encoders for specific tasks. Here, we ask: if an LM is fine-tuned, does the encoding of linguistic information in it change, as measured by EP tests? Specifically, we focus on the task of Question Answering (QA) and conduct experiments on multiple datasets. We find that EP test results do not change significantly when the fine-tuned model performs well or in adversarial situations where the model is forced to learn wrong correlations. From a similar finding, some recent papers conclude that fine-tuning does not change linguistic knowledge in encoders but they do not provide an explanation. We find that EP models are susceptible to exploiting spurious correlations in the EP datasets. When this dataset bias is corrected, we do see an improvement in the EP test results as expected.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Conversational QA Dataset Generation with Answer Revision",
        "paper_url": "https://aclanthology.org/2022.coling-1.140/",
        "paper_authors": [
            "Seonjeong Hwang",
            "Gary Geunbae Lee"
        ],
        "paper_abstract": "Conversational question-answer generation is a task that automatically generates a large-scale conversational question answering dataset based on input passages. In this paper, we introduce a novel framework that extracts question-worthy phrases from a passage and then generates corresponding questions considering previous conversations. In particular, our framework revises the extracted answers after generating questions so that answers exactly match paired questions. Experimental results show that our simple answer revision approach leads to significant improvement in the quality of synthetic data. Moreover, we prove that our framework can be effectively utilized for domain adaptation of conversational question answering.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DABERT: Dual Attention Enhanced BERT for Semantic Matching",
        "paper_url": "https://aclanthology.org/2022.coling-1.141/",
        "paper_authors": [
            "Sirui Wang",
            "Di Liang",
            "Jian Song",
            "Yuntao Li",
            "Wei Wu"
        ],
        "paper_abstract": "Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiveness of our proposed method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.142/",
        "paper_authors": [
            "Siyuan Wang",
            "Zhongyu Wei",
            "Zhihao Fan",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Multi-hop reasoning requires aggregating multiple documents to answer a complex question. Existing methods usually decompose the multi-hop question into simpler single-hop questions to solve the problem for illustrating the explainable reasoning process. However, they ignore grounding on the supporting facts of each reasoning step, which tends to generate inaccurate decompositions. In this paper, we propose an interpretable stepwise reasoning framework to incorporate both single-hop supporting sentence identification and single-hop question generation at each intermediate step, and utilize the inference of the current hop for the next until reasoning out the final result. We employ a unified reader model for both intermediate hop reasoning and final hop inference and adopt joint optimization for more accurate and robust multi-hop reasoning. We conduct experiments on two benchmark datasets HotpotQA and 2WikiMultiHopQA. The results show that our method can effectively boost performance and also yields a better interpretable reasoning process without decomposition supervision.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Less Is Better: Recovering Intended-Feature Subspace to Robustify NLU Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.143/",
        "paper_authors": [
            "Ting Wu",
            "Tao Gui"
        ],
        "paper_abstract": "Datasets with significant proportions of bias present threats for training a trustworthy model on NLU tasks. Despite yielding great progress, current debiasing methods impose excessive reliance on the knowledge of bias attributes. Definition of the attributes, however, is elusive and varies across different datasets. In addition, leveraging these attributes at input level to bias mitigation may leave a gap between intrinsic properties and the underlying decision rule. To narrow down this gap and liberate the supervision on bias, we suggest extending bias mitigation into feature space. Therefore, a novel model, Recovering Intended-Feature Subspace with Knowledge-Free (RISK) is developed. Assuming that shortcut features caused by various biases are unintended for prediction, RISK views them as redundant features. When delving into a lower manifold to remove redundancies, RISK reveals that an extremely low-dimensional subspace with intended features can robustly represent the highly biased dataset. Empirical results demonstrate our model can consistently improve model generalization to out-of-distribution set, and achieves a new state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CORN: Co-Reasoning Network for Commonsense Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.144/",
        "paper_authors": [
            "Xin Guan",
            "Biwei Cao",
            "Qingqing Gao",
            "Zheng Yin",
            "Bo Liu",
            "Jiuxin Cao"
        ],
        "paper_abstract": "Commonsense question answering (QA) requires machines to utilize the QA content and external commonsense knowledge graph (KG) for reasoning when answering questions. Existing work uses two independent modules to model the QA contextual text representation and relationships between QA entities in KG, which prevents information sharing between modules for co-reasoning. In this paper, we propose a novel model, Co-Reasoning Network (CORN), which adopts a bidirectional multi-level connection structure based on Co-Attention Transformer. The structure builds bridges to connect each layer of the text encoder and graph encoder, which can introduce the QA entity relationship from KG to the text encoder and bring contextual text information to the graph encoder, so that these features can be deeply interactively fused to form comprehensive text and graph node representations. Meanwhile, we propose a QA-aware node based KG subgraph construction method. The QA-aware nodes aggregate the question entity nodes and the answer entity nodes, and further guide the expansion and construction process of the subgraph to enhance the connectivity and reduce the introduction of noise. We evaluate our model on QA benchmarks in the CommonsenseQA and OpenBookQA datasets, and CORN achieves state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases",
        "paper_url": "https://aclanthology.org/2022.coling-1.145/",
        "paper_authors": [
            "Xixin Hu",
            "Xuan Wu",
            "Yiheng Shu",
            "Yuzhong Qu"
        ],
        "paper_abstract": "Question answering over knowledge bases (KBQA) for complex questions is a challenging task in natural language processing. Recently, generation-based methods that translate natural language questions to executable logical forms have achieved promising performance. These methods use auxiliary information to augment the logical form generation of questions with unseen KB items or novel combinations, but the noise introduced can also leads to more incorrect results. In this work, we propose GMT-KBQA, a Generation-based KBQA method via Multi-Task learning, to better retrieve and utilize auxiliary information. GMT-KBQA first obtains candidate entities and relations through dense retrieval, and then introduces a multi-task model which jointly learns entity disambiguation, relation classification, and logical form generation. Experimental results show that GMT-KBQA achieves state-of-the-art results on both ComplexWebQuestions and WebQuestionsSP datasets. Furthermore, the detailed evaluation demonstrates that GMT-KBQA benefits from the auxiliary tasks and has a strong generalization capability.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers",
        "paper_url": "https://aclanthology.org/2022.coling-1.146/",
        "paper_authors": [
            "Yiming Ju",
            "Weikang Wang",
            "Yuanzhe Zhang",
            "Suncong Zheng",
            "Kang Liu",
            "Jun Zhao"
        ],
        "paper_abstract": "Forcing the answer of the Question Answering (QA) task to be a single text span might be restrictive since the answer can be multiple spans in the context. Moreover, we found that multi-span answers often appear with two characteristics when building the QA system for a real-world application. First, multi-span answers might be caused by users lacking domain knowledge and asking ambiguous questions, which makes the question need to be answered with conditions. Second, there might be hierarchical relations among multiple answer spans. Some recent span-extraction QA datasets include multi-span samples, but they only contain unconditional and parallel answers, which cannot be used to tackle this problem. To bridge the gap, we propose a new task: conditional question answering with hierarchical multi-span answers, where both the hierarchical relations and the conditions need to be extracted. Correspondingly, we introduce CMQA, a Conditional Multiple-span Chinese Question Answering dataset to study the new proposed task. The final release of CMQA consists of 7,861 QA pairs and 113,089 labels, where all samples contain multi-span answers, 50.4% of samples are conditional, and 56.6% of samples are hierarchical. CMQA can serve as a benchmark to study the new proposed task and help study building QA systems for real-world applications. The low performance of models drawn from related literature shows that the new proposed task is challenging for the community to solve.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning.",
        "paper_url": "https://aclanthology.org/2022.coling-1.147/",
        "paper_authors": [
            "Yitian Li",
            "Jidong Tian",
            "Wenqing Chen",
            "Caoyun Fan",
            "Hao He",
            "Yaohui Jin"
        ],
        "paper_abstract": "Reasoning and knowledge-related skills are considered as two fundamental skills for natural language understanding (NLU) tasks such as machine reading comprehension (MRC) and natural language inference (NLI). However, it is not clear to what extent an NLU task defined on a dataset correlates to a specific NLU skill. On the one hand, evaluating the correlation requires an understanding of the significance of the NLU skill in a dataset. Significance judges whether a dataset includes sufficient material to help the model master this skill. On the other hand, it is also necessary to evaluate the dependence of the task on the NLU skill. Dependence is a measure of how much the task defined on a dataset depends on the skill. In this paper, we propose a systematic method to diagnose the correlations between an NLU dataset and a specific skill, and then take a fundamental reasoning skill, logical reasoning, as an example for analysis. The method adopts a qualitative indicator to indicate the significance while adopting a quantitative indicator to measure the dependence. We perform diagnosis on 8 MRC datasets (including two types) and 3 NLI datasets and acquire intuitively reasonable results. We then perform the analysis to further understand the results and the proposed indicators. Based on the analysis, although the diagnostic method has some limitations, it is still an effective method to perform a basic diagnosis of the correlation between the dataset and logical reasoning skill, which also can be generalized to other NLU skills.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.148/",
        "paper_authors": [
            "Yu Gu",
            "Yu Su"
        ],
        "paper_abstract": "Question answering on knowledge bases (KBQA) poses a unique challenge for semantic parsing research due to two intertwined challenges: large search space and ambiguities in schema linking. Conventional ranking-based KBQA models, which rely on a candidate enumeration step to reduce the search space, struggle with flexibility in predicting complicated queries and have impractical running time. In this paper, we present ArcaneQA, a novel generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking. Experimental results on multiple popular KBQA datasets demonstrate the highly competitive performance of ArcaneQA in both effectiveness and efficiency.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Question Answering via Answer Diversifying",
        "paper_url": "https://aclanthology.org/2022.coling-1.149/",
        "paper_authors": [
            "Yuxiang Nie",
            "Heyan Huang",
            "Zewen Chi",
            "Xian-Ling Mao"
        ],
        "paper_abstract": "Unsupervised question answering is an attractive task due to its independence on labeled data. Previous works usually make use of heuristic rules as well as pre-trained models to construct data and train QA models. However, most of these works regard named entity (NE) as the only answer type, which ignores the high diversity of answers in the real world. To tackle this problem, we propose a novel unsupervised method by diversifying answers, named DiverseQA. Specifically, the proposed method is composed of three modules: data construction, data augmentation and denoising filter. Firstly, the data construction module extends the extracted named entity into a longer sentence constituent as the new answer span to construct a QA dataset with diverse answers. Secondly, the data augmentation module adopts an answer-type dependent data augmentation process via adversarial training in the embedding level. Thirdly, the denoising filter module is designed to alleviate the noise in the constructed data. Extensive experiments show that the proposed method outperforms previous unsupervised models on five benchmark datasets, including SQuADv1.1, NewsQA, TriviaQA, BioASQ, and DuoRC. Besides, the proposed method shows strong performance in the few-shot learning setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Weakly Supervised Formula Learner for Solving Mathematical Problems",
        "paper_url": "https://aclanthology.org/2022.coling-1.150/",
        "paper_authors": [
            "Yuxuan Wu",
            "Hideki Nakayama"
        ],
        "paper_abstract": "Mathematical reasoning task is a subset of the natural language question answering task. Existing work suggested solving this task with a two-phase approach, where the model first predicts formulas from questions and then calculates answers from such formulas. This approach achieved desirable performance in existing work. However, its reliance on annotated formulas as intermediate labels throughout its training limited its application. In this work, we put forward the idea to enable models to learn optimal formulas autonomously. We proposed Weakly Supervised Formula Learner, a learning framework that drives the formula exploration with weak supervision from the final answers to mathematical problems. Our experiments are conducted on two representative mathematical reasoning datasets MathQA and Math23K. On MathQA, our method outperformed baselines trained on complete yet imperfect formula annotations. On Math23K, our method outperformed other weakly supervised learning methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Reducing Spurious Correlations for Answer Selection by Feature Decorrelation and Language Debiasing",
        "paper_url": "https://aclanthology.org/2022.coling-1.151/",
        "paper_authors": [
            "Zeyi Zhong",
            "Min Yang",
            "Ruifeng Xu"
        ],
        "paper_abstract": "Deep neural models have become the mainstream in answer selection, yielding state-of-the-art performance. However, these models tend to rely on spurious correlations between prediction labels and input features, which in general suffer from robustness and generalization. In this paper, we propose a novel Spurious Correlation reduction method to improve the robustness of the neural ANswer selection models (SCAN) from the sample and feature perspectives by removing the feature dependencies and language biases in answer selection. First, from the sample perspective, we propose a feature decorrelation module by learning a weight for each instance at the training phase to remove the feature dependencies and reduce the spurious correlations without prior knowledge of such correlations. Second, from the feature perspective, we propose a feature debiasing module with contrastive learning to alleviate the negative language biases (spurious correlations) and further improve the robustness of the AS models. Experimental results on three benchmark datasets show that SCAN achieves substantial improvements over strong baselines. For reproducibility, we will release our code and data upon the publication of this paper.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.152/",
        "paper_authors": [
            "Zhengbao Jiang",
            "Jun Araki",
            "Haibo Ding",
            "Graham Neubig"
        ],
        "paper_abstract": "Generative question answering (QA) models generate answers to questions either solely based on the parameters of the model (the closed-book setting) or additionally retrieving relevant evidence (the open-book setting). Generative QA models can answer some relatively complex questions, but the mechanism through which they do so is still poorly understood. We perform several studies aimed at better understanding the multi-hop reasoning capabilities of generative QA models. First, we decompose multi-hop questions into multiple corresponding single-hop questions, and find marked inconsistency in QA models\u2019 answers on these pairs of ostensibly identical question chains. Second, we find that models lack zero-shot multi-hop reasoning ability: when trained only on single-hop questions, models generalize poorly to multi-hop questions. Finally, we demonstrate that it is possible to improve models\u2019 zero-shot multi-hop reasoning capacity through two methods that approximate real multi-hop natural language (NL) questions by training on either concatenation of single-hop questions or logical forms (SPARQL). In sum, these results demonstrate that multi-hop reasoning does not emerge naturally in generative QA models, but can be encouraged by advances in training or modeling techniques. Code is available at https://github.com/jzbjyb/multihop.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Domain Adaptation for Question Answering via Question Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.153/",
        "paper_authors": [
            "Zhenrui Yue",
            "Huimin Zeng",
            "Ziyi Kou",
            "Lanyu Shang",
            "Dong Wang"
        ],
        "paper_abstract": "Question answering (QA) has demonstrated impressive progress in answering questions from customized domains. Nevertheless, domain adaptation remains one of the most elusive challenges for QA systems, especially when QA systems are trained in a source domain but deployed in a different target domain. In this work, we investigate the potential benefits of question classification for QA domain adaptation. We propose a novel framework: Question Classification for Question Answering (QC4QA). Specifically, a question classifier is adopted to assign question classes to both the source and target data. Then, we perform joint training in a self-supervised fashion via pseudo-labeling. For optimization, inter-domain discrepancy between the source and target domain is reduced via maximum mean discrepancy (MMD) distance. We additionally minimize intra-class discrepancy among QA samples of the same question class for fine-grained adaptation performance. To the best of our knowledge, this is the first work in QA domain adaptation to leverage question classification with self-supervised adaptation. We demonstrate the effectiveness of the proposed QC4QA with consistent improvements against the state-of-the-art baselines on multiple datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Prompt-based Conservation Learning for Multi-hop Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.154/",
        "paper_authors": [
            "Zhenyun Deng",
            "Yonghua Zhu",
            "Yang Chen",
            "Qianqian Qi",
            "Michael Witbrock",
            "Patricia Riddle"
        ],
        "paper_abstract": "Multi-hop question answering (QA) requires reasoning over multiple documents to answer a complex question and provide interpretable supporting evidence. However, providing supporting evidence is not enough to demonstrate that a model has performed the desired reasoning to reach the correct answer. Most existing multi-hop QA methods fail to answer a large fraction of sub-questions, even if their parent questions are answered correctly. In this paper, we propose the Prompt-based Conservation Learning (PCL) framework for multi-hop QA, which acquires new knowledge from multi-hop QA tasks while conserving old knowledge learned on single-hop QA tasks, mitigating forgetting. Specifically, we first train a model on existing single-hop QA tasks, and then freeze this model and expand it by allocating additional sub-networks for the multi-hop QA task. Moreover, to condition pre-trained language models to stimulate the kind of reasoning required for specific multi-hop questions, we learn soft prompts for the novel sub-networks to perform type-specific reasoning. Experimental results on the HotpotQA benchmark show that PCL is competitive for multi-hop QA and retains good performance on the corresponding single-hop sub-questions, demonstrating the efficacy of PCL in mitigating knowledge loss by forgetting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GLAF: Global-to-Local Aggregation and Fission Network for Semantic Level Fact Verification",
        "paper_url": "https://aclanthology.org/2022.coling-1.155/",
        "paper_authors": [
            "Zhiyuan Ma",
            "Jianjun Li",
            "Guohui Li",
            "Yongjing Cheng"
        ],
        "paper_abstract": "Accurate fact verification depends on performing fine-grained reasoning over crucial entities by capturing their latent logical relations hidden in multiple evidence clues, which is generally lacking in existing fact verification models. In this work, we propose a novel Global-to-Local Aggregation and Fission network (GLAF) to fill this gap. Instead of treating entire sentences or all semantic elements within them as nodes to construct a coarse-grained or unstructured evidence graph as in previous methods, GLAF constructs a fine-grained and structured evidence graph by parsing the rambling sentences into structural triple-level reasoning clues and regarding them as graph nodes to achieve fine-grained and interpretable evidence graph reasoning. Specifically, to capture latent logical relations between the clues, GLAF first employs a local fission reasoning layer to conduct fine-grained multi-hop reasoning, and then uses a global evidence aggregation layer to achieve information sharing and the interchange of evidence clues for final claim label prediction. Experimental results on the FEVER dataset demonstrate the effectiveness of GLAF, showing that it achieves the state-of-the-art performance by obtaining a 77.62% FEVER score.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Exploiting Hybrid Semantics of Relation Paths for Multi-hop Question Answering over Knowledge Graphs",
        "paper_url": "https://aclanthology.org/2022.coling-1.156/",
        "paper_authors": [
            "Zile Qiao",
            "Wei Ye",
            "Tong Zhang",
            "Tong Mo",
            "Weiping Li",
            "Shikun Zhang"
        ],
        "paper_abstract": "Answering natural language questions on knowledge graphs (KGQA) remains a great challenge in terms of understanding complex questions via multi-hop reasoning. Previous efforts usually exploit large-scale entity-related text corpus or knowledge graph (KG) embeddings as auxiliary information to facilitate answer selection. However, the rich semantics implied in off-the-shelf relation paths between entities is far from well explored. This paper proposes improving multi-hop KGQA by exploiting relation paths\u2019 hybrid semantics. Specifically, we integrate explicit textual information and implicit KG structural features of relation paths based on a novel rotate-and-scale entity link prediction framework. Extensive experiments on three existing KGQA datasets demonstrate the superiority of our method, especially in multi-hop scenarios. Further investigation confirms our method\u2019s systematical coordination between questions and relation paths to identify answer entities.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptive Threshold Selective Self-Attention for Chinese NER",
        "paper_url": "https://aclanthology.org/2022.coling-1.157/",
        "paper_authors": [
            "Biao Hu",
            "Zhen Huang",
            "Minghao Hu",
            "Ziwen Zhang",
            "Yong Dou"
        ],
        "paper_abstract": "Recently, Transformer has achieved great success in Chinese named entity recognition (NER) owing to its good parallelism and ability to model long-range dependencies, which utilizes self-attention to encode context. However, the fully connected way of self-attention may scatter the attention distribution and allow some irrelevant character information to be integrated, leading to entity boundaries being misidentified. In this paper, we propose a data-driven Adaptive Threshold Selective Self-Attention (ATSSA) mechanism that aims to dynamically select the most relevant characters to enhance the Transformer architecture for Chinese NER. In ATSSA, the attention score threshold of each query is automatically generated, and characters with attention score higher than the threshold are selected by the query while others are discarded, so as to address irrelevant attention integration. Experiments on four benchmark Chinese NER datasets show that the proposed ATSSA brings 1.68 average F1 score improvements to the baseline model and achieves state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cluster-aware Pseudo-Labeling for Supervised Open Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.158/",
        "paper_authors": [
            "Bin Duan",
            "Shusen Wang",
            "Xingxian Liu",
            "Yajing Xu"
        ],
        "paper_abstract": "Supervised open relation extraction aims to discover novel relations by leveraging supervised data of pre-defined relations. However, most existing methods do not achieve effective knowledge transfer from pre-defined relations to novel relations, they have difficulties generating high-quality pseudo-labels for unsupervised data of novel relations and usually suffer from the error propagation issue. In this paper, we propose a Cluster-aware Pseudo-Labeling (CaPL) method to improve the pseudo-labels quality and transfer more knowledge for discovering novel relations. Specifically, the model is firstly pre-trained with the pre-defined relations to learn the relation representations. To improve the pseudo-labels quality, the distances between each instance and all cluster centers are used to generate the cluster-aware soft pseudo-labels for novel relations. To mitigate the catastrophic forgetting issue, we design the consistency regularization loss to make better use of the pseudo-labels and jointly train the model with both unsupervised and supervised data. Experimental results on two public datasets demonstrate that our proposed method achieves new state-of-the-arts performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Few-shot Named Entity Recognition with Entity-level Prototypical Network Enhanced by Dispersedly Distributed Prototypes",
        "paper_url": "https://aclanthology.org/2022.coling-1.159/",
        "paper_authors": [
            "Bin Ji",
            "Shasha Li",
            "Shaoduo Gan",
            "Jie Yu",
            "Jun Ma",
            "Huijun Liu",
            "Jing Yang"
        ],
        "paper_abstract": "Few-shot named entity recognition (NER) enables us to build a NER system for a new domain using very few labeled examples. However, existing prototypical networks for this task suffer from roughly estimated label dependency and closely distributed prototypes, thus often causing misclassifications. To address the above issues, we propose EP-Net, an Entity-level Prototypical Network enhanced by dispersedly distributed prototypes. EP-Net builds entity-level prototypes and considers text spans to be candidate entities, so it no longer requires the label dependency. In addition, EP-Net trains the prototypes from scratch to distribute them dispersedly and aligns spans to prototypes in the embedding space using a space projection. Experimental results on two evaluation tasks and the Few-NERD settings demonstrate that EP-Net consistently outperforms the previous strong models in terms of overall performance. Extensive analyses further validate the effectiveness of EP-Net.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Different Data, Different Modalities! Reinforced Data Splitting for Effective Multimodal Information Extraction from Social Media Posts",
        "paper_url": "https://aclanthology.org/2022.coling-1.160/",
        "paper_authors": [
            "Bo Xu",
            "Shizhou Huang",
            "Ming Du",
            "Hongya Wang",
            "Hui Song",
            "Chaofeng Sha",
            "Yanghua Xiao"
        ],
        "paper_abstract": "Recently, multimodal information extraction from social media posts has gained increasing attention in the natural language processing community. Despite their success, current approaches overestimate the significance of images. In this paper, we argue that different social media posts should consider different modalities for multimodal information extraction. Multimodal models cannot always outperform unimodal models. Some posts are more suitable for the multimodal model, while others are more suitable for the unimodal model. Therefore, we propose a general data splitting strategy to divide the social media posts into two sets so that these two sets can achieve better performance under the information extraction models of the corresponding modalities. Specifically, for an information extraction task, we first propose a data discriminator that divides social media posts into a multimodal and a unimodal set. Then we feed these sets into the corresponding models. Finally, we combine the results of these two models to obtain the final extraction results. Due to the lack of explicit knowledge, we use reinforcement learning to train the data discriminator. Experiments on two different multimodal information extraction tasks demonstrate the effectiveness of our method. The source code of this paper can be found in https://github.com/xubodhu/RDS.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Augmentation, Retrieval, Generation: Event Sequence Prediction with a Three-Stage Sequence-to-Sequence Approach",
        "paper_url": "https://aclanthology.org/2022.coling-1.161/",
        "paper_authors": [
            "Bo Zhou",
            "Chenhao Wang",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao",
            "Jiexin Xu",
            "Xiaojian Jiang",
            "Qiuxia Li"
        ],
        "paper_abstract": "Being able to infer possible events related to a specific target is critical to natural language processing. One challenging task in this line is event sequence prediction, which aims at predicting a sequence of events given a goal. Currently existing approach models this task as a statistical induction problem, to predict a sequence of events by exploring the similarity between the given goal and the known sequences of events. However, this statistical based approach is complex and predicts a limited variety of events. At the same time this approach ignores the rich knowledge of external events that is important for predicting event sequences. In this paper, in order to predict more diverse events, we first reformulate the event sequence prediction problem as a sequence generation problem. Then to leverage external event knowledge, we propose a three-stage model including augmentation, retrieval and generation. Experimental results on the event sequence prediction dataset show that our model outperforms existing methods, demonstrating the effectiveness of the proposed model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generating Temporally-ordered Event Sequences via Event Optimal Transport",
        "paper_url": "https://aclanthology.org/2022.coling-1.162/",
        "paper_authors": [
            "Bo Zhou",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao",
            "Jiexin Xu",
            "Xiaojian Jiang",
            "Qiuxia Li"
        ],
        "paper_abstract": "Generating temporally-ordered event sequences in texts is important to natural language processing. Two emerging tasks in this direction are temporal event ordering (rearranging the set of events to correct order) and event infilling (generating an event at a specified position). To tackle the two related tasks, the existing method adopts a vanilla sequence-to-sequence model via maximum likelihood estimation (MLE). However, applying this approach to these tasks will cause two issues. One issue is that the MLE loss emphasizes strict local alignment and ignores the global semantics of the event. The other issue is that the model adopts a word-level objective to model events in texts, failing to evaluate the predicted results of the model from the perspective of event sequence. To alleviate these issues, we present a novel model to tackle the generation of temporally-ordered event sequences via Event Optimal Transport (EOT). First, we treat the events in the sequence as modeling units and explicitly extract the semantics of the events. Second, to provide event sequence-level evaluation of the predicted results of the model, we directly match events in sequences. Extensive experimental results show that our approach outperforms previous models on all evaluation datasets. In particular, the accuracy is improved by 7.7%, and the Macro F1 is improved by 7.2% on one of the datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Continual Relation Extraction through Prototypical Contrastive Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.163/",
        "paper_authors": [
            "Chengwei Hu",
            "Deqing Yang",
            "Haoliang Jin",
            "Zhen Chen",
            "Yanghua Xiao"
        ],
        "paper_abstract": "Continual relation extraction (CRE) aims to extract relations towards the continuous and iterative arrival of new data, of which the major challenge is the catastrophic forgetting of old tasks. In order to alleviate this critical problem for enhanced CRE performance, we propose a novel Continual Relation Extraction framework with Contrastive Learning, namely CRECL, which is built with a classification network and a prototypical contrastive network to achieve the incremental-class learning of CRE. Specifically, in the contrastive network a given instance is contrasted with the prototype of each candidate relations stored in the memory module. Such contrastive learning scheme ensures the data distributions of all tasks more distinguishable, so as to alleviate the catastrophic forgetting further. Our experiment results not only demonstrate our CRECL\u2019s advantage over the state-of-the-art baselines on two public datasets, but also verify the effectiveness of CRECL\u2019s contrastive learning on improving performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Prompt-based Text Entailment for Low-Resource Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.164/",
        "paper_authors": [
            "Dongfang Li",
            "Baotian Hu",
            "Qingcai Chen"
        ],
        "paper_abstract": "Pre-trained Language Models (PLMs) have been applied in NLP tasks and achieve promising results. Nevertheless, the fine-tuning procedure needs labeled data of the target domain, making it difficult to learn in low-resource and non-trivial labeled scenarios. To address these challenges, we propose Prompt-based Text Entailment (PTE) for low-resource named entity recognition, which better leverages knowledge in the PLMs. We first reformulate named entity recognition as the text entailment task. The original sentence with entity type-specific prompts is fed into PLMs to get entailment scores for each candidate. The entity type with the top score is then selected as final label. Then, we inject tagging labels into prompts and treat words as basic units instead of n-gram spans to reduce time complexity in generating candidates by n-grams enumeration. Experimental results demonstrate that the proposed method PTE achieves competitive performance on the CoNLL03 dataset, and better than fine-tuned counterparts on the MIT Movie and Few-NERD dataset in low-resource settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Key Mention Pairs Guided Document-Level Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.165/",
        "paper_authors": [
            "Feng Jiang",
            "Jianwei Niu",
            "Shasha Mo",
            "Shengda Fan"
        ],
        "paper_abstract": "Document-level Relation Extraction (DocRE) aims at extracting relations between entities in a given document. Since different mention pairs may express different relations or even no relation, it is crucial to identify key mention pairs responsible for the entity-level relation labels. However, most recent studies treat different mentions equally while predicting the relations between entities, leading to sub-optimal performance. To this end, we propose a novel DocRE model called Key Mention pairs Guided Relation Extractor (KMGRE) to directly model mention-level relations, containing two modules: a mention-level relation extractor and a key instance classifier. These two modules could be iteratively optimized with an EM-based algorithm to enhance each other. We also propose a new method to solve the multi-label problem in optimizing the mention-level relation extractor. Experimental results on two public DocRE datasets demonstrate that the proposed model is effective and outperforms previous state-of-the-art models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Hybrid Model of Classification and Generation for Spatial Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.166/",
        "paper_authors": [
            "Feng Wang",
            "Peifeng Li",
            "Qiaoming Zhu"
        ],
        "paper_abstract": "Extracting spatial relations from texts is a fundamental task for natural language understanding and previous studies only regard it as a classification task, ignoring those spatial relations with null roles due to their poor information. To address the above issue, we first view spatial relation extraction as a generation task and propose a novel hybrid model HMCGR for this task. HMCGR contains a generation and a classification model, while the former can generate those null-role relations and the latter can extract those non-null-role relations to complement each other. Moreover, a reflexivity evaluation mechanism is applied to further improve the accuracy based on the reflexivity principle of spatial relation. Experimental results on SpaceEval show that HMCGR outperforms the SOTA baselines significantly.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mining Health-related Cause-Effect Statements with High Precision at Large Scale",
        "paper_url": "https://aclanthology.org/2022.coling-1.167/",
        "paper_authors": [
            "Ferdinand Schlatt",
            "Dieter Bettin",
            "Matthias Hagen",
            "Benno Stein",
            "Martin Potthast"
        ],
        "paper_abstract": "An efficient assessment of the health relatedness of text passages is important to mine the web at scale to conduct health sociological analyses or to develop a health search engine. We propose a new efficient and effective termhood score for predicting the health relatedness of phrases and sentences, which achieves 69% recall at over 90% precision on a web dataset with cause-effect statements. It is more effective than state-of-the-art medical entity linkers and as effective but much faster than BERT-based approaches. Using our method, we compile the Webis Medical CauseNet 2022, a new resource of 7.8 million health-related cause-effect statements such as \u201cStudies show that stress induces insomnia\u201d in which the cause (\u2018stress\u2019) and effect (\u2018insomnia\u2019) are labeled.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Find the Funding: Entity Linking with Incomplete Funding Knowledge Bases",
        "paper_url": "https://aclanthology.org/2022.coling-1.168/",
        "paper_authors": [
            "Gizem Aydin",
            "Seyed Amin Tabatabaei",
            "George Tsatsaronis",
            "Faegheh Hasibi"
        ],
        "paper_abstract": "Automatic extraction of funding information from academic articles adds significant value to industry and research communities, including tracking research outcomes by funding organizations, profiling researchers and universities based on the received funding, and supporting open access policies. Two major challenges of identifying and linking funding entities are: (i) sparse graph structure of the Knowledge Base (KB), which makes the commonly used graph-based entity linking approaches suboptimal for the funding domain, (ii) missing entities in KB, which (unlike recent zero-shot approaches) requires marking entity mentions without KB entries as NIL. We propose an entity linking model that can perform NIL prediction and overcome data scarcity issues in a time and data-efficient manner. Our model builds on a transformer-based mention detection and a bi-encoder model to perform entity linking. We show that our model outperforms strong existing baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "KiPT: Knowledge-injected Prompt Tuning for Event Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.169/",
        "paper_authors": [
            "Haochen Li",
            "Tong Mo",
            "Hongcheng Fan",
            "Jingkun Wang",
            "Jiaxi Wang",
            "Fuhao Zhang",
            "Weiping Li"
        ],
        "paper_abstract": "Event detection aims to detect events from the text by identifying and classifying event triggers (the most representative words). Most of the existing works rely heavily on complex downstream networks and require sufficient training data. Thus, those models may be structurally redundant and perform poorly when data is scarce. Prompt-based models are easy to build and are promising for few-shot tasks. However, current prompt-based methods may suffer from low precision because they have not introduced event-related semantic knowledge (e.g., part of speech, semantic correlation, etc.). To address these problems, this paper proposes a Knowledge-injected Prompt Tuning (KiPT) model. Specifically, the event detection task is formulated into a condition generation task. Then, knowledge-injected prompts are constructed using external knowledge bases, and a prompt tuning strategy is leveraged to optimize the prompts. Extensive experiments indicate that KiPT outperforms strong baselines, especially in few-shot scenarios.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.170/",
        "paper_authors": [
            "Hu Cao",
            "Jingye Li",
            "Fangfang Su",
            "Fei Li",
            "Hao Fei",
            "Shengqiong Wu",
            "Bobo Li",
            "Liang Zhao",
            "Donghong Ji"
        ],
        "paper_abstract": "Event extraction (EE) is an essential task of information extraction, which aims to extract structured event information from unstructured text. Most prior work focuses on extracting flat events while neglecting overlapped or nested ones. A few models for overlapped and nested EE includes several successive stages to extract event triggers and arguments,which suffer from error propagation. Therefore, we design a simple yet effective tagging scheme and model to formulate EE as word-word relation recognition, called OneEE. The relations between trigger or argument words are simultaneously recognized in one stage with parallel grid tagging, thus yielding a very fast event extraction speed. The model is equipped with an adaptive event fusion module to generate event-aware representations and a distance-aware predictor to integrate relative distance information for word-word relation recognition, which are empirically demonstrated to be effective mechanisms. Experiments on 3 overlapped and nested EE benchmarks, namely FewFC, Genia11, and Genia13, show that OneEE achieves the state-of-the-art (SOTA) results. Moreover, the inference speed of OneEE is faster than those of baselines in the same condition, and can be further substantially improved since it supports parallel inference.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion",
        "paper_url": "https://aclanthology.org/2022.coling-1.171/",
        "paper_authors": [
            "Jianhao Shen",
            "Chenguang Wang",
            "Linyuan Gong",
            "Dawn Song"
        ],
        "paper_abstract": "The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via fine-tuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a low-resource regime, thanks to the better use of semantics. The code and datasets are available at https://github.com/pkusjh/LASS.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Event Detection with Dual Relational Graph Attention Networks",
        "paper_url": "https://aclanthology.org/2022.coling-1.172/",
        "paper_authors": [
            "Jiaxin Mi",
            "Po Hu",
            "Peng Li"
        ],
        "paper_abstract": "Event detection, which aims to identify instances of specific event types from pieces of text, is a fundamental task in information extraction. Most existing approaches leverage syntactic knowledge with a set of syntactic relations to enhance event detection. However, a side effect of these syntactic-based approaches is that they may confuse different syntactic relations and tend to introduce redundant or noisy information, which may lead to performance degradation. To this end, we propose a simple yet effective model named DualGAT (Dual Relational Graph Attention Networks), which exploits the complementary nature of syntactic and semantic relations to alleviate the problem. Specifically, we first construct a dual relational graph that both aggregates syntactic and semantic relations to the key nodes in the graph, so that event-relevant information can be comprehensively captured from multiple perspectives (i.e., syntactic and semantic views). We then adopt augmented relational graph attention networks to encode the graph and optimize its attention weights by introducing contextual information, which further improves the performance of event detection. Extensive experiments conducted on the standard ACE2005 benchmark dataset indicate that our method significantly outperforms the state-of-the-art methods and verifies the superiority of DualGAT over existing syntactic-based methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Multi-Format Transfer Learning Model for Event Argument Extraction via Variational Information Bottleneck",
        "paper_url": "https://aclanthology.org/2022.coling-1.173/",
        "paper_authors": [
            "Jie Zhou",
            "Qi Zhang",
            "Qin Chen",
            "Qi Zhang",
            "Liang He",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Event argument extraction (EAE) aims to extract arguments with given roles from texts, which have been widely studied in natural language processing. Most previous works have achieved good performance in specific EAE datasets with dedicated neural architectures. Whereas, these architectures are usually difficult to adapt to new datasets/scenarios with various annotation schemas or formats. Furthermore, they rely on large-scale labeled data for training, which is unavailable due to the high labelling cost in most cases. In this paper, we propose a multi-format transfer learning model with variational information bottleneck, which makes use of the information especially the common knowledge in existing datasets for EAE in new datasets. Specifically, we introduce a shared-specific prompt framework to learn both format-shared and format-specific knowledge from datasets with different formats. In order to further absorb the common knowledge for EAE and eliminate the irrelevant noise, we integrate variational information bottleneck into our architecture to refine the shared representation. We conduct extensive experiments on three benchmark datasets, and obtain new state-of-the-art performance on EAE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "RSGT: Relational Structure Guided Temporal Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.174/",
        "paper_authors": [
            "Jie Zhou",
            "Shenpo Dong",
            "Hongkui Tu",
            "Xiaodong Wang",
            "Yong Dou"
        ],
        "paper_abstract": "Temporal relation extraction aims to extract temporal relations between event pairs, which is crucial for natural language understanding. Few efforts have been devoted to capturing the global features. In this paper, we propose RSGT: Relational Structure Guided Temporal Relation Extraction to extract the relational structure features that can fit for both inter-sentence and intra-sentence relations. Specifically, we construct a syntactic-and-semantic-based graph to extract relational structures. Then we present a graph neural network based model to learn the representation of this graph. After that, an auxiliary temporal neighbor prediction task is used to fine-tune the encoder to get more comprehensive node representations. Finally, we apply a conflict detection and correction algorithm to adjust the wrongly predicted labels. Experiments on two well-known datasets, MATRES and TB-Dense, demonstrate the superiority of our method (2.3% F1 improvement on MATRES, 3.5% F1 improvement on TB-Dense).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Hierarchy-Aware Quaternion Knowledge Graph Embeddings with Representing Relations as 3D Rotations",
        "paper_url": "https://aclanthology.org/2022.coling-1.175/",
        "paper_authors": [
            "Jinfa Yang",
            "Xianghua Ying",
            "Yongjie Shi",
            "Xin Tong",
            "Ruibin Wang",
            "Taiyan Chen",
            "Bowei Xing"
        ],
        "paper_abstract": "Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links. It is crucial for knowledge graph embedding models to model and infer various relation patterns, such as symmetry/antisymmetry. However, many existing approaches fail to model semantic hierarchies, which are common in the real world. We propose a new model called HRQE, which represents entities as pure quaternions. The relational embedding consists of two parts: (a) Using unit quaternions to represent the rotation part in 3D space, where the head entities are rotated by the corresponding relations through Hamilton product. (b) Using scale parameters to constrain the modulus of entities to make them have hierarchical distributions. To the best of our knowledge, HRQE is the first model that can encode symmetry/antisymmetry, inversion, composition, multiple relation patterns and learn semantic hierarchies simultaneously. Experimental results demonstrate the effectiveness of HRQE against some of the SOTA methods on four well-established knowledge graph completion benchmarks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Two Languages Are Better than One: Bilingual Enhancement for Chinese Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.176/",
        "paper_authors": [
            "Jinzhong Ning",
            "Zhihao Yang",
            "Zhizheng Wang",
            "Yuanyuan Sun",
            "Hongfei Lin",
            "Jian Wang"
        ],
        "paper_abstract": "Chinese Named Entity Recognition (NER) has continued to attract research attention. However, most existing studies only explore the internal features of the Chinese language but neglect other lingual modal features. Actually, as another modal knowledge of the Chinese language, English contains rich prompts about entities that can potentially be applied to improve the performance of Chinese NER. Therefore, in this study, we explore the bilingual enhancement for Chinese NER and propose a unified bilingual interaction module called the Adapted Cross-Transformers with Global Sparse Attention (ACT-S) to capture the interaction of bilingual information. We utilize a model built upon several different ACT-Ss to integrate the rich English information into the Chinese representation. Moreover, our model can learn the interaction of information between bilinguals (inter-features) and the dependency information within Chinese (intra-features). Compared with existing Chinese NER methods, our proposed model can better handle entities with complex structures. The English text that enhances the model is automatically generated by machine translation, avoiding high labour costs. Experimental results on four well-known benchmark datasets demonstrate the effectiveness and robustness of our proposed model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Read Extensively, Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents NER",
        "paper_url": "https://aclanthology.org/2022.coling-1.177/",
        "paper_authors": [
            "Jun Zhao",
            "Xin Zhao",
            "WenYu Zhan",
            "Tao Gui",
            "Qi Zhang",
            "Liang Qiao",
            "Zhanzhan Cheng",
            "Shiliang Pu"
        ],
        "paper_abstract": "The introduction of multimodal information and pretraining technique significantly improves entity recognition from visually-rich documents. However, most of the existing methods pay unnecessary attention to irrelevant regions of the current document while ignoring the potentially valuable information in related documents. To deal with this problem, this work proposes a cross-document semantic enhancement method, which consists of two modules: 1) To prevent distractions from irrelevant regions in the current document, we design a learnable attention mask mechanism, which is used to adaptively filter redundant information in the current document. 2) To further enrich the entity-related context, we propose a cross-document information awareness technique, which enables the model to collect more evidence across documents to assist in prediction. The experimental results on two documents understanding benchmarks covering eight languages demonstrate that our method outperforms the SOTA methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "STAD: Self-Training with Ambiguous Data for Low-Resource Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.178/",
        "paper_authors": [
            "Junjie Yu",
            "Xing Wang",
            "Jiangjiang Zhao",
            "Chunjie Yang",
            "Wenliang Chen"
        ],
        "paper_abstract": "We present a simple yet effective self-training approach, named as STAD, for low-resource relation extraction. The approach first classifies the auto-annotated instances into two groups: confident instances and uncertain instances, according to the probabilities predicted by a teacher model. In contrast to most previous studies, which mainly only use the confident instances for self-training, we make use of the uncertain instances. To this end, we propose a method to identify ambiguous but useful instances from the uncertain instances and then divide the relations into candidate-label set and negative-label set for each ambiguous instance. Next, we propose a set-negative training method on the negative-label sets for the ambiguous instances and a positive training method for the confident instances. Finally, a joint-training method is proposed to build the final relation extraction system on all data. Experimental results on two widely used datasets SemEval2010 Task-8 and Re-TACRED with low-resource settings demonstrate that this new self-training approach indeed achieves significant and consistent improvements when comparing to several competitive self-training systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Flat Multi-modal Interaction Transformer for Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.179/",
        "paper_authors": [
            "Junyu Lu",
            "Dixiang Zhang",
            "Jiaxing Zhang",
            "Pingjian Zhang"
        ],
        "paper_abstract": "Multi-modal named entity recognition (MNER) aims at identifying entity spans and recognizing their categories in social media posts with the aid of images. However, in dominant MNER approaches, the interaction of different modalities is usually carried out through the alternation of self-attention and cross-attention or over-reliance on the gating machine, which results in imprecise and biased correspondence between fine-grained semantic units of text and image. To address this issue, we propose a Flat Multi-modal Interaction Transformer (FMIT) for MNER. Specifically, we first utilize noun phrases in sentences and general domain words to obtain visual cues. Then, we transform the fine-grained semantic representation of the vision and text into a unified lattice structure and design a novel relative position encoding to match different modalities in Transformer. Meanwhile, we propose to leverage entity boundary detection as an auxiliary task to alleviate visual bias. Experiments show that our methods achieve the new state-of-the-art performance on two benchmark datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MetaSLRCL: A Self-Adaptive Learning Rate and Curriculum Learning Based Framework for Few-Shot Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.180/",
        "paper_authors": [
            "Kailin Zhao",
            "Xiaolong Jin",
            "Saiping Guan",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "paper_abstract": "Due to the lack of labeled data in many realistic scenarios, a number of few-shot learning methods for text classification have been proposed, among which the meta learning based ones have recently attracted much attention. Such methods usually consist of a learner as the classifier and a meta learner for specializing the learner to different tasks. For the learner, learning rate is crucial to its performance. However, existing methods treat it as a hyper parameter and adjust it manually, which is time-consuming and laborious. Intuitively, for different tasks and neural network layers, the learning rates should be different and self-adaptive. For the meta learner, it requires a good generalization ability so as to quickly adapt to new tasks. Motivated by these issues, we propose a novel meta learning framework, called MetaSLRCL, for few-shot text classification. Specifically, we present a novel meta learning mechanism to obtain different learning rates for different tasks and neural network layers so as to enable the learner to quickly adapt to new training data. Moreover, we propose a task-oriented curriculum learning mechanism to help the meta learner achieve a better generalization ability by learning from different tasks with increasing difficulties. Extensive experiments on three benchmark datasets demonstrate the effectiveness of MetaSLRCL.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Simple Temporal Information Matching Mechanism for Entity Alignment between Temporal Knowledge Graphs",
        "paper_url": "https://aclanthology.org/2022.coling-1.181/",
        "paper_authors": [
            "Li Cai",
            "Xin Mao",
            "Meirong Ma",
            "Hao Yuan",
            "Jianchao Zhu",
            "Man Lan"
        ],
        "paper_abstract": "Entity alignment (EA) aims to find entities in different knowledge graphs (KGs) that refer to the same object in the real world. Recent studies incorporate temporal information to augment the representations of KGs. The existing methods for EA between temporal KGs (TKGs) utilize a time-aware attention mechanisms to incorporate relational and temporal information into entity embeddings. The approaches outperform the previous methods by using temporal information. However, we believe that it is not necessary to learn the embeddings of temporal information in KGs since most TKGs have uniform temporal representations. Therefore, we propose a simple GNN model combined with a temporal information matching mechanism, which achieves better performance with less time and fewer parameters. Furthermore, since alignment seeds are difficult to label in real-world applications, we also propose a method to generate unsupervised alignment seeds via the temporal information of TKG. Extensive experiments on public datasets indicate that our supervised method significantly outperforms the previous methods and the unsupervised one has competitive performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DCT-Centered Temporal Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.182/",
        "paper_authors": [
            "Liang Wang",
            "Peifeng Li",
            "Sheng Xu"
        ],
        "paper_abstract": "Most previous work on temporal relation extraction only focused on extracting the temporal relations among events or suffered from the issue of different expressions of events, timexes and Document Creation Time (DCT). Moreover, DCT can act as a hub to semantically connect the other events and timexes in a document. Unfortunately, previous work cannot benefit from such critical information. To address the above issues, we propose a unified DCT-centered Temporal Relation Extraction model DTRE to identify the relations among events, timexes and DCT. Specifically, sentence-style DCT representation is introduced to address the first issue and unify event expressions, timexes and DCT. Then, a DCT-aware graph is applied to obtain their contextual structural representations. Furthermore, a DCT-anchoring multi-task learning framework is proposed to jointly predict three types of temporal relations in a batch. Finally, we apply a DCT-guided global inference to further enhance the global consistency among different relations. Experimental results on three datasets show that our DTRE outperforms several SOTA baselines on E-E, E-T and E-D significantly.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Document-level Biomedical Relation Extraction Based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning",
        "paper_url": "https://aclanthology.org/2022.coling-1.183/",
        "paper_authors": [
            "Lishuang Li",
            "Ruiyuan Lian",
            "Hongbin Lu",
            "Jingyao Tang"
        ],
        "paper_abstract": "Document-level biomedical relation extraction (Bio-DocuRE) is an important branch of biomedical text mining that aims to automatically extract all relation facts from the biomedical text. Since there are a considerable number of relations in biomedical documents that need to be judged by other existing relations, logical reasoning has become a research hotspot in the past two years. However, current models with reasoning are single-granularity only based on one element information, ignoring the complementary fact of different granularity reasoning information. In addition, obtaining rich document information is a prerequisite for logical reasoning, but most of the previous models cannot sufficiently utilize document information, which limits the reasoning ability of the model. In this paper, we propose a novel Bio-DocuRE model called FILR, based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning. Specifically, FILR presents a multi-dimensional information fusion module MDIF to extract sufficient global document information. Then FILR proposes a multi-granularity reasoning module MGLR to obtain rich inference information through the reasoning of both entity-pairs and mention-pairs. We evaluate our FILR model on two widely used biomedical corpora CDR and GDA. Experimental results show that FILR achieves state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Simple Yet Powerful: An Overlooked Architecture for Nested Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.184/",
        "paper_authors": [
            "Matias Rojas",
            "Felipe Bravo-Marquez",
            "Jocelyn Dunstan"
        ],
        "paper_abstract": "Named Entity Recognition (NER) is an important task in Natural Language Processing that aims to identify text spans belonging to predefined categories. Traditional NER systems ignore nested entities, which are entities contained in other entity mentions. Although several methods have been proposed to address this case, most of them rely on complex task-specific structures and ignore potentially useful baselines for the task. We argue that this creates an overly optimistic impression of their performance. This paper revisits the Multiple LSTM-CRF (MLC) model, a simple, overlooked, yet powerful approach based on training independent sequence labeling models for each entity type. Extensive experiments with three nested NER corpora show that, regardless of the simplicity of this model, its performance is better or at least as well as more sophisticated methods. Furthermore, we show that the MLC architecture achieves state-of-the-art results in the Chilean Waiting List corpus by including pre-trained language models. In addition, we implemented an open-source library that computes task-specific metrics for nested NER. The results suggest that metrics used in previous work do not measure well the ability of a model to detect nested entities, while our metrics provide new evidence on how existing approaches handle the task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification",
        "paper_url": "https://aclanthology.org/2022.coling-1.185/",
        "paper_authors": [
            "Meiqi Chen",
            "Yixin Cao",
            "Kunquan Deng",
            "Mukai Li",
            "Kun Wang",
            "Jing Shao",
            "Yan Zhang"
        ],
        "paper_abstract": "Document-level Event Causality Identification (DECI) aims to identify event-event causal relations in a document. Existing works usually build an event graph for global reasoning across multiple sentences. However, the edges between events have to be carefully designed through heuristic rules or external tools. In this paper, we propose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI, to ease the graph construction and improve it over the noisy edge issue. Different from conventional event graphs, we define a pair of events as a node and build a complete event relational graph without any prior knowledge or tools. This naturally formulates DECI as a node classification problem, and thus we capture the causation transitivity among event pairs via a graph transformer. Furthermore, we design a criss-cross constraint and an adaptive focal loss for the imbalanced classification, to alleviate the issues of false positives and false negatives. Extensive experiments on two benchmark datasets show that ERGO greatly outperforms previous state-of-the-art (SOTA) methods (12.8% F1 gains on average).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DRK: Discriminative Rule-based Knowledge for Relieving Prediction Confusions in Few-shot Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.186/",
        "paper_authors": [
            "Mengru Wang",
            "Jianming Zheng",
            "Fei Cai",
            "Taihua Shao",
            "Honghui Chen"
        ],
        "paper_abstract": "Few-shot relation extraction aims to identify the relation type between entities in a given text in the low-resource scenario. Albeit much progress, existing meta-learning methods still fall into prediction confusions owing to the limited inference ability over shallow text features. To relieve these confusions, this paper proposes a discriminative rule-based knowledge (DRK) method. Specifically, DRK adopts a logic-aware inference module to ease the word-overlap confusion, which introduces a logic rule to constrain the inference process, thereby avoiding the adverse effect of shallow text features. Also, DRK employs a discrimination finding module to alleviate the entity-type confusion, which explores distinguishable text features via a hierarchical contrastive learning. We conduct extensive experiments on four types of meta tasks and the results show promising improvements from DRK (6.0% accuracy gains on average). Besides, error analyses reveal the word-overlap and entity-type errors are the main courses of mispredictions in few-shot relation extraction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DocQueryNet: Value Retrieval with Arbitrary Queries for Form-like Documents",
        "paper_url": "https://aclanthology.org/2022.coling-1.187/",
        "paper_authors": [
            "Mingfei Gao",
            "Le Xue",
            "Chetan Ramaiah",
            "Chen Xing",
            "Ran Xu",
            "Caiming Xiong"
        ],
        "paper_abstract": "We propose, DocQueryNet, a value retrieval method with arbitrary queries for form-like documents to reduce human effort of processing forms. Unlike previous methods that only address a fixed set of field items, our method predicts target value for an arbitrary query based on the understanding of the layout and semantics of a form. To further boost model performance, we propose a simple document language modeling (SimpleDLM) strategy to improve document understanding on large-scale model pre-training. Experimental results show that DocQueryNet outperforms previous designs significantly and the SimpleDLM further improves our performance on value retrieval by around 17% F1 score compared with the state-of-the-art pre-training method. Code is available here, https://github.com/salesforce/QVR-SimpleDLM.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DoSEA: A Domain-specific Entity-aware Framework for Cross-Domain Named Entity Recogition",
        "paper_url": "https://aclanthology.org/2022.coling-1.188/",
        "paper_authors": [
            "Minghao Tang",
            "Peng Zhang",
            "Yongquan He",
            "Yongxiu Xu",
            "Chengpeng Chao",
            "Hongbo Xu"
        ],
        "paper_abstract": "Cross-domain named entity recognition aims to improve performance in a target domain with shared knowledge from a well-studied source domain. The previous sequence-labeling based method focuses on promoting model parameter sharing among domains. However, such a paradigm essentially ignores the domain-specific information and suffers from entity type conflicts. To address these issues, we propose a novel machine reading comprehension based framework, named DoSEA, which can identify domain-specific semantic differences and mitigate the subtype conflicts between domains. Concretely, we introduce an entity existence discrimination task and an entity-aware training setting, to recognize inconsistent entity annotations in the source domain and bring additional reference to better share information across domains. Experiments on six datasets prove the effectiveness of our DoSEA. Our source code can be obtained from https://github.com/mhtang1995/DoSEA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.189/",
        "paper_authors": [
            "Minqian Liu",
            "Shiyu Chang",
            "Lifu Huang"
        ],
        "paper_abstract": "Lifelong event detection aims to incrementally update a model with new event types and data while retaining the capability on previously learned old types. One critical challenge is that the model would catastrophically forget old types when continually trained on new data. In this paper, we introduce Episodic Memory Prompts (EMP) to explicitly retain the learned task-specific knowledge. Our method adopts continuous prompt for each task and they are optimized to instruct the model prediction and learn event-specific representation. The EMPs learned in previous tasks are carried along with the model in subsequent tasks, and can serve as a memory module that keeps the old knowledge and transferring to new tasks. Experiment results demonstrate the effectiveness of our method. Furthermore, we also conduct a comprehensive analysis of the new and old event types in lifelong learning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect",
        "paper_url": "https://aclanthology.org/2022.coling-1.190/",
        "paper_authors": [
            "Naihao Deng",
            "Yulong Chen",
            "Yue Zhang"
        ],
        "paper_abstract": "Text-to-SQL has attracted attention from both the natural language processing and database communities because of its ability to convert the semantics in natural language into SQL queries and its practical application in building natural language interfaces to database systems. The major challenges in text-to-SQL lie in encoding the meaning of natural utterances, decoding to SQL queries, and translating the semantics between these two forms. These challenges have been addressed to different extents by the recent advances. However, there is still a lack of comprehensive surveys for this task. To this end, we review recent progress on text-to-SQL for datasets, methods, and evaluation and provide this systematic survey, addressing the aforementioned challenges and discussing potential future directions. We hope this survey can serve as quick access to existing work and motivate future research.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An MRC Framework for Semantic Role Labeling",
        "paper_url": "https://aclanthology.org/2022.coling-1.191/",
        "paper_authors": [
            "Nan Wang",
            "Jiwei Li",
            "Yuxian Meng",
            "Xiaofei Sun",
            "Han Qiu",
            "Ziyao Wang",
            "Guoyin Wang",
            "Jun He"
        ],
        "paper_abstract": "Semantic Role Labeling (SRL) aims at recognizing the predicate-argument structure of a sentence and can be decomposed into two subtasks: predicate disambiguation and argument labeling. Prior work deals with these two tasks independently, which ignores the semantic connection between the two tasks. In this paper, we propose to use the machine reading comprehension (MRC) framework to bridge this gap. We formalize predicate disambiguation as multiple-choice machine reading comprehension, where the descriptions of candidate senses of a given predicate are used as options to select the correct sense. The chosen predicate sense is then used to determine the semantic roles for that predicate, and these semantic roles are used to construct the query for another MRC model for argument labeling. In this way, we are able to leverage both the predicate semantics and the semantic role semantics for argument labeling. We also propose to select a subset of all the possible semantic roles for computational efficiency. Experiments show that the proposed framework achieves state-of-the-art or comparable results to previous work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PCBERT: Parent and Child BERT for Chinese Few-shot NER",
        "paper_url": "https://aclanthology.org/2022.coling-1.192/",
        "paper_authors": [
            "Peichao Lai",
            "Feiyang Ye",
            "Lin Zhang",
            "Zhiwei Chen",
            "Yanggeng Fu",
            "Yingjie Wu",
            "Yilei Wang"
        ],
        "paper_abstract": "Achieving good performance on few-shot or zero-shot datasets has been a long-term challenge for NER. The conventional semantic transfer approaches on NER will decrease model performance when the semantic distribution is quite different, especially in Chinese few-shot NER. Recently, prompt-tuning has been thoroughly considered for low-resource tasks. But there is no effective prompt-tuning approach for Chinese few-shot NER. In this work, we propose a prompt-based Parent and Child BERT (PCBERT) for Chinese few-shot NER. To train an annotating model on high-resource datasets and then discover more implicit labels on low-resource datasets. We further design a label extension strategy to achieve label transferring from high-resource datasets. We evaluated our model on Weibo and the other three sampling Chinese NER datasets, and the experimental result demonstrates our approach\u2019s effectiveness in few-shot learning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Label Smoothing for Text Mining",
        "paper_url": "https://aclanthology.org/2022.coling-1.193/",
        "paper_authors": [
            "Peiyang Liu",
            "Xiangyu Xi",
            "Wei Ye",
            "Shikun Zhang"
        ],
        "paper_abstract": "Current text mining models are trained with 0-1 hard label that indicates whether an instance belongs to a class, ignoring rich information of the relevance degree. Soft label, which involved each label of varying degrees than the hard label, is considered more suitable for describing instances. The process of generating soft labels from hard labels is defined as label smoothing (LS). Classical LS methods focus on universal data mining tasks so that they ignore the valuable text features in text mining tasks. This paper presents a novel keyword-based LS method to automatically generate soft labels from hard labels via exploiting the relevance between labels and text instances. Generated soft labels are then incorporated into existing models as auxiliary targets during the training stage, capable of improving models without adding any extra parameters. Results of extensive experiments on text classification and large-scale text retrieval datasets demonstrate that soft labels generated by our method contain rich knowledge of text features, improving the performance of corresponding models under both balanced and unbalanced settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Diverse Multi-Answer Retrieval with Determinantal Point Processes",
        "paper_url": "https://aclanthology.org/2022.coling-1.194/",
        "paper_authors": [
            "Poojitha Nandigam",
            "Nikhil Rayaprolu",
            "Manish Shrivastava"
        ],
        "paper_abstract": "Often questions provided to open-domain question answering systems are ambiguous. Traditional QA systems that provide a single answer are incapable of answering ambiguous questions since the question may be interpreted in several ways and may have multiple distinct answers. In this paper, we address multi-answer retrieval which entails retrieving passages that can capture majority of the diverse answers to the question. We propose a re-ranking based approach using Determinantal point processes utilizing BERT as kernels. Our method jointly considers query-passage relevance and passage-passage correlation to retrieve passages that are both query-relevant and diverse. Results demonstrate that our re-ranking technique outperforms state-of-the-art method on the AmbigQA dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Deep Embedded Clustering via Learning Cluster-level Representations",
        "paper_url": "https://aclanthology.org/2022.coling-1.195/",
        "paper_authors": [
            "Qing Yin",
            "Zhihua Wang",
            "Yunya Song",
            "Yida Xu",
            "Shuai Niu",
            "Liang Bai",
            "Yike Guo",
            "Xian Yang"
        ],
        "paper_abstract": "Driven by recent advances in neural networks, various Deep Embedding Clustering (DEC) based short text clustering models are being developed. In these works, latent representation learning and text clustering are performed simultaneously. Although these methods are becoming increasingly popular, they use pure cluster-oriented objectives, which can produce meaningless representations. To alleviate this problem, several improvements have been developed to introduce additional learning objectives in the clustering process, such as models based on contrastive learning. However, existing efforts rely heavily on learning meaningful representations at the instance level. They have limited focus on learning global representations, which are necessary to capture the overall data structure at the cluster level. In this paper, we propose a novel DEC model, which we named the deep embedded clustering model with cluster-level representation learning (DECCRL) to jointly learn cluster and instance level representations. Here, we extend the embedded topic modelling approach to introduce reconstruction constraints to help learn cluster-level representations. Experimental results on real-world short text datasets demonstrate that our model produces meaningful clusters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Decoupling Mixture-of-Graphs: Unseen Relational Learning for Knowledge Graph Completion by Fusing Ontology and Textual Experts",
        "paper_url": "https://aclanthology.org/2022.coling-1.196/",
        "paper_authors": [
            "Ran Song",
            "Shizhu He",
            "Suncong Zheng",
            "Shengxiang Gao",
            "Kang Liu",
            "Zhengtao Yu",
            "Jun Zhao"
        ],
        "paper_abstract": "Knowledge Graph Embedding (KGE) has been proposed and successfully utilized to knowledge Graph Completion (KGC). But classic KGE paradigm often fail in unseen relation representations. Previous studies mainly utilize the textual descriptions of relations and its neighbor relations to represent unseen relations. In fact, the semantics of a relation can be expressed by three kinds of graphs: factual graph, ontology graph, textual description graph, and they can complement each other. A more common scenario in the real world is that seen and unseen relations appear at the same time. In this setting, the training set (only seen relations) and testing set (both seen and unseen relations) own different distributions. And the train-test inconsistency problem will make KGE methods easiy overfit on seen relations and under-performance on unseen relations. In this paper, we propose decoupling mixture-of-graph experts (DMoG) for unseen relations learning, which could represent the unseen relations in the factual graph by fusing ontology and textual graphs, and decouple fusing space and reasoning space to alleviate overfitting for seen relations. The experiments on two unseen only public datasets and a mixture dataset verify the effectiveness of the proposed method, which improves the state-of-the-art methods by 6.84% in Hits@10 on average.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CETA: A Consensus Enhanced Training Approach for Denoising in Distantly Supervised Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.197/",
        "paper_authors": [
            "Ruri Liu",
            "Shasha Mo",
            "Jianwei Niu",
            "Shengda Fan"
        ],
        "paper_abstract": "Distantly supervised relation extraction aims to extract relational facts from texts but suffers from noisy instances. Existing methods usually select reliable sentences that rely on potential noisy labels, resulting in wrongly selecting many noisy training instances or underutilizing a large amount of valuable training data. This paper proposes a sentence-level DSRE method beyond typical instance selection approaches by preventing samples from falling into the wrong classification space on the feature space. Specifically, a theorem for denoising and the corresponding implementation, named Consensus Enhanced Training Approach (CETA), are proposed in this paper. By training the model with CETA, samples of different classes are separated, and samples of the same class are closely clustered in the feature space. Thus the model can easily establish the robust classification boundary to prevent noisy labels from biasing wrongly labeled samples into the wrong classification space. This process is achieved by enhancing the classification consensus between two discrepant classifiers and does not depend on any potential noisy labels, thus avoiding the above two limitations. Extensive experiments on widely-used benchmarks have demonstrated that CETA significantly outperforms the previous methods and achieves new state-of-the-art results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.198/",
        "paper_authors": [
            "Saadullah Amin",
            "Pasquale Minervini",
            "David Chang",
            "Pontus Stenetorp",
            "Guenter Neumann"
        ],
        "paper_abstract": "Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation extraction benchmarks and found a significant overlap between training and test relationships ranging from 26% to 86%. Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction findings to biomedical relation extraction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious Correlations from a Feature Perspective",
        "paper_url": "https://aclanthology.org/2022.coling-1.199/",
        "paper_authors": [
            "Shihan Dou",
            "Rui Zheng",
            "Ting Wu",
            "SongYang Gao",
            "Junjie Shan",
            "Qi Zhang",
            "Yueming Wu",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Natural language understanding (NLU) models tend to rely on spurious correlations (i.e., dataset bias) to achieve high performance on in-distribution datasets but poor performance on out-of-distribution ones. Most of the existing debiasing methods often identify and weaken these samples with biased features (i.e., superficial surface features that cause such spurious correlations). However, down-weighting these samples obstructs the model in learning from the non-biased parts of these samples. To tackle this challenge, in this paper, we propose to eliminate spurious correlations in a fine-grained manner from a feature space perspective. Specifically, we introduce Random Fourier Features and weighted re-sampling to decorrelate the dependencies between features to mitigate spurious correlations. After obtaining decorrelated features, we further design a mutual-information-based method to purify them, which forces the model to learn features that are more relevant to tasks. Extensive experiments on two well-studied NLU tasks demonstrate that our method is superior to other comparative approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Event Causality Identification via Derivative Prompt Joint Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.200/",
        "paper_authors": [
            "Shirong Shen",
            "Heng Zhou",
            "Tongtong Wu",
            "Guilin Qi"
        ],
        "paper_abstract": "This paper studies event causality identification, which aims at predicting the causality relation for a pair of events in a sentence. Regarding event causality identification as a supervised classification task, most existing methods suffer from the problem of insufficient annotated data. In this paper, we propose a new derivative prompt joint learning model for event causality identification, which leverages potential causal knowledge in the pre-trained language model to tackle the data scarcity problem. Specifically, rather than external data or knowledge augmentation, we derive two relevant prompt tasks from event causality identification to enhance the model\u2019s ability to identify explicit and implicit causality. We evaluate our model on two benchmark datasets and the results show that our model has great advantages over previous methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Event Causality Extraction with Event Argument Correlations",
        "paper_url": "https://aclanthology.org/2022.coling-1.201/",
        "paper_authors": [
            "Shiyao Cui",
            "Jiawei Sheng",
            "Xin Cong",
            "Quangang Li",
            "Tingwen Liu",
            "Jinqiao Shi"
        ],
        "paper_abstract": "Event Causality Identification (ECI), which aims to detect whether a causality relation exists between two given textual events, is an important task for event causality understanding. However, the ECI task ignores crucial event structure and cause-effect causality component information, making it struggle for downstream applications. In this paper, we introduce a novel task, namely Event Causality Extraction (ECE), aiming to extract the cause-effect event causality pairs with their structured event information from plain texts. The ECE task is more challenging since each event can contain multiple event arguments, posing fine-grained correlations between events to decide the cause-effect event pair. Hence, we propose a method with a dual grid tagging scheme to capture the intra- and inter-event argument correlations for ECE. Further, we devise a event type-enhanced model architecture to realize the dual grid tagging scheme. Experiments demonstrate the effectiveness of our method, and extensive analyses point out several future directions for ECE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER",
        "paper_url": "https://aclanthology.org/2022.coling-1.202/",
        "paper_authors": [
            "Shuzheng Si",
            "Shuang Zeng",
            "Jiaxing Lin",
            "Baobao Chang"
        ],
        "paper_abstract": "Unlabeled Entity Problem (UEP) in Named Entity Recognition (NER) datasets seriously hinders the improvement of NER performance. This paper proposes SCL-RAI to cope with this problem. Firstly, we decrease the distance of span representations with the same label while increasing it for different ones via span-based contrastive learning, which relieves the ambiguity among entities and improves the robustness of the model over unlabeled entities. Then we propose retrieval augmented inference to mitigate the decision boundary shifting problem. Our method significantly outperforms the previous SOTA method by 4.21% and 8.64% F1-score on two real-world datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Relation Extraction Dataset for Knowledge Extraction from Web Tables",
        "paper_url": "https://aclanthology.org/2022.coling-1.203/",
        "paper_authors": [
            "Siffi Singh",
            "Alham Fikri Aji",
            "Gaurav Singh",
            "Christos Christodoulopoulos"
        ],
        "paper_abstract": "Relational web-tables are significant sources of structural information that are widely used for relation extraction and population of facts into knowledge graphs. To transform the web-table data into knowledge, we need to identify the relations that exist between column pairs. Currently, there are only a handful of publicly available datasets with relations annotated against natural web-tables. Most datasets are constructed using synthetic tables that lack valuable metadata information, or are limited in size to be considered as a challenging evaluation set. In this paper, we present REDTab, the largest natural-table relation extraction dataset. We have annotated ~9K tables and ~22K column pairs using crowd sourced annotators from MTurk, which has 50x larger number of column pairs than the existing human-annotated benchmark. Our test set is specially designed to be challenging as observed in our experiment results using TaBERT. We publicly release REDTab as a benchmark for the evaluation process in relation extraction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automatic Keyphrase Generation by Incorporating Dual Copy Mechanisms in Sequence-to-Sequence Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.204/",
        "paper_authors": [
            "Siyu Wang",
            "Jianhui Jiang",
            "Yao Huang",
            "Yin Wang"
        ],
        "paper_abstract": "The keyphrase generation task is a challenging work that aims to generate a set of keyphrases for a piece of text. Many previous studies based on the sequence-to-sequence model were used to generate keyphrases, and they introduce a copy mechanism to achieve good results. However, we observed that most of the keyphrases are composed of some important words (seed words) in the source text, and if these words can be identified accurately and copied to create more keyphrases, the performance of the model might be improved. To address this challenge, we propose a DualCopyNet model, which introduces an additional sequence labeling layer for identifying seed words, and further copies the words for generating new keyphrases by dual copy mechanisms. Experimental results demonstrate that our model outperforms the baseline models and achieves an obvious performance improvement.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dependency-aware Prototype Learning for Few-shot Relation Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.205/",
        "paper_authors": [
            "Tianshu Yu",
            "Min Yang",
            "Xiaoyan Zhao"
        ],
        "paper_abstract": "Few-shot relation classification aims to classify the relation type between two given entities in a sentence by training with a few labeled instances for each relation. However, most of existing models fail to distinguish multiple relations that co-exist in one sentence. This paper presents a novel dependency-aware prototype learning (DAPL) method for few-shot relation classification. Concretely, we utilize dependency trees and shortest dependency paths (SDP) as structural information to complement the contextualized representations of input sentences by using the dependency-aware embedding as attention inputs to learn attentive sentence representations. In addition, we introduce a gate controlled update mechanism to update the dependency-aware representations according to the output of each network layer. Extensive experiments on the FewRel dataset show that DAPL achieves substantially better performance than strong baselines. For reproducibility, we will release our code and data upon the publication of this paper at https://github.com/publicstaticvo/DAPL.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MECI: A Multilingual Dataset for Event Causality Identification",
        "paper_url": "https://aclanthology.org/2022.coling-1.206/",
        "paper_authors": [
            "Viet Dac Lai",
            "Amir Pouran Ben Veyseh",
            "Minh Van Nguyen",
            "Franck Dernoncourt",
            "Thien Huu Nguyen"
        ],
        "paper_abstract": "Event Causality Identification (ECI) is the task of detecting causal relations between events mentioned in the text. Although this task has been extensively studied for English materials, it is under-explored for many other languages. A major reason for this issue is the lack of multilingual datasets that provide consistent annotations for event causality relations in multiple non-English languages. To address this issue, we introduce a new multilingual dataset for ECI, called MECI. The dataset employs consistent annotation guidelines for five typologically different languages, i.e., English, Danish, Spanish, Turkish, and Urdu. Our dataset thus enable a new research direction on cross-lingual transfer learning for ECI. Our extensive experiments demonstrate high quality for MECI that can provide ample research challenges and directions for future research. We will publicly release MECI to promote research on multilingual ECI.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Method Entity Extraction from Biomedical Texts",
        "paper_url": "https://aclanthology.org/2022.coling-1.207/",
        "paper_authors": [
            "Waqar Bin Kalim",
            "Robert E. Mercer"
        ],
        "paper_abstract": "In the field of Natural Language Processing (NLP), extracting method entities from biomedical text has been a challenging task. Scientific research papers commonly consist of complex keywords and domain-specific terminologies, and new terminologies are continuously appearing. In this research, we find method terminologies in biomedical text using both rule-based and machine learning techniques. We first use linguistic features to extract method sentence candidates from a large corpus of biomedical text. Then, we construct a silver standard biomedical corpus composed of these sentences. With a rule-based method that makes use of the Stanza dependency parsing module, we label the method entities in these sentences. Using this silver standard corpus we train two machine learning algorithms to automatically extract method entities from biomedical text. Our results show that it is possible to develop machine learning models that can automatically extract method entities to a reasonable accuracy without the need for a gold standard dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Optimal Partial Transport Based Sentence Selection for Long-form Document Matching",
        "paper_url": "https://aclanthology.org/2022.coling-1.208/",
        "paper_authors": [
            "Weijie Yu",
            "Liang Pang",
            "Jun Xu",
            "Bing Su",
            "Zhenhua Dong",
            "Ji-Rong Wen"
        ],
        "paper_abstract": "One typical approach to long-form document matching is first conducting alignment between cross-document sentence pairs, and then aggregating all of the sentence-level matching signals. However, this approach could be problematic because the alignment between documents is partial \u2014 despite two documents as a whole are well-matched, most of the sentences could still be dissimilar. Those dissimilar sentences lead to spurious sentence-level matching signals which may overwhelm the real ones, increasing the difficulties of learning the matching function. Therefore, accurately selecting the key sentences for document matching is becoming a challenging issue. To address the issue, we propose a novel matching approach that equips existing document matching models with an Optimal Partial Transport (OPT) based component, namely OPT-Match, which selects the sentences that play a major role in matching. Enjoying the partial transport properties of OPT, the selected key sentences can not only effectively enhance the matching accuracy, but also be explained as the rationales for the matching results. Extensive experiments on four publicly available datasets demonstrated that existing methods equipped with OPT-Match consistently outperformed the corresponding underlying methods. Evaluations also showed that the key sentences selected by OPT-Match were consistent with human-provided rationales.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting",
        "paper_url": "https://aclanthology.org/2022.coling-1.209/",
        "paper_authors": [
            "Xiang Chen",
            "Lei Li",
            "Shumin Deng",
            "Chuanqi Tan",
            "Changliang Xu",
            "Fei Huang",
            "Luo Si",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "paper_abstract": "Most NER methods rely on extensive labeled data for model training, which struggles in the low-resource scenarios with limited training data. Existing dominant approaches usually suffer from the challenge that the target domain has different label sets compared with a resource-rich source domain, which can be concluded as class transfer and domain transfer. In this paper, we propose a lightweight tuning paradigm for low-resource NER via pluggable prompting (LightNER). Specifically, we construct the unified learnable verbalizer of entity categories to generate the entity span sequence and entity categories without any label-specific classifiers, thus addressing the class transfer issue. We further propose a pluggable guidance module by incorporating learnable parameters into the self-attention layer as guidance, which can re-modulate the attention and adapt pre-trained weights. Note that we only tune those inserted module with the whole parameter of the pre-trained language model fixed, thus, making our approach lightweight and flexible for low-resource scenarios and can better transfer knowledge across domains. Experimental results show that LightNER can obtain comparable performance in the standard supervised setting and outperform strong baselines in low-resource settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-modal Contrastive Attention Model for Medical Report Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.210/",
        "paper_authors": [
            "Xiao Song",
            "Xiaodan Zhang",
            "Junzhong Ji",
            "Ying Liu",
            "Pengxu Wei"
        ],
        "paper_abstract": "Medical report automatic generation has gained increasing interest recently as a way to help radiologists write reports more efficiently. However, this image-to-text task is rather challenging due to the typical data biases: 1) Normal physiological structures dominate the images, with only tiny abnormalities; 2) Normal descriptions accordingly dominate the reports. Existing methods have attempted to solve these problems, but they neglect to exploit useful information from similar historical cases. In this paper, we propose a novel Cross-modal Contrastive Attention (CMCA) model to capture both visual and semantic information from similar cases, with mainly two modules: a Visual Contrastive Attention Module for refining the unique abnormal regions compared to the retrieved case images; a Cross-modal Attention Module for matching the positive semantic information from the case reports. Extensive experiments on two widely-used benchmarks, IU X-Ray and MIMIC-CXR, demonstrate that the proposed model outperforms the state-of-the-art methods on almost all metrics. Further analyses also validate that our proposed model is able to improve the reports with more accurate abnormal findings and richer descriptions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Domain-Specific NER via Retrieving Correlated Samples",
        "paper_url": "https://aclanthology.org/2022.coling-1.211/",
        "paper_authors": [
            "Xin Zhang",
            "Yong Jiang",
            "Xiaobin Wang",
            "Xuming Hu",
            "Yueheng Sun",
            "Pengjun Xie",
            "Meishan Zhang"
        ],
        "paper_abstract": "Successful Machine Learning based Named Entity Recognition models could fail on texts from some special domains, for instance, Chinese addresses and e-commerce titles, where requires adequate background knowledge. Such texts are also difficult for human annotators. In fact, we can obtain some potentially helpful information from correlated texts, which have some common entities, to help the text understanding. Then, one can easily reason out the correct answer by referencing correlated samples. In this paper, we suggest enhancing NER models with correlated samples. We draw correlated samples by the sparse BM25 retriever from large-scale in-domain unlabeled data. To explicitly simulate the human reasoning process, we perform a training-free entity type calibrating by majority voting. To capture correlation features in the training stage, we suggest to model correlated samples by the transformer-based multi-instance cross-encoder. Empirical results on datasets of the above two domains show the efficacy of our methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Type-enriched Hierarchical Contrastive Strategy for Fine-Grained Entity Typing",
        "paper_url": "https://aclanthology.org/2022.coling-1.212/",
        "paper_authors": [
            "Xinyu Zuo",
            "Haijin Liang",
            "Ning Jing",
            "Shuang Zeng",
            "Zhou Fang",
            "Yu Luo"
        ],
        "paper_abstract": "Fine-grained entity typing (FET) aims to deduce specific semantic types of the entity mentions in the text. Modern methods for FET mainly focus on learning what a certain type looks like. And few works directly model the type differences, that is, let models know the extent that which one type is different from others. To alleviate this problem, we propose a type-enriched hierarchical contrastive strategy for FET. Our method can directly model the differences between hierarchical types and improve the ability to distinguish multi-grained similar types. On the one hand, we embed type into entity contexts to make type information directly perceptible. On the other hand, we design a constrained contrastive strategy on the hierarchical structure to directly model the type differences, which can simultaneously perceive the distinguishability between types at different granularity. Experimental results on three benchmarks, BBN, OntoNotes, and FIGER show that our method achieves significant performance on FET by effectively modeling type differences.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Document-Level Relation Extraction via Pair-Aware and Entity-Enhanced Representation Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.213/",
        "paper_authors": [
            "Xiusheng Huang",
            "Hang Yang",
            "Yubo Chen",
            "Jun Zhao",
            "Kang Liu",
            "Weijian Sun",
            "Zuyu Zhao"
        ],
        "paper_abstract": "Document-level relation extraction aims to recognize relations among multiple entity pairs from a whole piece of article. Recent methods achieve considerable performance but still suffer from two challenges: a) the relational entity pairs are sparse, b) the representation of entity pairs is insufficient. In this paper, we propose Pair-Aware and Entity-Enhanced(PAEE) model to solve the aforementioned two challenges. For the first challenge, we design a Pair-Aware Representation module to predict potential relational entity pairs, which constrains the relation extraction to the predicted entity pairs subset rather than all pairs; For the second, we introduce a Entity-Enhanced Representation module to assemble directional entity pairs and obtain a holistic understanding of the entire document. Experimental results show that our approach can obtain state-of-the-art performance on four benchmark datasets DocRED, DWIE, CDR and GDA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Zero-Shot Entity Linking Candidate Generation with Ultra-Fine Entity Type Information",
        "paper_url": "https://aclanthology.org/2022.coling-1.214/",
        "paper_authors": [
            "Xuhui Sui",
            "Ying Zhang",
            "Kehui Song",
            "Baohang Zhou",
            "Guoqing Zhao",
            "Xin Wei",
            "Xiaojie Yuan"
        ],
        "paper_abstract": "Entity linking, which aims at aligning ambiguous entity mentions to their referent entities in a knowledge base, plays a key role in multiple natural language processing tasks. Recently, zero-shot entity linking task has become a research hotspot, which links mentions to unseen entities to challenge the generalization ability. For this task, the training set and test set are from different domains, and thus entity linking models tend to be overfitting due to the tendency of memorizing the properties of entities that appear frequently in the training set. We argue that general ultra-fine-grained type information can help the linking models to learn contextual commonality and improve their generalization ability to tackle the overfitting problem. However, in the zero-shot entity linking setting, any type information is not available and entities are only identified by textual descriptions. Thus, we first extract the ultra-fine entity type information from the entity textual descriptions. Then, we propose a hierarchical multi-task model to improve the high-level zero-shot entity linking candidate generation task by utilizing the entity typing task as an auxiliary low-level task, which introduces extracted ultra-fine type information into the candidate generation task. Experimental results demonstrate the effectiveness of utilizing the ultra-fine entity type information and our proposed method achieves state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CofeNet: Context and Former-Label Enhanced Net for Complicated Quotation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.215/",
        "paper_authors": [
            "Yequan Wang",
            "Xiang Li",
            "Aixin Sun",
            "Xuying Meng",
            "Huaming Liao",
            "Jiafeng Guo"
        ],
        "paper_abstract": "Quotation extraction aims to extract quotations from written text. There are three components in a quotation: source refers to the holder of the quotation, cue is the trigger word(s), and content is the main body. Existing solutions for quotation extraction mainly utilize rule-based approaches and sequence labeling models. While rule-based approaches often lead to low recalls, sequence labeling models cannot well handle quotations with complicated structures. In this paper, we propose the Context and Former-Label Enhanced Net () for quotation extraction. is able to extract complicated quotations with components of variable lengths and complicated structures. On two public datasets (and ) and one proprietary dataset (), we show that our achieves state-of-the-art performance on complicated quotation extraction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest",
        "paper_url": "https://aclanthology.org/2022.coling-1.216/",
        "paper_authors": [
            "Yifan Jin",
            "Jiangmeng Li",
            "Zheng Lian",
            "Chengbo Jiao",
            "Xiaohui Hu"
        ],
        "paper_abstract": "Medical Relation Extraction (MRE) task aims to extract relations between entities in medical texts. Traditional relation extraction methods achieve impressive success by exploring the syntactic information, e.g., dependency tree. However, the quality of the 1-best dependency tree for medical texts produced by an out-of-domain parser is relatively limited so that the performance of medical relation extraction method may degenerate. To this end, we propose a method to jointly model semantic and syntactic information from medical texts based on causal explanation theory. We generate dependency forests consisting of the semantic-embedded 1-best dependency tree. Then, a task-specific causal explainer is adopted to prune the dependency forests, which are further fed into a designed graph convolutional network to learn the corresponding representation for downstream task. Empirically, the various comparisons on benchmark medical datasets demonstrate the effectiveness of our model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Aspect-based Sentiment Analysis as Machine Reading Comprehension",
        "paper_url": "https://aclanthology.org/2022.coling-1.217/",
        "paper_authors": [
            "Yifei Yang",
            "Hai Zhao"
        ],
        "paper_abstract": "Existing studies typically handle aspect-based sentiment analysis by stacking multiple neural modules, which inevitably result in severe error propagation. Instead, we propose a novel end-to-end framework, MRCOOL: MRC-PrOmpt mOdeL framework, where numerous sentiment aspects are elicited by a machine reading comprehension (MRC) model and their corresponding sentiment polarities are classified in a prompt learning way. Experiments show that our end-to-end framework consistently yields promising results on widely-used benchmark datasets which significantly outperform existing state-of-the-art models or achieve comparable performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Nested Named Entity Recognition as Corpus Aware Holistic Structure Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.218/",
        "paper_authors": [
            "Yifei Yang",
            "Zuchao Li",
            "Hai Zhao"
        ],
        "paper_abstract": "As a fundamental natural language processing task and one of core knowledge extraction techniques, named entity recognition (NER) is widely used to extract information from texts for downstream tasks. Nested NER is a branch of NER in which the named entities (NEs) are nested with each other. However, most of the previous studies on nested NER usually apply linear structure to model the nested NEs which are actually accommodated in a hierarchical structure. Thus in order to address this mismatch, this work models the full nested NEs in a sentence as a holistic structure, then we propose a holistic structure parsing algorithm to disclose the entire NEs once for all. Besides, there is no research on applying corpus-level information to NER currently. To make up for the loss of this information, we introduce Point-wise Mutual Information (PMI) and other frequency features from corpus-aware statistics for even better performance by holistic modeling from sentence-level to corpus-level. Experiments show that our model yields promising results on widely-used benchmarks which approach or even achieve state-of-the-art. Further empirical studies show that our proposed corpus-aware features can substantially improve NER domain adaptation, which demonstrates the surprising advantage of our proposed corpus-level holistic structure modeling.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DESED: Dialogue-based Explanation for Sentence-level Event Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.219/",
        "paper_authors": [
            "Yinyi Wei",
            "Shuaipeng Liu",
            "Jianwei Lv",
            "Xiangyu Xi",
            "Hailei Yan",
            "Wei Ye",
            "Tong Mo",
            "Fan Yang",
            "Guanglu Wan"
        ],
        "paper_abstract": "Many recent sentence-level event detection efforts focus on enriching sentence semantics, e.g., via multi-task or prompt-based learning. Despite the promising performance, these methods commonly depend on label-extensive manual annotations or require domain expertise to design sophisticated templates and rules. This paper proposes a new paradigm, named dialogue-based explanation, to enhance sentence semantics for event detection. By saying dialogue-based explanation of an event, we mean explaining it through a consistent information-intensive dialogue, with the original event description as the start utterance. We propose three simple dialogue generation methods, whose outputs are then fed into a hybrid attention mechanism to characterize the complementary event semantics. Extensive experimental results on two event detection datasets verify the effectiveness of our method and suggest promising research opportunities in the dialogue-based explanation paradigm.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Data Augmentation for Few-Shot Knowledge Graph Completion from Hierarchical Perspective",
        "paper_url": "https://aclanthology.org/2022.coling-1.220/",
        "paper_authors": [
            "Yuanzhou Yao",
            "Zhao Zhang",
            "Yongjun Xu",
            "Chao Li"
        ],
        "paper_abstract": "Few-shot knowledge graph completion (FKGC) has become a new research focus in the field of knowledge graphs in recent years, which aims to predict the missing links for relations that only have a few associative triples. Existing models attempt to solve the problem via learning entity and relation representations. However, the limited training data severely hinders the performance of existing models. To this end, we propose to solve the FKGC problem with the data augmentation technique. Specifically, we perform data augmentation from two perspectives, i.e., inter-task view and intra-task view. The former generates new tasks for FKGC, while the latter enriches the support or query set for an individual task. It is worth noting that the proposed framework can be applied to a number of existing FKGC models. Experimental evaluation on two public datasets indicates our model is capable of achieving substantial improvements over baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.221/",
        "paper_authors": [
            "Yubing Ren",
            "Yanan Cao",
            "Fang Fang",
            "Ping Guo",
            "Zheng Lin",
            "Wei Ma",
            "Yi Liu"
        ],
        "paper_abstract": "Transforming the large amounts of unstructured text on the Internet into structured event knowledge is a critical, yet unsolved goal of NLP, especially when addressing document-level text. Existing methods struggle in Document-level Event Extraction (DEE) due to its two intrinsic challenges: (a) Nested arguments, which means one argument is the sub-string of another one. (b) Multiple events, which indicates we should identify multiple events and assemble the arguments for them. In this paper, we propose a role-interactive multi-event head attention network (CLIO) to solve these two challenges jointly. The key idea is to map different events to multiple subspaces (i.e. multi-event head). In each event subspace, we draw the semantic representation of each role closer to its corresponding arguments, then we determine whether the current event exists. To further optimize event representation, we propose an event representation enhancing strategy to regularize pre-trained embedding space to be more isotropic. Our experiments on two widely used DEE datasets show that CLIO achieves consistent improvements over previous methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "COPNER: Contrastive Learning with Prompt Guiding for Few-shot Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.222/",
        "paper_authors": [
            "Yucheng Huang",
            "Kai He",
            "Yige Wang",
            "Xianli Zhang",
            "Tieliang Gong",
            "Rui Mao",
            "Chen Li"
        ],
        "paper_abstract": "Distance metric learning has become a popular solution for few-shot Named Entity Recognition (NER). The typical setup aims to learn a similarity metric for measuring the semantic similarity between test samples and referents, where each referent represents an entity class. The effect of this setup may, however, be compromised for two reasons. First, there is typically a limited optimization exerted on the representations of entity tokens after initing by pre-trained language models. Second, the referents may be far from representing corresponding entity classes due to the label scarcity in the few-shot setting. To address these challenges, we propose a novel approach named COntrastive learning with Prompt guiding for few-shot NER (COPNER). We introduce a novel prompt composed of class-specific words to COPNER to serve as 1) supervision signals for conducting contrastive learning to optimize token representations; 2) metric referents for distance-metric inference on test samples. Experimental results demonstrate that COPNER outperforms state-of-the-art models with a significant margin in most cases. Moreover, COPNER shows great potential in the zero-shot setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Few Clean Instances Help Denoising Distant Supervision",
        "paper_url": "https://aclanthology.org/2022.coling-1.223/",
        "paper_authors": [
            "Yufang Liu",
            "Ziyin Huang",
            "Yijun Wang",
            "Changzhi Sun",
            "Man Lan",
            "Yuanbin Wu",
            "Xiaofeng Mou",
            "Ding Wang"
        ],
        "paper_abstract": "Existing distantly supervised relation extractors usually rely on noisy data for both model training and evaluation, which may lead to garbage-in-garbage-out systems. To alleviate the problem, we study whether a small clean dataset could help improve the quality of distantly supervised models. We show that besides getting a more convincing evaluation of models, a small clean dataset also helps us to build more robust denoising models. Specifically, we propose a new criterion for clean instance selection based on influence functions. It collects sample-level evidence for recognizing good instances (which is more informative than loss-level evidence). We also propose a teacher-student mechanism for controlling purity of intermediate results when bootstrapping the clean set. The whole approach is model-agnostic and demonstrates strong performances on both denoising real (NYT) and synthetic noisy datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SEE-Few: Seed, Expand and Entail for Few-shot Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.224/",
        "paper_authors": [
            "Zeng Yang",
            "Linhai Zhang",
            "Deyu Zhou"
        ],
        "paper_abstract": "Few-shot named entity recognition (NER) aims at identifying named entities based on only few labeled instances. Current few-shot NER methods focus on leveraging existing datasets in the rich-resource domains which might fail in a training-from-scratch setting where no source-domain data is used. To tackle training-from-scratch setting, it is crucial to make full use of the annotation information (the boundaries and entity types). Therefore, in this paper, we propose a novel multi-task (Seed, Expand and Entail) learning framework, SEE-Few, for Few-shot NER without using source domain data. The seeding and expanding modules are responsible for providing as accurate candidate spans as possible for the entailing module. The entailing module reformulates span classification as a textual entailment task, leveraging both the contextual clues and entity type information. All the three modules share the same text encoder and are jointly learned. Experimental results on several benchmark datasets under the training-from-scratch setting show that the proposed method outperformed several state-of-the-art few-shot NER methods with a large margin. Our code is available at https://github.com/unveiled-the-red-hat/SEE-Few.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Ruleformer: Context-aware Rule Mining over Knowledge Graph",
        "paper_url": "https://aclanthology.org/2022.coling-1.225/",
        "paper_authors": [
            "Zezhong Xu",
            "Peng Ye",
            "Hui Chen",
            "Meng Zhao",
            "Huajun Chen",
            "Wen Zhang"
        ],
        "paper_abstract": "Rule mining is an effective approach for reasoning over knowledge graph (KG). Existing works mainly concentrate on mining rules. However, there might be several rules that could be applied for reasoning for one relation, and how to select appropriate rules for completion of different triples has not been discussed. In this paper, we propose to take the context information into consideration, which helps select suitable rules for the inference tasks. Based on this idea, we propose a transformer-based rule mining approach, Ruleformer. It consists of two blocks: 1) an encoder extracting the context information from subgraph of head entities with modified attention mechanism, and 2) a decoder which aggregates the subgraph information from the encoder output and generates the probability of relations for each step of reasoning. The basic idea behind Ruleformer is regarding rule mining process as a sequence to sequence task. To make the subgraph a sequence input to the encoder and retain the graph structure, we devise a relational attention mechanism in Transformer. The experiment results show the necessity of considering these information in rule mining task and the effectiveness of our model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Are People Located in the Places They Mention in Their Tweets? A Multimodal Approach",
        "paper_url": "https://aclanthology.org/2022.coling-1.226/",
        "paper_authors": [
            "Zhaomin Xiao",
            "Eduardo Blanco"
        ],
        "paper_abstract": "This paper introduces the problem of determining whether people are located in the places they mention in their tweets. In particular, we investigate the role of text and images to solve this challenging problem. We present a new corpus of tweets that contain both text and images. Our analyses show that this problem is multimodal at its core: human judgments depend on whether annotators have access to the text, the image, or both. Experimental results show that a neural architecture that combines both modalities yields better results. We also conduct an error analysis to provide insights into why and when each modality is beneficial.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-modal Contrastive Representation Learning for Entity Alignment",
        "paper_url": "https://aclanthology.org/2022.coling-1.227/",
        "paper_authors": [
            "Zhenxi Lin",
            "Ziheng Zhang",
            "Meng Wang",
            "Yinghui Shi",
            "Xian Wu",
            "Yefeng Zheng"
        ],
        "paper_abstract": "Multi-modal entity alignment aims to identify equivalent entities between two different multi-modal knowledge graphs, which consist of structural triples and images associated with entities. Most previous works focus on how to utilize and encode information from different modalities, while it is not trivial to leverage multi-modal knowledge in entity alignment because of the modality heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive Learning based Entity Alignment model, to obtain effective joint representations for multi-modal entity alignment. Different from previous works, MCLEA considers task-oriented modality and models the inter-modal relationships for each entity representation. In particular, MCLEA firstly learns multiple individual representations from multiple modalities, and then performs contrastive learning to jointly model intra-modal and inter-modal interactions. Extensive experimental results show that MCLEA outperforms state-of-the-art baselines on public datasets under both supervised and unsupervised settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Nonparametric Forest-Structured Neural Topic Modeling",
        "paper_url": "https://aclanthology.org/2022.coling-1.228/",
        "paper_authors": [
            "Zhihong Zhang",
            "Xuewen Zhang",
            "Yanghui Rao"
        ],
        "paper_abstract": "Neural topic models have been widely used in discovering the latent semantics from a corpus. Recently, there are several researches on hierarchical neural topic models since the relationships among topics are valuable for data analysis and exploration. However, the existing hierarchical neural topic models are limited to generate a single topic tree. In this study, we present a nonparametric forest-structured neural topic model by firstly applying the self-attention mechanism to capture parent-child topic relationships, and then build a sparse directed acyclic graph to form a topic forest. Experiments indicate that our model can automatically learn a forest-structured topic hierarchy with indefinite numbers of trees and leaves, and significantly outperforms the baseline models on topic hierarchical rationality and affinity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge Graph Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.229/",
        "paper_authors": [
            "Zhiping Luo",
            "Wentao Xu",
            "Weiqing Liu",
            "Jiang Bian",
            "Jian Yin",
            "Tie-Yan Liu"
        ],
        "paper_abstract": "Learning the embeddings of knowledge graphs (KG) is vital in artificial intelligence, and can benefit various downstream applications, such as recommendation and question answering. In recent years, many research efforts have been proposed for knowledge graph embedding (KGE). However, most previous KGE methods ignore the semantic similarity between the related entities and entity-relation couples in different triples since they separately optimize each triple with the scoring function. To address this problem, we propose a simple yet efficient contrastive learning framework for tensor decomposition based (TDB) KGE, which can shorten the semantic distance of the related entities and entity-relation couples in different triples and thus improve the performance of KGE. We evaluate our proposed method on three standard KGE datasets: WN18RR, FB15k-237 and YAGO3-10. Our method can yield some new state-of-the-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR dataset, 37.8% MRR, 28.6% Hits@1 on FB15k-237 dataset, and 59.1% MRR, 51.8% Hits@1 on the YAGO3-10 dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.230/",
        "paper_authors": [
            "Zhiwei Yang",
            "Jing Ma",
            "Hechang Chen",
            "Hongzhan Lin",
            "Ziyang Luo",
            "Yi Chang"
        ],
        "paper_abstract": "Existing fake news detection methods aim to classify a piece of news as true or false and provide veracity explanations, achieving remarkable performances. However, they often tailor automated solutions on manual fact-checked reports, suffering from limited news coverage and debunking delays. When a piece of news has not yet been fact-checked or debunked, certain amounts of relevant raw reports are usually disseminated on various media outlets, containing the wisdom of crowds to verify the news claim and explain its verdict. In this paper, we propose a novel Coarse-to-fine Cascaded Evidence-Distillation (CofCED) neural network for explainable fake news detection based on such raw reports, alleviating the dependency on fact-checked ones. Specifically, we first utilize a hierarchical encoder for web text representation, and then develop two cascaded selectors to select the most explainable sentences for verdicts on top of the selected top-K reports in a coarse-to-fine manner. Besides, we construct two explainable fake news datasets, which is publicly available. Experimental results demonstrate that our model significantly outperforms state-of-the-art detection baselines and generates high-quality explanations from diverse evaluation perspectives.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Document-level Event Factuality Identification via Machine Reading Comprehension Frameworks with Transfer Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.231/",
        "paper_authors": [
            "Zhong Qian",
            "Heng Zhang",
            "Peifeng Li",
            "Qiaoming Zhu",
            "Guodong Zhou"
        ],
        "paper_abstract": "Document-level Event Factuality Identification (DEFI) predicts the factuality of a specific event based on a document from which the event can be derived, which is a fundamental and crucial task in Natural Language Processing (NLP). However, most previous studies only considered sentence-level task and did not adopt document-level knowledge. Moreover, they modelled DEFI as a typical text classification task depending on annotated information heavily, and limited to the task-specific corpus only, which resulted in data scarcity. To tackle these issues, we propose a new framework formulating DEFI as Machine Reading Comprehension (MRC) tasks considering both Span-Extraction (Ext) and Multiple-Choice (Mch). Our model does not employ any other explicit annotated information, and utilizes Transfer Learning (TL) to extract knowledge from universal large-scale MRC corpora for cross-domain data augmentation. The empirical results on DLEFM corpus demonstrate that the proposed model outperforms several state-of-the-arts.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unregulated Chinese-to-English Data Expansion Does NOT Work for Neural Event Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.232/",
        "paper_authors": [
            "Zhongqiu Li",
            "Yu Hong",
            "Jie Wang",
            "Shiming He",
            "Jianmin Yao",
            "Guodong Zhou"
        ],
        "paper_abstract": "We leverage cross-language data expansion and retraining to enhance neural Event Detection (abbr., ED) on English ACE corpus. Machine translation is utilized for expanding English training set of ED from that of Chinese. However, experimental results illustrate that such strategy actually results in performance degradation. The survey of translations suggests that the mistakenly-aligned triggers in the expanded data negatively influences the retraining process. We refer this phenomenon to \u201ctrigger falsification\u201d. To overcome the issue, we apply heuristic rules for regulating the expanded data, fixing the distracting samples that contain the falsified triggers. The supplementary experiments show that the rule-based regulation is beneficial, yielding the improvement of about 1.6% F1-score for ED. We additionally prove that, instead of transfer learning from the translated ED data, the straight data combination by random pouring surprisingly performs better.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Finding Influential Instances for Distantly Supervised Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.233/",
        "paper_authors": [
            "Zifeng Wang",
            "Rui Wen",
            "Xi Chen",
            "Shao-Lun Huang",
            "Ningyu Zhang",
            "Yefeng Zheng"
        ],
        "paper_abstract": "Distant supervision (DS) is a strong way to expand the datasets for enhancing relation extraction (RE) models but often suffers from high label noise. Current works based on attention, reinforcement learning, or GAN are black-box models so they neither provide meaningful interpretation of sample selection in DS nor stability on different domains. On the contrary, this work proposes a novel model-agnostic instance sampling method for DS by influence function (IF), namely REIF. Our method identifies favorable/unfavorable instances in the bag based on IF, then does dynamic instance sampling. We design a fast influence sampling algorithm that reduces the computational complexity from \ud835\udcaa(mn) to \ud835\udcaa(1), with analyzing its robustness on the selected sampling function. Experiments show that by simply sampling the favorable instances during training, REIF is able to win over a series of baselines which have complicated architectures. We also demonstrate that REIF can support interpretable instance selection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Simple Model for Distantly Supervised Relation Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.234/",
        "paper_authors": [
            "Ziqin Rao",
            "Fangxiang Feng",
            "Ruifan Li",
            "Xiaojie Wang"
        ],
        "paper_abstract": "Distantly supervised relation extraction is challenging due to the noise within data. Recent methods focus on exploiting bag representations based on deep neural networks with complex de-noising scheme to achieve remarkable performance. In this paper, we propose a simple but effective BERT-based Graph convolutional network Model (i.e., BGM). Our BGM comprises of an instance embedding module and a bag representation module. The instance embedding module uses a BERT-based pretrained language model to extract key information from each instance. The bag representaion module constructs the corresponding bag graph then apply a convolutional operation to obtain the bag representation. Our BGM model achieves a considerable improvement on two benchmark datasets, i.e., NYT10 and GDS.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Augmenting Legal Judgment Prediction with Contrastive Case Relations",
        "paper_url": "https://aclanthology.org/2022.coling-1.235/",
        "paper_authors": [
            "Dugang Liu",
            "Weihao Du",
            "Lei Li",
            "Weike Pan",
            "Zhong Ming"
        ],
        "paper_abstract": "Existing legal judgment prediction methods usually only consider one single case fact description as input, which may not fully utilize the information in the data such as case relations and frequency. In this paper, we propose a new perspective that introduces some contrastive case relations to construct case triples as input, and a corresponding judgment prediction framework with case triples modeling (CTM). Our CTM can more effectively utilize beneficial information to refine the encoding and decoding processes through three customized modules, including the case triple module, the relational attention module, and the category decoder module. Finally, we conduct extensive experiments on two public datasets to verify the effectiveness of our CTM, including overall evaluation, compatibility analysis, ablation studies, analysis of gain source and visualization of case representations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Constrained Regeneration for Cross-Lingual Query-Focused Extractive Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.236/",
        "paper_authors": [
            "Elsbeth Turcan",
            "David Wan",
            "Faisal Ladhak",
            "Petra Galuscakova",
            "Sukanta Sen",
            "Svetlana Tchistiakova",
            "Weijia Xu",
            "Marine Carpuat",
            "Kenneth Heafield",
            "Douglas Oard",
            "Kathleen McKeown"
        ],
        "paper_abstract": "Query-focused summaries of foreign-language, retrieved documents can help a user understand whether a document is actually relevant to the query term. A standard approach to this problem is to first translate the source documents and then perform extractive summarization to find relevant snippets. However, in a cross-lingual setting, the query term does not necessarily appear in the translations of relevant documents. In this work, we show that constrained machine translation and constrained post-editing can improve human relevance judgments by including a query term in a summary when its translation appears in the source document. We also present several strategies for selecting only certain documents for regeneration which yield further improvements",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Programmable Annotation with Diversed Heuristics and Data Denoising",
        "paper_url": "https://aclanthology.org/2022.coling-1.237/",
        "paper_authors": [
            "Ernie Chang",
            "Alex Marin",
            "Vera Demberg"
        ],
        "paper_abstract": "Neural natural language generation (NLG) and understanding (NLU) models are costly and require massive amounts of annotated data to be competitive. Recent data programming frameworks address this bottleneck by allowing human supervision to be provided as a set of labeling functions to construct generative models that synthesize weak labels at scale. However, these labeling functions are difficult to build from scratch for NLG/NLU models, as they often require complex rule sets to be specified. To this end, we propose a novel data programming framework that can jointly construct labeled data for language generation and understanding tasks \u2013 by allowing the annotators to modify an automatically-inferred alignment rule set between sequence labels and text, instead of writing rules from scratch. Further, to mitigate the effect of poor quality labels, we propose a dually-regularized denoising mechanism for optimizing the NLU and NLG models. On two benchmarks we show that the framework can generate high-quality data that comes within a 1.48 BLEU and 6.42 slot F1 of the 100% human-labeled data (42k instances) with just 100 labeled data samples \u2013 outperforming benchmark annotation frameworks and other semi-supervised approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Text-to-Text Extraction and Verbalization of Biomedical Event Graphs",
        "paper_url": "https://aclanthology.org/2022.coling-1.238/",
        "paper_authors": [
            "Giacomo Frisoni",
            "Gianluca Moro",
            "Lorenzo Balzani"
        ],
        "paper_abstract": "Biomedical events represent complex, graphical, and semantically rich interactions expressed in the scientific literature. Almost all contributions in the event realm orbit around semantic parsing, usually employing discriminative architectures and cumbersome multi-step pipelines limited to a small number of target interaction types. We present the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, allowing us to fuse all the resources so far designed for different tasks. To this end, we present a new event graph linearization technique and release highly comprehensive event-text paired datasets, covering more than 150 event types from multiple biology subareas (English language). By streamlining parsing and generation to translations, we propose baseline transformer model results according to multiple biomedical text mining benchmarks and NLG metrics. Our extractive models achieve greater state-of-the-art performance than single-task competitors and show promising capabilities for the controlled generation of coherent natural language utterances from structured data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multimodal Semi-supervised Learning for Disaster Tweet Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.239/",
        "paper_authors": [
            "Iustin Sirbu",
            "Tiberiu Sosea",
            "Cornelia Caragea",
            "Doina Caragea",
            "Traian Rebedea"
        ],
        "paper_abstract": "During natural disasters, people often use social media platforms, such as Twitter, to post information about casualties and damage produced by disasters. This information can help relief authorities gain situational awareness in nearly real time, and enable them to quickly distribute resources where most needed. However, annotating data for this purpose can be burdensome, subjective and expensive. In this paper, we investigate how to leverage the copious amounts of unlabeled data generated on social media by disaster eyewitnesses and affected individuals during disaster events. To this end, we propose a semi-supervised learning approach to improve the performance of neural models on several multimodal disaster tweet classification tasks. Our approach shows significant improvements, obtaining up to 7.7% improvements in F-1 in low-data regimes and 1.9% when using the entire training data. We make our code and data publicly available at https://github.com/iustinsirbu13/multimodal-ssl-for-disaster-tweet-classification.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automated Essay Scoring via Pairwise Contrastive Regression",
        "paper_url": "https://aclanthology.org/2022.coling-1.240/",
        "paper_authors": [
            "Jiayi Xie",
            "Kaiwei Cai",
            "Li Kong",
            "Junsheng Zhou",
            "Weiguang Qu"
        ],
        "paper_abstract": "Automated essay scoring (AES) involves the prediction of a score relating to the writing quality of an essay. Most existing works in AES utilize regression objectives or ranking objectives respectively. However, the two types of methods are highly complementary. To this end, in this paper we take inspiration from contrastive learning and propose a novel unified Neural Pairwise Contrastive Regression (NPCR) model in which both objectives are optimized simultaneously as a single loss. Specifically, we first design a neural pairwise ranking model to guarantee the global ranking order in a large list of essays, and then we further extend this pairwise ranking model to predict the relative scores between an input essay and several reference essays. Additionally, a multi-sample voting strategy is employed for inference. We use Quadratic Weighted Kappa to evaluate our model on the public Automated Student Assessment Prize (ASAP) dataset, and the experimental results demonstrate that NPCR outperforms previous methods by a large margin, achieving the state-of-the-art average performance for the AES task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision",
        "paper_url": "https://aclanthology.org/2022.coling-1.241/",
        "paper_authors": [
            "Khalil Mrini",
            "Harpreet Singh",
            "Franck Dernoncourt",
            "Seunghyun Yoon",
            "Trung Bui",
            "Walter W. Chang",
            "Emilia Farcas",
            "Ndapa Nakashole"
        ],
        "paper_abstract": "Current medical question answering systems have difficulty processing long, detailed and informally worded questions submitted by patients, called Consumer Health Questions (CHQs). To address this issue, we introduce a medical question understanding and answering system with knowledge grounding and semantic self-supervision. Our system is a pipeline that first summarizes a long, medical, user-written question, using a supervised summarization loss. Then, our system performs a two-step retrieval to return answers. The system first matches the summarized user question with an FAQ from a trusted medical knowledge base, and then retrieves a fixed number of relevant sentences from the corresponding answer document. In the absence of labels for question matching or answer relevance, we design 3 novel, self-supervised and semantically-guided losses. We evaluate our model against two strong retrieval-based question answering baselines. Evaluators ask their own questions and rate the answers retrieved by our baselines and own system according to their relevance. They find that our system retrieves more relevant answers, while achieving speeds 20 times faster. Our self-supervised losses also help the summarizer achieve higher scores in ROUGE, as well as in human evaluation metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Progressive Framework for Role-Aware Rumor Resolution",
        "paper_url": "https://aclanthology.org/2022.coling-1.242/",
        "paper_authors": [
            "Lei Chen",
            "Guanying Li",
            "Zhongyu Wei",
            "Yang Yang",
            "Baohua Zhou",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Existing works on rumor resolution have shown great potential in recognizing word appearance and user participation. However, they ignore the intrinsic propagation mechanisms of rumors and present poor adaptive ability when unprecedented news emerges. To exploit the fine-grained rumor diffusion patterns and generalize rumor resolution methods, we formulate a predecessor task to identify triggering posts, and then exploit their characteristics to facilitate rumor verification. We design a tree-structured annotation interface and extend PHEME dataset with labels on the message level. Data analysis shows that triggers play a critical role in verifying rumors and present similar lingual patterns across irrelevant events. We propose a graph-based model considering the direction and interaction of information flow to implement role-aware rumor resolution. Experimental results demonstrate the effectiveness of our proposed model and progressive scheme.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Uncertainty-aware Propagation Structure Reconstruction for Fake News Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.243/",
        "paper_authors": [
            "Lingwei Wei",
            "Dou Hu",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "paper_abstract": "The widespread of fake news has detrimental societal effects. Recent works model information propagation as graph structure and aggregate structural features from user interactions for fake news detection. However, they usually neglect a broader propagation uncertainty issue, caused by some missing and unreliable interactions during actual spreading, and suffer from learning accurate and diverse structural properties. In this paper, we propose a novel dual graph-based model, Uncertainty-aware Propagation Structure Reconstruction (UPSR) for improving fake news detection. Specifically, after the original propagation modeling, we introduce propagation structure reconstruction to fully explore latent interactions in the actual propagation. We design a novel Gaussian Propagation Estimation to refine the original deterministic node representation by multiple Gaussian distributions and arise latent interactions with KL divergence between distributions in a multi-facet manner. Extensive experiments on two real-world datasets demonstrate the effectiveness and superiority of our model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Unified Propagation Forest-based Framework for Fake News Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.244/",
        "paper_authors": [
            "Lingwei Wei",
            "Dou Hu",
            "Yantong Lai",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "paper_abstract": "Fake news\u2019s quick propagation on social media brings severe social ramifications and economic damage. Previous fake news detection usually learn semantic and structural patterns within a single target propagation tree. However, they are usually limited in narrow signals since they do not consider latent information cross other propagation trees. Motivated by a common phenomenon that most fake news is published around a specific hot event/topic, this paper develops a new concept of propagation forest to naturally combine propagation trees in a semantic-aware clustering. We propose a novel Unified Propagation Forest-based framework (UniPF) to fully explore latent correlations between propagation trees to improve fake news detection. Besides, we design a root-induced training strategy, which encourages representations of propagation trees to be closer to their prototypical root nodes. Extensive experiments on four benchmarks consistently suggest the effectiveness and scalability of UniPF.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CLoSE: Contrastive Learning of Subframe Embeddings for Political Bias Classification of News Media",
        "paper_url": "https://aclanthology.org/2022.coling-1.245/",
        "paper_authors": [
            "Michelle YoungJin Kim",
            "Kristen Marie Johnson"
        ],
        "paper_abstract": "Framing is a political strategy in which journalists and politicians emphasize certain aspects of a societal issue in order to influence and sway public opinion. Frameworks for detecting framing in news articles or social media posts are critical in understanding the spread of biased information in our society. In this paper, we propose CLoSE, a multi-task BERT-based model which uses contrastive learning to embed indicators of frames from news articles in order to predict political bias. We evaluate the performance of our proposed model on subframes and political bias classification tasks. We also demonstrate the model\u2019s classification accuracy on zero-shot and few-shot learning tasks, providing a promising avenue for framing detection in unlabeled data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Grammatical Error Correction: Are We There Yet?",
        "paper_url": "https://aclanthology.org/2022.coling-1.246/",
        "paper_authors": [
            "Muhammad Reza Qorib",
            "Hwee Tou Ng"
        ],
        "paper_abstract": "There has been much recent progress in natural language processing, and grammatical error correction (GEC) is no exception. We found that state-of-the-art GEC systems (T5 and GECToR) outperform humans by a wide margin on the CoNLL-2014 test set, a benchmark GEC test corpus, as measured by the standard F0.5 evaluation metric. However, a careful examination of their outputs reveals that there are still classes of errors that they fail to correct. This suggests that creating new test data that more accurately measure the true performance of GEC systems constitutes important future work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CXR Data Annotation and Classification with Pre-trained Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.247/",
        "paper_authors": [
            "Nina Zhou",
            "Ai Ti Aw",
            "Zhuo Han Liu",
            "Cher heng Tan",
            "Yonghan Ting",
            "Wen Xiang Chen",
            "Jordan sim zheng Ting"
        ],
        "paper_abstract": "Clinical data annotation has been one of the major obstacles for applying machine learning approaches in clinical NLP. Open-source tools such as NegBio and CheXpert are usually designed on data from specific institutions, which limit their applications to other institutions due to the differences in writing style, structure, language use as well as label definition. In this paper, we propose a new weak supervision annotation framework with two improvements compared to existing annotation frameworks: 1) we propose to select representative samples for efficient manual annotation; 2) we propose to auto-annotate the remaining samples, both leveraging on a self-trained sentence encoder. This framework also provides a function for identifying inconsistent annotation errors. The utility of our proposed weak supervision annotation framework is applicable to any given data annotation task, and it provides an efficient form of sample selection and data auto-annotation with better classification results for real applications.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers",
        "paper_url": "https://aclanthology.org/2022.coling-1.248/",
        "paper_authors": [
            "Piji Li"
        ],
        "paper_abstract": "The task of Chinese Spelling Check (CSC) is aiming to detect and correct spelling errors that can be found in the text. While manually annotating a high-quality dataset is expensive and time-consuming, thus the scale of the training dataset is usually very small (e.g., SIGHAN15 only contains 2339 samples for training), therefore supervised-learning based models usually suffer the data sparsity limitation and over-fitting issue, especially in the era of big language models. In this paper, we are dedicated to investigating the unsupervised paradigm to address the CSC problem and we propose a framework named uChecker to conduct unsupervised spelling error detection and correction. Masked pretrained language models such as BERT are introduced as the backbone model considering their powerful language diagnosis capability. Benefiting from the various and flexible MASKing operations, we propose a Confusionset-guided masking strategy to fine-train the masked language model to further improve the performance of unsupervised detection and correction. Experimental results on standard datasets demonstrate the effectiveness of our proposed model uChecker in terms of character-level and sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of spelling error detection and correction respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation",
        "paper_url": "https://aclanthology.org/2022.coling-1.249/",
        "paper_authors": [
            "Qijiong Liu",
            "Jieming Zhu",
            "Quanyu Dai",
            "Xiao-Ming Wu"
        ],
        "paper_abstract": "Understanding news content is critical to improving the quality of news recommendation. To achieve this goal, recent studies have attempted to apply pre-trained language models (PLMs) such as BERT for semantic-enhanced news recommendation. Despite their great success in offline evaluation, it is still a challenge to apply such large PLMs in real-time ranking model due to the stringent requirement in inference and updating time. To bridge this gap, we propose a plug-and-play pre-trainer, namely PREC, to learn both user and news encoders through multi-task pre-training. Instead of directly leveraging sophisticated PLMs for end-to-end inference, we focus on how to use the derived user and item representations to boost the performance of conventional lightweight models for click-through-rate prediction. This enables efficient online inference as well as compatibility to conventional models, which would significantly ease the practical deployment. We validate the effectiveness of PREC through both offline evaluation on public datasets and online A/B testing in an industrial application.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer",
        "paper_url": "https://aclanthology.org/2022.coling-1.250/",
        "paper_authors": [
            "Qiong Nan",
            "Danding Wang",
            "Yongchun Zhu",
            "Qiang Sheng",
            "Yuhui Shi",
            "Juan Cao",
            "Jintao Li"
        ],
        "paper_abstract": "Social media spreads both real news and fake news in various domains including politics, health, entertainment, etc. It is crucial to automatically detect fake news, especially for news of influential domains like politics and health because they may lead to serious social impact, e.g., panic in the COVID-19 pandemic. Some studies indicate the correlation between domains and perform multi-domain fake news detection. However, these multi-domain methods suffer from a seesaw problem that the performance of some domains is often improved by hurting the performance of other domains, which could lead to an unsatisfying performance in the specific target domains. To address this issue, we propose a Domain- and Instance-level Transfer Framework for Fake News Detection (DITFEND), which could improve the performance of specific target domains. To transfer coarse-grained domain-level knowledge, we train a general model with data of all domains from the meta-learning perspective. To transfer fine-grained instance-level knowledge and adapt the general model to a target domain, a language model is trained on the target domain to evaluate the transferability of each data instance in source domains and re-weight the instance\u2019s contribution. Experiments on two real-world datasets demonstrate the effectiveness of DITFEND. According to both offline and online experiments, the DITFEND shows superior effectiveness for fake news detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Student Surpasses Teacher: Imitation Attack for Black-Box NLP APIs",
        "paper_url": "https://aclanthology.org/2022.coling-1.251/",
        "paper_authors": [
            "Qiongkai Xu",
            "Xuanli He",
            "Lingjuan Lyu",
            "Lizhen Qu",
            "Gholamreza Haffari"
        ],
        "paper_abstract": "Machine-learning-as-a-service (MLaaS) has attracted millions of users to their splendid large-scale models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we conduct unsupervised domain adaptation and multi-victim ensemble to showing that attackers could potentially surpass victims, which is beyond previous understanding of model extraction. Extensive experiments on both benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models on transferred domains. We consider our work as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.252/",
        "paper_authors": [
            "Rajiv Movva",
            "Jinhao Lei",
            "Shayne Longpre",
            "Ajay Gupta",
            "Chris DuBois"
        ],
        "paper_abstract": "Quantization, knowledge distillation, and magnitude pruning are among the most popular methods for neural network compression in NLP. Independently, these methods reduce model size and can accelerate inference, but their relative benefit and combinatorial interactions have not been rigorously studied. For each of the eight possible subsets of these techniques, we compare accuracy vs. model size tradeoffs across six BERT architecture sizes and eight GLUE tasks. We find that quantization and distillation consistently provide greater benefit than pruning. Surprisingly, except for the pair of pruning and quantization, using multiple methods together rarely yields diminishing returns. Instead, we observe complementary and super-multiplicative reductions to model size. Our work quantitatively demonstrates that combining compression methods can synergistically reduce model size, and that practitioners should prioritize (1) quantization, (2) knowledge distillation, and (3) pruning to maximize accuracy vs. model size tradeoffs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PlugAT: A Plug and Play Module to Defend against Textual Adversarial Attack",
        "paper_url": "https://aclanthology.org/2022.coling-1.253/",
        "paper_authors": [
            "Rui Zheng",
            "Rong Bao",
            "Qin Liu",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang",
            "Rui Xie",
            "Wei Wu"
        ],
        "paper_abstract": "Adversarial training, which minimizes the loss of adversarially perturbed examples, has received considerable attention. However, these methods require modifying all model parameters and optimizing the model from scratch, which is parameter inefficient and unfriendly to the already deployed models. As an alternative, we propose a pluggable defense module PlugAT, to provide robust predictions by adding a few trainable parameters to the model inputs while keeping the original model frozen. To reduce the potential side effects of using defense modules, we further propose a novel forgetting restricted adversarial training, which filters out bad adversarial examples that impair the performance of original ones. The PlugAT-equipped BERT model substantially improves robustness over several strong baselines on various text classification tasks, whilst training only 9.1% parameters. We observe that defense modules trained under the same model architecture have domain adaptation ability between similar text classification datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automatic ICD Coding Exploiting Discourse Structure and Reconciled Code Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.254/",
        "paper_authors": [
            "Shurui Zhang",
            "Bozheng Zhang",
            "Fuxin Zhang",
            "Bo Sang",
            "Wanchun Yang"
        ],
        "paper_abstract": "The International Classification of Diseases (ICD) is the foundation of global health statistics and epidemiology. The ICD is designed to translate health conditions into alphanumeric codes. A number of approaches have been proposed for automatic ICD coding, since manual coding is labor-intensive and there is a global shortage of healthcare workers. However, existing studies did not exploit the discourse structure of clinical notes, which provides rich contextual information for code assignment. In this paper, we exploit the discourse structure by leveraging section type classification and section type embeddings. We also focus on the class-imbalanced problem and the heterogeneous writing style between clinical notes and ICD code definitions. The proposed reconciled embedding approach is able to tackle them simultaneously. Experimental results on the MIMIC dataset show that our model outperforms all previous state-of-the-art models by a large margin. The source code is available at https://github.com/discnet2022/discnet",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Summarizing Healthcare Questions in Low-Resource Setting",
        "paper_url": "https://aclanthology.org/2022.coling-1.255/",
        "paper_authors": [
            "Shweta Yadav",
            "Cornelia Caragea"
        ],
        "paper_abstract": "The current advancement in abstractive document summarization depends to a large extent on a considerable amount of human-annotated datasets. However, the creation of large-scale datasets is often not feasible in closed domains, such as medical and healthcare domains, where human annotation requires domain expertise. This paper presents a novel data selection strategy to generate diverse and semantic questions in a low-resource setting with the aim to summarize healthcare questions. Our method exploits the concept of guided semantic-overlap and diversity-based objective functions to optimally select the informative and diverse set of synthetic samples for data augmentation. Our extensive experiments on benchmark healthcare question summarization datasets demonstrate the effectiveness of our proposed data selection strategy by achieving new state-of-the-art results. Our human evaluation shows that our method generates diverse, fluent, and informative summarized questions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Doc-GCN: Heterogeneous Graph Convolutional Networks for Document Layout Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.256/",
        "paper_authors": [
            "Siwen Luo",
            "Yihao Ding",
            "Siqu Long",
            "Josiah Poon",
            "Soyeon Caren Han"
        ],
        "paper_abstract": "Recognizing the layout of unstructured digital documents is crucial when parsing the documents into the structured, machine-readable format for downstream applications. Recent studies in Document Layout Analysis usually rely on visual cues to understand documents while ignoring other information, such as contextual information or the relationships between document layout components, which are vital to boost better layout analysis performance. Our Doc-GCN presents an effective way to harmonize and integrate heterogeneous aspects for Document Layout Analysis. We construct different graphs to capture the four main features aspects of document layout components, including syntactic, semantic, density, and appearance features. Then, we apply graph convolutional networks to enhance each aspect of features and apply the node-level pooling for integration. Finally, we concatenate features of all aspects and feed them into the 2-layer MLPs for document layout component classification. Our Doc-GCN achieves state-of-the-art results on three widely used DLA datasets: PubLayNet, FUNSD, and DocBank. The code will be released at https://github.com/adlnlp/doc_gcn",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Analytic Automated Essay Scoring Based on Deep Neural Networks Integrating Multidimensional Item Response Theory",
        "paper_url": "https://aclanthology.org/2022.coling-1.257/",
        "paper_authors": [
            "Takumi Shibata",
            "Masaki Uto"
        ],
        "paper_abstract": "Essay exams have been attracting attention as a way of measuring the higher-order abilities of examinees, but they have two major drawbacks in that grading them is expensive and raises questions about fairness. As an approach to overcome these problems, automated essay scoring (AES) is in increasing need. Many AES models based on deep neural networks have been proposed in recent years and have achieved high accuracy, but most of these models are designed to predict only a single overall score. However, to provide detailed feedback in practical situations, we often require not only the overall score but also analytic scores corresponding to various aspects of the essay. Several neural AES models that can predict both the analytic scores and the overall score have also been proposed for this very purpose. However, conventional models are designed to have complex neural architectures for each analytic score, which makes interpreting the score prediction difficult. To improve the interpretability of the prediction while maintaining scoring accuracy, we propose a new neural model for automated analytic scoring that integrates a multidimensional item response theory model, which is a popular psychometric model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DP-Rewrite: Towards Reproducibility and Transparency in Differentially Private Text Rewriting",
        "paper_url": "https://aclanthology.org/2022.coling-1.258/",
        "paper_authors": [
            "Timour Igamberdiev",
            "Thomas Arnold",
            "Ivan Habernal"
        ],
        "paper_abstract": "Text rewriting with differential privacy (DP) provides concrete theoretical guarantees for protecting the privacy of individuals in textual documents. In practice, existing systems may lack the means to validate their privacy-preserving claims, leading to problems of transparency and reproducibility. We introduce DP-Rewrite, an open-source framework for differentially private text rewriting which aims to solve these problems by being modular, extensible, and highly customizable. Our system incorporates a variety of downstream datasets, models, pre-training procedures, and evaluation metrics to provide a flexible way to lead and validate private text rewriting research. To demonstrate our software in practice, we provide a set of experiments as a case study on the ADePT DP text rewriting system, detecting a privacy leak in its pre-training approach. Our system is publicly available, and we hope that it will help the community to make DP text rewriting research more accessible and transparent.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Harnessing Abstractive Summarization for Fact-Checked Claim Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.259/",
        "paper_authors": [
            "Varad Bhatnagar",
            "Diptesh Kanojia",
            "Kameswari Chebrolu"
        ],
        "paper_abstract": "Social media platforms have become new battlegrounds for anti-social elements, with misinformation being the weapon of choice. Fact-checking organizations try to debunk as many claims as possible while staying true to their journalistic processes but cannot cope with its rapid dissemination. We believe that the solution lies in partial automation of the fact-checking life cycle, saving human time for tasks which require high cognition. We propose a new workflow for efficiently detecting previously fact-checked claims that uses abstractive summarization to generate crisp queries. These queries can then be executed on a general-purpose retrieval system associated with a collection of previously fact-checked claims. We curate an abstractive text summarization dataset comprising noisy claims from Twitter and their gold summaries. It is shown that retrieval performance improves 2x by using popular out-of-the-box summarization models and 3x by fine-tuning them on the accompanying dataset compared to verbatim querying. Our approach achieves Recall@5 and MRR of 35% and 0.3, compared to baseline values of 10% and 0.1, respectively. Our dataset, code, and models are available publicly: https://github.com/varadhbhatnagar/FC-Claim-Det/.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Generate Explanation from e-Hospital Services for Medical Suggestion",
        "paper_url": "https://aclanthology.org/2022.coling-1.260/",
        "paper_authors": [
            "Wei-Lin Chen",
            "An-Zi Yen",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "paper_abstract": "Explaining the reasoning of neural models has attracted attention in recent years. Providing highly-accessible and comprehensible explanations in natural language is useful for humans to understand model\u2019s prediction results. In this work, we present a pilot study to investigate explanation generation with a narrative and causal structure for the scenario of health consulting. Our model generates a medical suggestion regarding the patient\u2019s concern and provides an explanation as the outline of the reasoning. To align the generated explanation with the suggestion, we propose a novel discourse-aware mechanism with multi-task learning. Experimental results show that our model achieves promising performances in both quantitative and human evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DeltaNet: Conditional Medical Report Generation for COVID-19 Diagnosis",
        "paper_url": "https://aclanthology.org/2022.coling-1.261/",
        "paper_authors": [
            "Xian Wu",
            "Shuxin Yang",
            "Zhaopeng Qiu",
            "Shen Ge",
            "Yangtian Yan",
            "Xingwang Wu",
            "Yefeng Zheng",
            "S. Kevin Zhou",
            "Li Xiao"
        ],
        "paper_abstract": "Fast screening and diagnosis are critical in COVID-19 patient treatment. In addition to the gold standard RT-PCR, radiological imaging like X-ray and CT also works as an important means in patient screening and follow-up. However, due to the excessive number of patients, writing reports becomes a heavy burden for radiologists. To reduce the workload of radiologists, we propose DeltaNet to generate medical reports automatically. Different from typical image captioning approaches that generate reports with an encoder and a decoder, DeltaNet applies a conditional generation process. In particular, given a medical image, DeltaNet employs three steps to generate a report: 1) first retrieving related medical reports, i.e., the historical reports from the same or similar patients; 2) then comparing retrieved images and current image to find the differences; 3) finally generating a new report to accommodate identified differences based on the conditional report. We evaluate DeltaNet on a COVID-19 dataset, where DeltaNet outperforms state-of-the-art approaches. Besides COVID-19, the proposed DeltaNet can be applied to other diseases as well. We validate its generalization capabilities on the public IU-Xray and MIMIC-CXR datasets for chest-related diseases.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MCS: An In-battle Commentary System for MOBA Games",
        "paper_url": "https://aclanthology.org/2022.coling-1.262/",
        "paper_authors": [
            "Xiaofeng Qi",
            "Chao Li",
            "Zhongping Liang",
            "Jigang Liu",
            "Cheng Zhang",
            "Yuanxin Wei",
            "Lin Yuan",
            "Guang Yang",
            "Lanxiao Huang",
            "Min Li"
        ],
        "paper_abstract": "This paper introduces a generative system for in-battle real-time commentary in mobile MOBA games. Event commentary is important for battles in MOBA games, which is applicable to a wide range of scenarios like live streaming, e-sports commentary and combat information analysis. The system takes real-time match statistics and events as input, and an effective transform method is designed to convert match statistics and utterances into consistent encoding space. This paper presents the general framework and implementation details of the proposed system, and provides experimental results on large-scale real-world match data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Two Stage Adaptation Framework for Frame Detection via Prompt Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.263/",
        "paper_authors": [
            "Xinyi Mou",
            "Zhongyu Wei",
            "Changjian Jiang",
            "Jiajie Peng"
        ],
        "paper_abstract": "Framing is a communication strategy to bias discussion by selecting and emphasizing. Frame detection aims to automatically analyze framing strategy. Previous works on frame detection mainly focus on a single scenario or issue, ignoring the special characteristics of frame detection that new events emerge continuously and policy agenda changes dynamically. To better deal with various context and frame typologies across different issues, we propose a two-stage adaptation framework. In the framing domain adaptation from pre-training stage, we design two tasks based on pivots and prompts to learn a transferable encoder, verbalizer, and prompts. In the downstream scenario generalization stage, the transferable components are applied to new issues and label sets. Experiment results demonstrate the effectiveness of our framework in different scenarios. Also, it shows superiority both in full-resource and low-resource conditions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Summarizing Patients\u2019 Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.264/",
        "paper_authors": [
            "Yanjun Gao",
            "Dmitriy Dligach",
            "Timothy Miller",
            "Dongfang Xu",
            "Matthew M. M. Churpek",
            "Majid Afshar"
        ],
        "paper_abstract": "Automatically summarizing patients\u2019 main problems from daily progress notes using natural language processing methods helps to battle against information and cognitive overload in hospital settings and potentially assists providers with computerized diagnostic decision support. Problem list summarization requires a model to understand, abstract, and generate clinical documentation. In this work, we propose a new NLP task that aims to generate a list of problems in a patient\u2019s daily care plan using input from the provider\u2019s progress notes during hospitalization. We investigate the performance of T5 and BART, two state-of-the-art seq2seq transformer architectures, in solving this problem. We provide a corpus built on top of progress notes from publicly available electronic health record progress notes in the Medical Information Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain text, and we experiment with a data augmentation method and a domain adaptation pre-training method to increase exposure to medical vocabulary and knowledge. Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence embedding, and F-score on medical concepts. Results show that T5 with domain adaptive pre-training achieves significant performance gains compared to a rule-based system and general domain pre-trained language models, indicating a promising direction for tackling the problem summarization task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Human-in-the-loop Robotic Grasping Using BERT Scene Representation",
        "paper_url": "https://aclanthology.org/2022.coling-1.265/",
        "paper_authors": [
            "Yaoxian Song",
            "Penglei Sun",
            "Pengfei Fang",
            "Linyi Yang",
            "Yanghua Xiao",
            "Yue Zhang"
        ],
        "paper_abstract": "Current NLP techniques have been greatly applied in different domains. In this paper, we propose a human-in-the-loop framework for robotic grasping in cluttered scenes, investigating a language interface to the grasping process, which allows the user to intervene by natural language commands. This framework is constructed on a state-of-the-art grasping baseline, where we substitute a scene-graph representation with a text representation of the scene using BERT. Experiments on both simulation and physical robot show that the proposed method outperforms conventional object-agnostic and scene-graph based methods in the literature. In addition, we find that with human intervention, performance can be significantly improved. Our dataset and code are available on our project website https://sites.google.com/view/hitl-grasping-bert.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automated Chinese Essay Scoring from Multiple Traits",
        "paper_url": "https://aclanthology.org/2022.coling-1.266/",
        "paper_authors": [
            "Yaqiong He",
            "Feng Jiang",
            "Xiaomin Chu",
            "Peifeng Li"
        ],
        "paper_abstract": "Automatic Essay Scoring (AES) is the task of using the computer to evaluate the quality of essays automatically. Current research on AES focuses on scoring the overall quality or single trait of prompt-specific essays. However, the users not only expect to obtain the overall score but also the instant feedback from different traits to help their writing in the real world. Therefore, we first annotate a mutli-trait dataset ACEA including 1220 argumentative essays from four traits, i.e., essay organization, topic, logic, and language. And then we design a hierarchical multi-task trait scorer HMTS to evaluate the quality of writing by modeling these four traits. Moreover, we propose an inter-sequence attention mechanism to enhance information interaction between different tasks and design the trait-specific features for various tasks in AES. The experimental results on ACEA show that our HMTS can effectively score essays from multiple traits, outperforming several strong models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantic-Preserving Adversarial Code Comprehension",
        "paper_url": "https://aclanthology.org/2022.coling-1.267/",
        "paper_authors": [
            "Yiyang Li",
            "Hongqiu Wu",
            "Hai Zhao"
        ],
        "paper_abstract": "Based on the tremendous success of pre-trained language models (PrLMs) for source code comprehension tasks, current literature studies either ways to further improve the performance (generalization) of PrLMs, or their robustness against adversarial attacks. However, they have to compromise on the trade-off between the two aspects and none of them consider improving both sides in an effective and practical way. To fill this gap, we propose Semantic-Preserving Adversarial Code Embeddings (SPACE) to find the worst-case semantic-preserving attacks while forcing the model to predict the correct labels under these worst cases. Experiments and analysis demonstrate that SPACE can stay robust against state-of-the-art attacks while boosting the performance of PrLMs for code.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning",
        "paper_url": "https://aclanthology.org/2022.coling-1.268/",
        "paper_authors": [
            "Yuhui Zuo",
            "Wei Zhu",
            "Guoyong GUET Cai"
        ],
        "paper_abstract": "Since open social platforms allow for a large and continuous flow of unverified information, rumors can emerge unexpectedly and spread quickly. However, existing rumor detection (RD) models often assume the same training and testing distributions and can not cope with the continuously changing social network environment. This paper proposed a Continual Prompt-Tuning RD (CPT-RD) framework, which avoids catastrophic forgetting (CF) of upstream tasks during sequential task learning and enables bidirectional knowledge transfer between domain tasks. Specifically, we propose the following strategies: (a) Our design explicitly decouples shared and domain-specific knowledge, thus reducing the interference among different domains during optimization; (b) Several technologies aim to transfer knowledge of upstream tasks to deal with emergencies; (c) A task-conditioned prompt-wise hypernetwork (TPHNet) is used to consolidate past domains. In addition, CPT-RD avoids CF without the necessity of a rehearsal buffer. Finally, CPT-RD is evaluated on English and Chinese RD datasets and is effective and efficient compared to prior state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "AiM: Taking Answers in Mind to Correct Chinese Cloze Tests in Educational Applications",
        "paper_url": "https://aclanthology.org/2022.coling-1.269/",
        "paper_authors": [
            "Yusen Zhang",
            "Zhongli Li",
            "Qingyu Zhou",
            "Ziyi Liu",
            "Chao Li",
            "Mina Ma",
            "Yunbo Cao",
            "Hongzhi Liu"
        ],
        "paper_abstract": "To automatically correct handwritten assignments, the traditional approach is to use an OCR model to recognize characters and compare them to answers. The OCR model easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference. However, teachers always have these answers in mind to review and correct assignments. In this paper, we focus on the Chinese cloze tests correction and propose a multimodal approach(named AiM). The encoded representations of answers interact with the visual information of students\u2019 handwriting. Instead of predicting \u2018right\u2019 or \u2018wrong\u2019, we perform the sequence labeling on the answer text to infer which answer character differs from the handwritten content in a fine-grained way. We take samples of OCR datasets as the positive samples for this task, and develop a negative sample augmentation method to scale up the training data. Experimental results show that AiM outperforms OCR-based methods by a large margin. Extensive studies demonstrate the effectiveness of our multimodal approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding",
        "paper_url": "https://aclanthology.org/2022.coling-1.270/",
        "paper_authors": [
            "Zichen Liu",
            "Xuyuan Liu",
            "Yanlong Wen",
            "Guoqing Zhao",
            "Fen Xia",
            "Xiaojie Yuan"
        ],
        "paper_abstract": "ICD coding is designed to assign the disease codes to electronic health records (EHRs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict ICD codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in EHRs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal Attention Network (TreeMAN) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about ICD coding. We can apply the same multi-label classifier from previous text models to the multimodal representations to predict ICD codes. Experiments on two MIMIC datasets show that our method outperforms prior state-of-the-art ICD coding approaches. The code is available at https://github.com/liu-zichen/TreeMAN.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing",
        "paper_url": "https://aclanthology.org/2022.coling-1.271/",
        "paper_authors": [
            "Ziming Huang",
            "Zhuoxuan Jiang",
            "Ke Wang",
            "Juntao Li",
            "Shanshan Feng",
            "Xian-Ling Mao"
        ],
        "paper_abstract": "Currently, human-bot symbiosis dialog systems, e.g. pre- and after-sales in E-commerce, are ubiquitous, and the dialog routing component is essential to improve the overall efficiency, reduce human resource cost and increase user experience. To satisfy this requirement, existing methods are mostly heuristic and cannot obtain high-quality performance. In this paper, we investigate the important problem by thoroughly mining both the data-to-task and task-to-task knowledge among various kinds of dialog data. To achieve the above target, we propose a comprehensive and general solution with multi-task learning framework, specifically including a novel dialog encoder and two tailored gated mechanism modules. The proposed Gated Mechanism enhanced Multi-task Model (G3M) can play the role of hierarchical information filtering and is non-invasive to the existing dialog systems. Experiments on two datasets collected from the real world demonstrate our method\u2019s effectiveness and the results achieve the state-of-the-art performance by relatively increasing 8.7%/11.8% on RMSE metric and 2.2%/4.4% on F1 metric.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Negation, Coordination, and Quantifiers in Contextualized Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.272/",
        "paper_authors": [
            "Aikaterini-Lida Kalouli",
            "Rita Sevastjanova",
            "Christin Beck",
            "Maribel Romero"
        ],
        "paper_abstract": "With the success of contextualized language models, much research explores what these models really learn and in which cases they still fail. Most of this work focuses on specific NLP tasks and on the learning outcome. Little research has attempted to decouple the models\u2019 weaknesses from specific tasks and focus on the embeddings per se and their mode of learning. In this paper, we take up this research opportunity: based on theoretical linguistic insights, we explore whether the semantic constraints of function words are learned and how the surrounding context impacts their embeddings. We create suitable datasets, provide new insights into the inner workings of LMs vis-a-vis function words and implement an assisting visual web interface for qualitative analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Tales and Tropes: Gender Roles from Word Embeddings in a Century of Children\u2019s Books",
        "paper_url": "https://aclanthology.org/2022.coling-1.273/",
        "paper_authors": [
            "Anjali Adukia",
            "Patricia Chiril",
            "Callista Christ",
            "Anjali Das",
            "Alex Eble",
            "Emileigh Harrison",
            "Hakizumwami Birali Runesha"
        ],
        "paper_abstract": "The manner in which gender is portrayed in materials used to teach children conveys messages about people\u2019s roles in society. In this paper, we measure the gendered depiction of central domains of social life in 100 years of highly influential children\u2019s books. We make two main contributions: (1) we find that the portrayal of gender in these books reproduces traditional gender norms in society, and (2) we publish StoryWords 1.0, the first word embeddings trained on such a large body of children\u2019s literature. We find that, relative to males, females are more likely to be represented in relation to their appearance than in relation to their competence; second, they are more likely to be represented in relation to their role in the family than their role in business. Finally, we find that non-binary or gender-fluid individuals are rarely mentioned. Our analysis advances understanding of the different messages contained in content commonly used to teach children, with immediate applications for practice, policy, and research.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations",
        "paper_url": "https://aclanthology.org/2022.coling-1.274/",
        "paper_authors": [
            "Borun Chen",
            "Hongyin Tang",
            "Jiahao Bu",
            "Kai Zhang",
            "Jingang Wang",
            "Qifan Wang",
            "Hai-Tao Zheng",
            "Wei Wu",
            "Liqian Yu"
        ],
        "paper_abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On the Nature of BERT: Correlating Fine-Tuning and Linguistic Competence",
        "paper_url": "https://aclanthology.org/2022.coling-1.275/",
        "paper_authors": [
            "Federica Merendi",
            "Felice Dell\u2019Orletta",
            "Giulia Venturi"
        ],
        "paper_abstract": "Several studies in the literature on the interpretation of Neural Language Models (NLM) focus on the linguistic generalization abilities of pre-trained models. However, little attention is paid to how the linguistic knowledge of the models changes during the fine-tuning steps. In this paper, we contribute to this line of research by showing to what extent a wide range of linguistic phenomena are forgotten across 50 epochs of fine-tuning, and how the preserved linguistic knowledge is correlated with the resolution of the fine-tuning task. To this end, we considered a quite understudied task where linguistic information plays the main role, i.e. the prediction of the evolution of written language competence of native language learners. In addition, we investigate whether it is possible to predict the fine-tuned NLM accuracy across the 50 epochs solely relying on the assessed linguistic competence. Our results are encouraging and show a high relationship between the model\u2019s linguistic competence and its ability to solve a linguistically-based downstream task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LayerConnect: Hypernetwork-Assisted Inter-Layer Connector to Enhance Parameter Efficiency",
        "paper_url": "https://aclanthology.org/2022.coling-1.276/",
        "paper_authors": [
            "Haoxiang Shi",
            "Rongsheng Zhang",
            "Jiaan Wang",
            "Cen Wang",
            "Yinhe Zheng",
            "Tetsuya Sakai"
        ],
        "paper_abstract": "Pre-trained Language Models (PLMs) are the cornerstone of the modern Natural Language Processing (NLP). However, as PLMs become heavier, fine tuning all their parameters loses their efficiency. Existing parameter-efficient methods generally focus on reducing the trainable parameters in PLMs but neglect the inference speed, which limits the ability to deploy PLMs. In this paper, we propose LayerConnect (hypernetwork-assisted inter-layer connectors) to enhance inference efficiency. Specifically, a light-weight connector with a linear structure is inserted between two Transformer layers, and the parameters inside each connector are tuned by a hypernetwork comprising an interpolator and a down-sampler. We perform extensive experiments on the widely used the GLUE benchmark. The experimental results verify the inference efficiency of our model. Compared to Adapter, our model parameters are reduced to approximately 11.75%, while the performance degradation is kept to less than 5% (2.5 points on average).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Effect of Post-processing on Contextualized Word Representations",
        "paper_url": "https://aclanthology.org/2022.coling-1.277/",
        "paper_authors": [
            "Hassan Sajjad",
            "Firoj Alam",
            "Fahim Dalvi",
            "Nadir Durrani"
        ],
        "paper_abstract": "Post-processing of static embedding has been shown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principal components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy) and sequence classification tasks. Our findings raise interesting points in relation to the research studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Does BERT Rediscover a Classical NLP Pipeline?",
        "paper_url": "https://aclanthology.org/2022.coling-1.278/",
        "paper_authors": [
            "Jingcheng Niu",
            "Wenjie Lu",
            "Gerald Penn"
        ],
        "paper_abstract": "Does BERT store surface knowledge in its bottom layers, syntactic knowledge in its middle layers, and semantic knowledge in its upper layers? In re-examining Jawahar et al. (2019) and Tenney et al.\u2019s (2019a) probes into the structure of BERT, we have found that the pipeline-like separation that they asserted lacks conclusive empirical support. BERT\u2019s structure is, however, linguistically founded, although perhaps in a way that is more nuanced than can be explained by layers alone. We introduce a novel probe, called GridLoc, through which we can also take into account token positions, training rounds, and random seeds. Using GridLoc, we are able to detect other, stronger regularities that suggest that pseudo-cognitive appeals to layer depth may not be the preferable mode of explanation for BERT\u2019s inner workings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "HG2Vec: Improved Word Embeddings from Dictionary and Thesaurus Based Heterogeneous Graph",
        "paper_url": "https://aclanthology.org/2022.coling-1.279/",
        "paper_authors": [
            "Qitong Wang",
            "Mohammed J Zaki"
        ],
        "paper_abstract": "Learning word embeddings is an essential topic in natural language processing. Most existing works use a vast corpus as a primary source while training, but this requires massive time and space for data pre-processing and model training. We propose a new model, HG2Vec, that learns word embeddings utilizing only dictionaries and thesauri. Our model reaches the state-of-art on multiple word similarity and relatedness benchmarks. We demonstrate that dictionaries and thesauri are effective resources to learn word embeddings. In addition, we exploit a new context-focused loss that models transitive relationships between word pairs and balances the performance between similarity and relatedness benchmarks, yielding superior results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Transferring Knowledge from Structure-aware Self-attention Language Model to Sequence-to-Sequence Semantic Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.280/",
        "paper_authors": [
            "Ran Ji",
            "Jianmin Ji"
        ],
        "paper_abstract": "Semantic parsing considers the task of mapping a natural language sentence into a target formal representation, where various sophisticated sequence-to-sequence (seq2seq) models have been applied with promising results. Generally, these target representations follow a syntax formalism that limits permitted forms. However, it is neither easy nor flexible to explicitly integrate this syntax formalism into a neural seq2seq model. In this paper, we present a structure-aware self-attention language model to capture structural information of target representations and propose a knowledge distillation based approach to incorporating the target language model into a seq2seq model, where grammar rules or sketches are not required in the training process. An ablation study shows that the proposed language model can notably improve the performance of the baseline model. The experiments show that our method achieves new state-of-the-art performance among neural approaches on four semantic parsing (ATIS, GEO) and Python code generation (Django, CoNaLa) tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enhancing Contextual Word Representations Using Embedding of Neighboring Entities in Knowledge Graphs",
        "paper_url": "https://aclanthology.org/2022.coling-1.281/",
        "paper_authors": [
            "Ryoko Tokuhisa",
            "Keisuke Kawano",
            "Akihiro Nakamura",
            "Satoshi Koide"
        ],
        "paper_abstract": "Pre-trained language models (PLMs) such as BERT and RoBERTa have dramatically improved the performance of various natural language processing tasks. Although these models are trained on large amounts of raw text, they have no explicit grounding in real-world entities. Knowledge graphs (KGs) are manually annotated with factual knowledge and store the relations between nodes corresponding to entities as labeled edges. This paper proposes a mechanism called KG-attention, which integrates the structure of a KG into recent PLM architectures. Unlike the existing PLM+KG integration methods, KG-attention generalizes the embeddings of neighboring entities using the relation embeddings; accordingly, it can handle relations between unconnected entities in the KG. Experimental results demonstrated that our method achieved significant improvements in a relation classification task, an entity typing task, and several language comprehension tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generic Overgeneralization in Pre-trained Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.282/",
        "paper_authors": [
            "Sello Ralethe",
            "Jan Buys"
        ],
        "paper_abstract": "Generic statements such as \u201cducks lay eggs\u201d make claims about kinds, e.g., ducks as a category. The generic overgeneralization effect refers to the inclination to accept false universal generalizations such as \u201call ducks lay eggs\u201d or \u201call lions have manes\u201d as true. In this paper, we investigate the generic overgeneralization effect in pre-trained language models experimentally. We show that pre-trained language models suffer from overgeneralization and tend to treat quantified generic statements such as \u201call ducks lay eggs\u201d as if they were true generics. Furthermore, we demonstrate how knowledge embedding methods can lessen this effect by injecting factual knowledge about kinds into pre-trained language models. To this end, we source factual knowledge about two types of generics, minority characteristic generics and majority characteristic generics, and inject this knowledge using a knowledge embedding model. Our results show that knowledge injection reduces, but does not eliminate, generic overgeneralization, and that majority characteristic generics of kinds are more susceptible to overgeneralization bias.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "How about Time? Probing a Multilingual Language Model for Temporal Relations",
        "paper_url": "https://aclanthology.org/2022.coling-1.283/",
        "paper_authors": [
            "Tommaso Caselli",
            "Irene Dini",
            "Felice Dell\u2019Orletta"
        ],
        "paper_abstract": "This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages. Results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddings. While obtaining competitive results against state-of-the-art systems, our probes indicate a lack of suitable encoded information to properly address this task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CogBERT: Cognition-Guided Pre-trained Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.284/",
        "paper_authors": [
            "Xiao Ding",
            "Bowen Chen",
            "Li Du",
            "Bing Qin",
            "Ting Liu"
        ],
        "paper_abstract": "We study the problem of integrating cognitive language processing signals (e.g., eye-tracking or EEG data) into pre-trained language models like BERT. Existing methods typically fine-tune pre-trained models on cognitive data, ignoring the semantic gap between the texts and cognitive signals. To fill the gap, we propose CogBERT, a framework that can induce fine-grained cognitive features from cognitive data and incorporate cognitive features into BERT by adaptively adjusting the weight of cognitive features for different NLP tasks. Extensive experiments show that: (1) Cognition-guided pre-trained models can consistently perform better than basic pre-trained models on ten NLP tasks. (2) Different cognitive features contribute differently to different NLP tasks. Based on this observation, we give a fine-grained explanation of why cognitive data is helpful for NLP. (3) Different transformer layers of pre-trained models should encode different cognitive features, with word-level cognitive features at the bottom and semantic-level cognitive features at the top. (4) Attention visualization demonstrates that CogBERT aligns with human gaze patterns and improves its natural language comprehension ability.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Can Transformers Process Recursive Nested Constructions, Like Humans?",
        "paper_url": "https://aclanthology.org/2022.coling-1.285/",
        "paper_authors": [
            "Yair Lakretz",
            "Th\u00e9o Desbordes",
            "Dieuwke Hupkes",
            "Stanislas Dehaene"
        ],
        "paper_abstract": "Recursive processing is considered a hallmark of human linguistic abilities. A recent study evaluated recursive processing in recurrent neural language models (RNN-LMs) and showed that such models perform below chance level on embedded dependencies within nested constructions \u2013 a prototypical example of recursion in natural language. Here, we study if state-of-the-art Transformer LMs do any better. We test eight different Transformer LMs on two different types of nested constructions, which differ in whether the embedded (inner) dependency is short or long range. We find that Transformers achieve near-perfect performance on short-range embedded dependencies, significantly better than previous results reported for RNN-LMs and humans. However, on long-range embedded dependencies, Transformers\u2019 performance sharply drops below chance level. Remarkably, the addition of only three words to the embedded dependency caused Transformers to fall from near-perfect to below-chance performance. Taken together, our results reveal how brittle syntactic processing is in Transformers, compared to humans.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "NSP-BERT: A Prompt-based Few-Shot Learner through an Original Pre-training Task \u2014\u2014 Next Sentence Prediction",
        "paper_url": "https://aclanthology.org/2022.coling-1.286/",
        "paper_authors": [
            "Yi Sun",
            "Yu Zheng",
            "Chao Hao",
            "Hangping Qiu"
        ],
        "paper_abstract": "Using prompts to utilize language models to perform various downstream tasks, also known as prompt-based learning or prompt-learning, has lately gained significant success in comparison to the pre-train and fine-tune paradigm. Nonetheless, virtually most prompt-based methods are token-level such as PET based on mask language model (MLM). In this paper, we attempt to accomplish several NLP tasks in the zero-shot and few-shot scenarios using a BERT original pre-training task abandoned by RoBERTa and other models\u2014\u2014Next Sentence Prediction (NSP). Unlike token-level techniques, our sentence-level prompt-based method NSP-BERT does not need to fix the length of the prompt or the position to be predicted, allowing it to handle tasks such as entity linking with ease. NSP-BERT can be applied to a variety of tasks based on its properties. We present an NSP-tuning approach with binary cross-entropy loss for single-sentence classification tasks that is competitive compared to PET and EFL. By continuing to train BERT on RoBERTa\u2019s corpus, the model\u2019s performance improved significantly, which indicates that the pre-training corpus is another important determinant of few-shot besides model size and prompt method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MetaPrompting: Learning to Learn Better Prompts",
        "paper_url": "https://aclanthology.org/2022.coling-1.287/",
        "paper_authors": [
            "Yutai Hou",
            "Hongyuan Dong",
            "Xinghao Wang",
            "Bohan Li",
            "Wanxiang Che"
        ],
        "paper_abstract": "Prompting method is regarded as one of the crucial progress for few-shot nature language processing. Recent research on prompting moves from discrete tokens based \u201chard prompts\u201d to continuous \u201csoft prompts\u201d, which employ learnable vectors as pseudo prompt tokens and achieve better performance. Though showing promising prospects, these soft-prompting methods are observed to rely heavily on good initialization to take effect. Unfortunately, obtaining a perfect initialization for soft prompts requires understanding of inner language models working and elaborate design, which is no easy task and has to restart from scratch for each new task. To remedy this, we propose a generalized soft prompting method called MetaPrompting, which adopts the well-recognized model-agnostic meta-learning algorithm to automatically find better prompt initialization that facilitates fast adaptation to new prompting tasks. Extensive experiments show MetaPrompting tackles soft prompt initialization problem and brings significant improvement on three different datasets (over 7 points improvement in accuracy for 1-shot setting), achieving new state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.288/",
        "paper_authors": [
            "Ze-Feng Gao",
            "Peiyu Liu",
            "Wayne Xin Zhao",
            "Zhong-Yi Lu",
            "Ji-Rong Wen"
        ],
        "paper_abstract": "Recently, Mixture-of-Experts (short as MoE) architecture has achieved remarkable success in increasing the model capacity of large-scale language models. However, MoE requires incorporating significantly more parameters than the base model being extended. In this paper, we propose building a parameter-efficient MoE architecture by sharing information across experts. We adopt matrix product operator (MPO, a tensor decomposition from quantum many-body physics) to reconstruct the parameter matrix in the expert layer and increase model capacity for pre-trained language models by sharing parameters of the central tensor (containing the core information) among different experts while enabling the specificity through the auxiliary tensors (complementing the central tensor) of different experts. To address the unbalanced optimization issue, we further design the gradient mask strategy for the MPO-based MoE architecture. Extensive experiments based on T5 and GPT-2 show improved performance and efficiency of the pre-trained language model (27.2x reduction in total parameters for the superior model performance, compared with the Switch Transformers). Our code is publicly available at https://github.com/RUCAIBox/MPO/MPOE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pre-trained Token-replaced Detection Model as Few-shot Learner",
        "paper_url": "https://aclanthology.org/2022.coling-1.289/",
        "paper_authors": [
            "Zicheng Li",
            "Shoushan Li",
            "Guodong Zhou"
        ],
        "paper_abstract": "Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating Diversity of Multiword Expressions in Annotated Text",
        "paper_url": "https://aclanthology.org/2022.coling-1.290/",
        "paper_authors": [
            "Adam Lion-Bouton",
            "Yagmur Ozturk",
            "Agata Savary",
            "Jean-Yves Antoine"
        ],
        "paper_abstract": "Diversity can be decomposed into three distinct concepts, namely: variety, balance and disparity. This paper borrows from the extensive formalization and measures of diversity developed in ecology in order to evaluate the variety and balance of multiword expression annotation produced by automatic annotation systems. The measures of richness, normalized richness, and two variations of Hill\u2019s evenness are considered in this paper. We observe how these measures behave against increasingly smaller samples of gold annotations of multiword expressions and use their comportment to validate or invalidate their pertinence for multiword expressions in annotated texts. We apply the validated measures to annotations in 14 languages produced by systems during the PARSEME shared task on automatic identification of multiword expressions and on the gold versions of the corpora. We also explore the limits of such evaluation by studying the impact of lemmatization errors in the Turkish corpus used in the shared task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CausalQA: A Benchmark for Causal Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.291/",
        "paper_authors": [
            "Alexander Bondarenko",
            "Magdalena Wolska",
            "Stefan Heindorf",
            "Lukas Bl\u00fcbaum",
            "Axel-Cyrille Ngonga Ngomo",
            "Benno Stein",
            "Pavel Braslavski",
            "Matthias Hagen",
            "Martin Potthast"
        ],
        "paper_abstract": "At least 5% of questions submitted to search engines ask about cause-effect relationships in some way. To support the development of tailored approaches that can answer such questions, we construct Webis-CausalQA-22, a benchmark corpus of 1.1 million causal questions with answers. We distinguish different types of causal questions using a novel typology derived from a data-driven, manual analysis of questions from ten large question answering (QA) datasets. Using high-precision lexical rules, we extract causal questions of each type from these datasets to create our corpus. As an initial baseline, the state-of-the-art QA model UnifiedQA achieves a ROUGE-L F1 score of 0.48 on our new benchmark.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.292/",
        "paper_authors": [
            "Amir Pouran Ben Veyseh",
            "Nicole Meister",
            "Seunghyun Yoon",
            "Rajiv Jain",
            "Franck Dernoncourt",
            "Thien Huu Nguyen"
        ],
        "paper_abstract": "Acronym extraction is the task of identifying acronyms and their expanded forms in texts that is necessary for various NLP applications. Despite major progress for this task in recent years, one limitation of existing AE research is that they are limited to the English language and certain domains (i.e., scientific and biomedical). Challenges of AE in other languages and domains are mainly unexplored. As such, lacking annotated datasets in multiple languages and domains has been a major issue to prevent research in this direction. To address this limitation, we propose a new dataset for multilingual and multi-domain AE. Specifically, 27,200 sentences in 6 different languages and 2 new domains, i.e., legal and scientific, are manually annotated for AE. Our experiments on the dataset show that AE in different languages and learning settings has unique challenges, emphasizing the necessity of further research on multilingual and multi-domain AE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Curating a Large-Scale Motivational Interviewing Dataset Using Peer Support Forums",
        "paper_url": "https://aclanthology.org/2022.coling-1.293/",
        "paper_authors": [
            "Anuradha Welivita",
            "Pearl Pu"
        ],
        "paper_abstract": "A significant limitation in developing therapeutic chatbots to support people going through psychological distress is the lack of high-quality, large-scale datasets capturing conversations between clients and trained counselors. As a remedy, researchers have focused their attention on scraping conversational data from peer support platforms such as Reddit. But the extent to which the responses from peers align with responses from trained counselors is understudied. We address this gap by analyzing the differences between responses from counselors and peers by getting trained counselors to annotate \u224817K such responses using Motivational Interviewing Treatment Integrity (MITI) code, a well-established behavioral coding system that differentiates between favorable and unfavorable responses. We developed an annotation pipeline with several stages of quality control. Due to its design, this method was able to achieve 97% of coverage, meaning that out of the 17.3K responses we successfully labeled 16.8K with a moderate agreement. We use this data to conclude the extent to which conversational data from peer support platforms align with real therapeutic conversations and discuss in what ways they can be exploited to train therapeutic chatbots.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CCTC: A Cross-Sentence Chinese Text Correction Dataset for Native Speakers",
        "paper_url": "https://aclanthology.org/2022.coling-1.294/",
        "paper_authors": [
            "Baoxin Wang",
            "Xingyi Duan",
            "Dayong Wu",
            "Wanxiang Che",
            "Zhigang Chen",
            "Guoping Hu"
        ],
        "paper_abstract": "The Chinese text correction (CTC) focuses on detecting and correcting Chinese spelling errors and grammatical errors. Most existing datasets of Chinese spelling check (CSC) and Chinese grammatical error correction (GEC) are focused on a single sentence written by Chinese-as-a-second-language (CSL) learners. We find that errors caused by native speakers differ significantly from those produced by non-native speakers. These differences make it inappropriate to use the existing test sets directly to evaluate text correction systems for native speakers. Some errors also require the cross-sentence information to be identified and corrected. In this paper, we propose a cross-sentence Chinese text correction dataset for native speakers. Concretely, we manually annotated 1,500 texts written by native speakers. The dataset consists of 30,811 sentences and more than 1,000,000 Chinese characters. It contains four types of errors: spelling errors, redundant words, missing words, and word ordering errors. We also test some state-of-the-art models on the dataset. The experimental results show that even the model with the best performance is 20 points lower than humans, which indicates that there is still much room for improvement. We hope that the new dataset can fill the gap in cross-sentence text correction for native Chinese speakers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "RealMedDial: A Real Telemedical Dialogue Dataset Collected from Online Chinese Short-Video Clips",
        "paper_url": "https://aclanthology.org/2022.coling-1.295/",
        "paper_authors": [
            "Bo Xu",
            "Hongtong Zhang",
            "Jian Wang",
            "Xiaokun Zhang",
            "Dezhi Hao",
            "Linlin Zong",
            "Hongfei Lin",
            "Fenglong Ma"
        ],
        "paper_abstract": "Intelligent medical services have attracted great research interests for providing automated medical consultation. However, the lack of corpora becomes a main obstacle to related research, particularly data from real scenarios. In this paper, we construct RealMedDial, a Chinese medical dialogue dataset based on real medical consultation. RealMedDial contains 2,637 medical dialogues and 24,255 utterances obtained from Chinese short-video clips of real medical consultations. We collected and annotated a wide range of meta-data with respect to medical dialogue including doctor profiles, hospital departments, diseases and symptoms for fine-grained analysis on language usage pattern and clinical diagnosis. We evaluate the performance of medical response generation, department routing and doctor recommendation on RealMedDial. Results show that RealMedDial are applicable to a wide range of NLP tasks with respect to medical dialogue.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TempoWiC: An Evaluation Benchmark for Detecting Meaning Shift in Social Media",
        "paper_url": "https://aclanthology.org/2022.coling-1.296/",
        "paper_authors": [
            "Daniel Loureiro",
            "Aminette D\u2019Souza",
            "Areej Nasser Muhajab",
            "Isabella A. White",
            "Gabriel Wong",
            "Luis Espinosa-Anke",
            "Leonardo Neves",
            "Francesco Barbieri",
            "Jose Camacho-Collados"
        ],
        "paper_abstract": "Language evolves over time, and word meaning changes accordingly. This is especially true in social media, since its dynamic nature leads to faster semantic shifts, making it challenging for NLP models to deal with new content and trends. However, the number of datasets and models that specifically address the dynamic nature of these social platforms is scarce. To bridge this gap, we present TempoWiC, a new benchmark especially aimed at accelerating research in social media-based meaning shift. Our results show that TempoWiC is a challenging benchmark, even for recently-released language models specialized in social media.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automatic Generation of Large-scale Multi-turn Dialogues from Reddit",
        "paper_url": "https://aclanthology.org/2022.coling-1.297/",
        "paper_authors": [
            "Daniil Huryn",
            "William M. Hutsell",
            "Jinho D. Choi"
        ],
        "paper_abstract": "This paper presents novel methods to automatically convert posts and their comments from discussion forums such as Reddit into multi-turn dialogues. Our methods are generalizable to any forums; thus, they allow us to generate a massive amount of dialogues for diverse topics that can be used to pretrain language models. Four methods are introduced, Greedy_Baseline, Greedy_Advanced, Beam Search and Threading, which are applied to posts from 10 subreddits and assessed. Each method makes a noticeable improvement over its predecessor such that the best method shows an improvement of 36.3% over the baseline for appropriateness. Our best method is applied to posts from those 10 subreddits for the creation of a corpus comprising 10,098 dialogues (3.3M tokens), 570 of which are compared against dialogues in three other datasets, Blended Skill Talk, Daily Dialogue, and Topical Chat. Our dialogues are found to be more engaging but slightly less natural than the ones in the other datasets, while it costs a fraction of human labor and money to generate our corpus compared to the others. To the best of our knowledge, it is the first work to create a large multi-turn dialogue corpus from Reddit that can advance neural dialogue systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ConFiguRe: Exploring Discourse-level Chinese Figures of Speech",
        "paper_url": "https://aclanthology.org/2022.coling-1.298/",
        "paper_authors": [
            "Dawei Zhu",
            "Qiusi Zhan",
            "Zhejian Zhou",
            "Yifan Song",
            "Jiebin Zhang",
            "Sujian Li"
        ],
        "paper_abstract": "Figures of speech, such as metaphor and irony, are ubiquitous in literature works and colloquial conversations. This poses great challenge for natural language understanding since figures of speech usually deviate from their ostensible meanings to express deeper semantic implications. Previous research lays emphasis on the literary aspect of figures and seldom provide a comprehensive exploration from a view of computational linguistics. In this paper, we first propose the concept of figurative unit, which is the carrier of a figure. Then we select 12 types of figures commonly used in Chinese, and build a Chinese corpus for Contextualized Figure Recognition (ConFiguRe). Different from previous token-level or sentence-level counterparts, ConFiguRe aims at extracting a figurative unit from discourse-level context, and classifying the figurative unit into the right figure type. On ConFiguRe, three tasks, i.e., figure extraction, figure type classification and figure recognition, are designed and the state-of-the-art techniques are utilized to implement the benchmarks. We conduct thorough experiments and show that all three tasks are challenging for existing models, thus requiring further research. Our dataset and code are publicly available at https://github.com/pku-tangent/ConFiguRe.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Twitter Topic Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.299/",
        "paper_authors": [
            "Dimosthenis Antypas",
            "Asahi Ushio",
            "Jose Camacho-Collados",
            "Vitor Silva",
            "Leonardo Neves",
            "Francesco Barbieri"
        ],
        "paper_abstract": "Social media platforms host discussions about a wide variety of topics that arise everyday. Making sense of all the content and organising it into categories is an arduous task. A common way to deal with this issue is relying on topic modeling, but topics discovered using this technique are difficult to interpret and can differ from corpus to corpus. In this paper, we present a new task based on tweet topic classification and release two associated datasets. Given a wide range of topics covering the most important discussion points in social media, we provide training and testing data from recent time periods that can be used to evaluate tweet classification models. Moreover, we perform a quantitative evaluation and analysis of current general- and domain-specific language models on the task, which provide more insights on the challenges and nature of the task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?",
        "paper_url": "https://aclanthology.org/2022.coling-1.300/",
        "paper_authors": [
            "Doan Nam Long Vu",
            "Nafise Sadat Moosavi",
            "Steffen Eger"
        ],
        "paper_abstract": "The evaluation of recent embedding-based evaluation metrics for text generation is primarily based on measuring their correlation with human evaluations on standard benchmarks. However, these benchmarks are mostly from similar domains to those used for pretraining word embeddings. This raises concerns about the (lack of) generalization of embedding-based metrics to new and noisy domains that contain a different vocabulary than the pretraining data. In this paper, we examine the robustness of BERTScore, one of the most popular embedding-based metrics for text generation. We show that (a) an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, (b) taking embeddings from the first layer of pretrained models improves the robustness of all metrics, and (c) the highest robustness is achieved when using character-level embeddings, instead of token-based embeddings, from the first layer of the pretrained model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating the Performance of Transformer-based Language Models for Neuroatypical Language",
        "paper_url": "https://aclanthology.org/2022.coling-1.301/",
        "paper_authors": [
            "Duanchen Liu",
            "Zoey Liu",
            "Qingyun Yang",
            "Yujing Huang",
            "Emily Prud\u2019hommeaux"
        ],
        "paper_abstract": "Difficulties with social aspects of language are among the hallmarks of autism spectrum disorder (ASD). These communication differences are thought to contribute to the challenges that adults with ASD experience when seeking employment, underscoring the need for interventions that focus on improving areas of weakness in pragmatic and social language. In this paper, we describe a transformer-based framework for identifying linguistic features associated with social aspects of communication using a corpus of conversations between adults with and without ASD and neurotypical conversational partners produced while engaging in collaborative tasks. While our framework yields strong accuracy overall, performance is significantly worse for the language of participants with ASD, suggesting that they use a more diverse set of strategies for some social linguistic functions. These results, while showing promise for the development of automated language analysis tools to support targeted language interventions for ASD, also reveal weaknesses in the ability of large contextualized language models to model neuroatypical language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TERMinator: A System for Scientific Texts Processing",
        "paper_url": "https://aclanthology.org/2022.coling-1.302/",
        "paper_authors": [
            "Elena Bruches",
            "Olga Tikhobaeva",
            "Yana Dementyeva",
            "Tatiana Batura"
        ],
        "paper_abstract": "This paper is devoted to the extraction of entities and semantic relations between them from scientific texts, where we consider scientific terms as entities. In this paper, we present a dataset that includes annotations for two tasks and develop a system called TERMinator for the study of the influence of language models on term recognition and comparison of different approaches for relation extraction. Experiments show that language models pre-trained on the target language are not always show the best performance. Also adding some heuristic approaches may improve the overall quality of the particular task. The developed tool and the annotated corpus are publicly available at https://github.com/iis-research-team/terminator and may be useful for other researchers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LipKey: A Large-Scale News Dataset for Absent Keyphrases Generation and Abstractive Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.303/",
        "paper_authors": [
            "Fajri Koto",
            "Timothy Baldwin",
            "Jey Han Lau"
        ],
        "paper_abstract": "Summaries, keyphrases, and titles are different ways of concisely capturing the content of a document. While most previous work has released the datasets of keyphrases and summarization separately, in this work, we introduce LipKey, the largest news corpus with human-written abstractive summaries, absent keyphrases, and titles. We jointly use the three elements via multi-task training and training as joint structured inputs, in the context of document summarization. We find that including absent keyphrases and titles as additional context to the source document improves transformer-based summarization models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding Attention for Vision-and-Language Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.304/",
        "paper_authors": [
            "Feiqi Cao",
            "Soyeon Caren Han",
            "Siqu Long",
            "Changwei Xu",
            "Josiah Poon"
        ],
        "paper_abstract": "Attention mechanism has been used as an important component across Vision-and-Language(VL) tasks in order to bridge the semantic gap between visual and textual features. While attention has been widely used in VL tasks, it has not been examined the capability of different attention alignment calculation in bridging the semantic gap between visual and textual clues. In this research, we conduct a comprehensive analysis on understanding the role of attention alignment by looking into the attention score calculation methods and check how it actually represents the visual region\u2019s and textual token\u2019s significance for the global assessment. We also analyse the conditions which attention score calculation mechanism would be more (or less) interpretable, and which may impact the model performance on three different VL tasks, including visual question answering, text-to-image generation, text-and-image matching (both sentence and image retrieval). Our analysis is the first of its kind and provides useful insights of the importance of each attention alignment score calculation when applied at the training phase of VL tasks, commonly ignored in attention-based cross modal models, and/or pretrained models. Our code is available at: https://github.com/adlnlp/Attention_VL",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Effective Data Augmentation for Sentence Classification Using One VAE per Class",
        "paper_url": "https://aclanthology.org/2022.coling-1.305/",
        "paper_authors": [
            "Fr\u00e9d\u00e9ric Piedboeuf",
            "Philippe Langlais"
        ],
        "paper_abstract": "In recent years, data augmentation has become an important field of machine learning. While images can use simple techniques such as cropping or rotating, textual data augmentation needs more complex manipulations to ensure that the generated examples are useful. Variational auto-encoders (VAE) and its conditional variant the Conditional-VAE (CVAE) are often used to generate new textual data, both relying on a good enough training of the generator so that it doesn\u2019t create examples of the wrong class. In this paper, we explore a simpler way to use VAE for data augmentation: the training of one VAE per class. We show on several dataset sizes, as well as on four different binary classification tasks, that it systematically outperforms other generative data augmentation techniques.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.306/",
        "paper_authors": [
            "Giacomo Frisoni",
            "Antonella Carbonaro",
            "Gianluca Moro",
            "Andrea Zammarchi",
            "Marco Avagnano"
        ],
        "paper_abstract": "Driven by deep learning breakthroughs, natural language generation (NLG) models have been at the center of steady progress in the last few years, with a ubiquitous task influence. However, since our ability to generate human-indistinguishable artificial text lags behind our capacity to assess it, it is paramount to develop and apply even better automatic evaluation metrics. To facilitate researchers to judge the effectiveness of their models broadly, we introduce NLG-Metricverse\u2014an end-to-end open-source library for NLG evaluation based on Python. Our framework provides a living collection of NLG metrics in a unified and easy-to-use environment, supplying tools to efficiently apply, analyze, compare, and visualize them. This includes (i) the extensive support to heterogeneous automatic metrics with n-arity management, (ii) the meta-evaluation upon individual performance, metric-metric and metric-human correlations, (iii) graphical interpretations for helping humans better gain score intuitions, (iv) formal categorization and convenient documentation to accelerate metrics understanding. NLG-Metricverse aims to increase the comparability and replicability of NLG research, hopefully stimulating new contributions in the area.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TestAug: A Framework for Augmenting Capability-based NLP Tests",
        "paper_url": "https://aclanthology.org/2022.coling-1.307/",
        "paper_authors": [
            "Guanqun Yang",
            "Mirazul Haque",
            "Qiaochu Song",
            "Wei Yang",
            "Xueqing Liu"
        ],
        "paper_abstract": "The recently proposed capability-based NLP testing allows model developers to test the functional capabilities of NLP models, revealing functional failures for models with good held-out evaluation scores. However, existing work on capability-based testing requires the developer to compose each individual test template from scratch. Such approach thus requires extensive manual efforts and is less scalable. In this paper, we investigate a different approach that requires the developer to only annotate a few test templates, while leveraging the GPT-3 engine to generate the majority of test cases. While our approach saves the manual efforts by design, it guarantees the correctness of the generated suites with a validity checker. Moreover, our experimental results show that the test suites generated by GPT-3 are more diverse than the manually created ones; they can also be used to detect more errors compared to manually created counterparts. Our test suites can be downloaded at https://anonymous-researcher-nlp.github.io/testaug/.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "KoCHET: A Korean Cultural Heritage Corpus for Entity-related Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.308/",
        "paper_authors": [
            "Gyeongmin Kim",
            "Jinsung Kim",
            "Junyoung Son",
            "Heuiseok Lim"
        ],
        "paper_abstract": "As digitized traditional cultural heritage documents have rapidly increased, resulting in an increased need for preservation and management, practical recognition of entities and typification of their classes has become essential. To achieve this, we propose KoCHET - a Korean cultural heritage corpus for the typical entity-related tasks, i.e., named entity recognition (NER), relation extraction (RE), and entity typing (ET). Advised by cultural heritage experts based on the data construction guidelines of government-affiliated organizations, KoCHET consists of respectively 112,362, 38,765, 113,198 examples for NER, RE, and ET tasks, covering all entity types related to Korean cultural heritage. Moreover, unlike the existing public corpora, modified redistribution can be allowed both domestic and foreign researchers. Our experimental results make the practical usability of KoCHET more valuable in terms of cultural heritage. We also provide practical insights of KoCHET in terms of statistical and linguistic analysis. Our corpus is freely available at https://github.com/Gyeongmin47/KoCHET.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MonoByte: A Pool of Monolingual Byte-level Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.309/",
        "paper_authors": [
            "Hugo Abonizio",
            "Leandro Rodrigues de Souza",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "paper_abstract": "The zero-shot cross-lingual ability of models pretrained on multilingual and even monolingual corpora has spurred many hypotheses to explain this intriguing empirical result. However, due to the costs of pretraining, most research uses public models whose pretraining methodology, such as the choice of tokenization, corpus size, and computational budget, might differ drastically. When researchers pretrain their own models, they often do so under a constrained budget, and the resulting models might underperform significantly compared to SOTA models. These experimental differences led to various inconsistent conclusions about the nature of the cross-lingual ability of these models. To help further research on the topic, we released 10 monolingual byte-level models rigorously pretrained under the same configuration with a large compute budget (equivalent to 420 days on a V100) and corpora that are 4 times larger than the original BERT\u2019s. Because they are tokenizer-free, the problem of unseen token embeddings is eliminated, thus allowing researchers to try a wider range of cross-lingual experiments in languages with different scripts. Additionally, we release two models pretrained on non-natural language texts that can be used in sanity-check experiments. Experiments on QA and NLI tasks show that our monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen our understanding of cross-lingual transferability in language models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Wizard of Tasks: A Novel Conversational Dataset for Solving Real-World Tasks in Conversational Settings",
        "paper_url": "https://aclanthology.org/2022.coling-1.310/",
        "paper_authors": [
            "Jason Ingyu Choi",
            "Saar Kuzi",
            "Nikhita Vedula",
            "Jie Zhao",
            "Giuseppe Castellucci",
            "Marcus Collins",
            "Shervin Malmasi",
            "Oleg Rokhlenko",
            "Eugene Agichtein"
        ],
        "paper_abstract": "Conversational Task Assistants (CTAs) are conversational agents whose goal is to help humans perform real-world tasks. CTAs can help in exploring available tasks, answering task-specific questions and guiding users through step-by-step instructions. In this work, we present Wizard of Tasks, the first corpus of such conversations in two domains: Cooking and Home Improvement. We crowd-sourced a total of 549 conversations (18,077 utterances) with an asynchronous Wizard-of-Oz setup, relying on recipes from WholeFoods Market for the cooking domain, and WikiHow articles for the home improvement domain. We present a detailed data analysis and show that the collected data can be a valuable and challenging resource for CTAs in two tasks: Intent Classification (IC) and Abstractive Question Answering (AQA). While on IC we acquired a high performing model (>85% F1), on AQA the performance is far from being satisfactory (~27% BertScore-F1), suggesting that more work is needed to solve the task of low-resource AQA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online News Comment",
        "paper_url": "https://aclanthology.org/2022.coling-1.311/",
        "paper_authors": [
            "Jean Lee",
            "Taejun Lim",
            "Heejun Lee",
            "Bogeun Jo",
            "Yangsok Kim",
            "Heegeun Yoon",
            "Soyeon Caren Han"
        ],
        "paper_abstract": "Online hate speech detection has become an important issue due to the growth of online content, but resources in languages other than English are extremely limited. We introduce K-MHaS, a new multi-label dataset for hate speech detection that effectively handles Korean language patterns. The dataset consists of 109k utterances from news comments and provides a multi-label classification using 1 to 4 labels, and handles subjectivity and intersectionality. We evaluate strong baselines on K-MHaS. KR-BERT with a sub-character tokenizer outperforms others, recognizing decomposed characters in each hate speech class.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Domain- and Task-Adaptation for VaccinChatNL, a Dutch COVID-19 FAQ Answering Corpus and Classification Model",
        "paper_url": "https://aclanthology.org/2022.coling-1.312/",
        "paper_authors": [
            "Jeska Buhmann",
            "Maxime De Bruyn",
            "Ehsan Lotfi",
            "Walter Daelemans"
        ],
        "paper_abstract": "FAQs are important resources to find information. However, especially if a FAQ concerns many question-answer pairs, it can be a difficult and time-consuming job to find the answer you are looking for. A FAQ chatbot can ease this process by automatically retrieving the relevant answer to a user\u2019s question. We present VaccinChatNL, a Dutch FAQ corpus on the topic of COVID-19 vaccination. Starting with 50 question-answer pairs we built VaccinChat, a FAQ chatbot, which we used to gather more user questions that were also annotated with the appropriate or new answer classes. This iterative process of gathering user questions, annotating them, and retraining the model with the increased data set led to a corpus that now contains 12,883 user questions divided over 181 answers. We provide the first publicly available Dutch FAQ answering data set of this size with large groups of semantically equivalent human-paraphrased questions. Furthermore, our study shows that before fine-tuning a classifier, continued pre-training of Dutch language models with task- and/or domain-specific data improves classification results. In addition, we show that large groups of semantically similar questions are important for obtaining well-performing intent classification models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation",
        "paper_url": "https://aclanthology.org/2022.coling-1.313/",
        "paper_authors": [
            "Junyu Luo",
            "Junxian Lin",
            "Chi Lin",
            "Cao Xiao",
            "Xinning Gui",
            "Fenglong Ma"
        ],
        "paper_abstract": "Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated MedLane dataset and the effectiveness of the proposed model DECLARE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "WikiHan: A New Comparative Dataset for Chinese Languages",
        "paper_url": "https://aclanthology.org/2022.coling-1.314/",
        "paper_authors": [
            "Kalvin Chang",
            "Chenxuan Cui",
            "Youngmin Kim",
            "David R. Mortensen"
        ],
        "paper_abstract": "Most comparative datasets of Chinese varieties are not digital; however, Wiktionary includes a wealth of transcriptions of words from these varieties. The usefulness of these data is limited by the fact that they use a wide range of variety-specific romanizations, making data difficult to compare. The current work collects this data into a single constituent (IPA, or International Phonetic Alphabet) and structured form (TSV) for use in comparative linguistics and Chinese NLP. At the time of writing, the dataset contains 67,943 entries across 8 varieties and Middle Chinese. The dataset is validated on a protoform reconstruction task using an encoder-decoder cross-attention architecture (Meloni et al 2021), achieving an accuracy of 54.11%, a PER (phoneme error rate) of 17.69%, and a FER (feature error rate) of 6.60%.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows",
        "paper_url": "https://aclanthology.org/2022.coling-1.315/",
        "paper_authors": [
            "Keisuke Shirai",
            "Atsushi Hashimoto",
            "Taichi Nishimura",
            "Hirotaka Kameko",
            "Shuhei Kurita",
            "Yoshitaka Ushiku",
            "Shinsuke Mori"
        ],
        "paper_abstract": "We present a new multimodal dataset called Visual Recipe Flow, which enables us to learn a cooking action result for each object in a recipe text. The dataset consists of object state changes and the workflow of the recipe text. The state change is represented as an image pair, while the workflow is represented as a recipe flow graph. We developed a web interface to reduce human annotation costs. The dataset allows us to try various applications, including multimodal information retrieval.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "IMPARA: Impact-Based Metric for GEC Using Parallel Data",
        "paper_url": "https://aclanthology.org/2022.coling-1.316/",
        "paper_authors": [
            "Koki Maeda",
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "paper_abstract": "Automatic evaluation of grammatical error correction (GEC) is essential in developing useful GEC systems. Existing methods for automatic evaluation require multiple reference sentences or manual scores. However, such resources are expensive, thereby hindering automatic evaluation for various domains and correction styles. This paper proposes an Impact-based Metric for GEC using PARAllel data, IMPARA, which utilizes correction impacts computed by parallel data comprising pairs of grammatical/ungrammatical sentences. As parallel data is cheaper than manually assessing evaluation scores, IMPARA can reduce the cost of data creation for automatic evaluation. Correlations between IMPARA and human scores indicate that IMPARA is comparable or better than existing evaluation methods. Furthermore, we find that IMPARA can perform evaluations that fit different domains and correction styles trained on various parallel data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evons: A Dataset for Fake and Real News Virality Analysis and Prediction",
        "paper_url": "https://aclanthology.org/2022.coling-1.317/",
        "paper_authors": [
            "Kriste Krstovski",
            "Angela Soomin Ryu",
            "Bruce Kogut"
        ],
        "paper_abstract": "We present a novel collection of news articles originating from fake and real news media sources for the analysis and prediction of news virality. Unlike existing fake news datasets which either contain claims, or news article headline and body, in this collection each article is supported with a Facebook engagement count which we consider as an indicator of the article virality. In addition we also provide the article description and thumbnail image with which the article was shared on Facebook. These images were automatically annotated with object tags and color attributes. Using cloud based vision analysis tools, thumbnail images were also analyzed for faces and detected faces were annotated with facial attributes. We empirically investigate the use of this collection on an example task of article virality prediction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Are Pretrained Multilingual Models Equally Fair across Languages?",
        "paper_url": "https://aclanthology.org/2022.coling-1.318/",
        "paper_authors": [
            "Laura Cabello Piqueras",
            "Anders S\u00f8gaard"
        ],
        "paper_abstract": "Pretrained multilingual language models can help bridge the digital language divide, enabling high-quality NLP models for lower-resourced languages. Studies of multilingual models have so far focused on performance, consistency, and cross-lingual generalisation. However, with their wide-spread application in the wild and downstream societal impact, it is important to put multilingual models under the same scrutiny as monolingual models. This work investigates the group fairness of multilingual models, asking whether these models are equally fair across languages. To this end, we create a new four-way multilingual dataset of parallel cloze test examples (MozArt), equipped with demographic information (balanced with regard to gender and native tongue) about the test participants. We evaluate three multilingual models on MozArt \u2013mBERT, XLM-R, and mT5\u2013 and show that across the four target languages, the three models exhibit different levels of group disparity, e.g., exhibiting near-equal risk for Spanish, but high levels of disparity for German.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Possible Stories: Evaluating Situated Commonsense Reasoning under Multiple Possible Scenarios",
        "paper_url": "https://aclanthology.org/2022.coling-1.319/",
        "paper_authors": [
            "Mana Ashida",
            "Saku Sugawara"
        ],
        "paper_abstract": "The possible consequences for the same context may vary depending on the situation we refer to. However, current studies in natural language processing do not focus on situated commonsense reasoning under multiple possible scenarios. This study frames this task by asking multiple questions with the same set of possible endings as candidate answers, given a short story text. Our resulting dataset, Possible Stories, consists of more than 4.5K questions over 1.3K story texts in English. We discover that even current strong pretrained language models struggle to answer the questions consistently, highlighting that the highest accuracy in an unsupervised setting (60.2%) is far behind human accuracy (92.5%). Through a comparison with existing datasets, we observe that the questions in our dataset contain minimal annotation artifacts in the answer options. In addition, our dataset includes examples that require counterfactual reasoning, as well as those requiring readers\u2019 reactions and fictional information, suggesting that our dataset can serve as a challenging testbed for future studies on situated commonsense reasoning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DiaBiz.Kom - towards a Polish Dialogue Act Corpus Based on ISO 24617-2 Standard",
        "paper_url": "https://aclanthology.org/2022.coling-1.320/",
        "paper_authors": [
            "Marcin Oleksy",
            "Jan Wieczorek",
            "Dorota Dru\u017cy\u0142owska",
            "Julia Klyus",
            "Aleksandra Domoga\u0142a",
            "Krzysztof Hwaszcz",
            "Hanna K\u0119dzierska",
            "Daria Miko\u015b",
            "Anita Wr\u00f3\u017c"
        ],
        "paper_abstract": "This article presents the specification and evaluation of DiaBiz.Kom \u2013 the corpus of dialogue texts in Polish. The corpus contains transcriptions of telephone conversations conducted according to a prepared scenario. The transcripts of conversations have been manually annotated with a layer of information concerning communicative functions. DiaBiz.Kom is the first corpus of this type prepared for the Polish language and will be used to develop a system of dialog analysis and modules for creating advanced chatbots.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Explainable Evaluation of Language Models on the Semantic Similarity of Visual Concepts",
        "paper_url": "https://aclanthology.org/2022.coling-1.321/",
        "paper_authors": [
            "Maria Lymperaiou",
            "George Manoliadis",
            "Orfeas Menis Mastromichalakis",
            "Edmund G. Dervakos",
            "Giorgos Stamou"
        ],
        "paper_abstract": "Recent breakthroughs in NLP research, such as the advent of Transformer models have indisputably contributed to major advancements in several tasks. However, few works research robustness and explainability issues of their evaluation strategies. In this work, we examine the behavior of high-performing pre-trained language models, focusing on the task of semantic similarity for visual vocabularies. First, we address the need for explainable evaluation metrics, necessary for understanding the conceptual quality of retrieved instances. Our proposed metrics provide valuable insights in local and global level, showcasing the inabilities of widely used approaches. Secondly, adversarial interventions on salient query semantics expose vulnerabilities of opaque metrics and highlight patterns in learned linguistic representations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Establishing Annotation Quality in Multi-label Annotations",
        "paper_url": "https://aclanthology.org/2022.coling-1.322/",
        "paper_authors": [
            "Marian Marchal",
            "Merel Scholman",
            "Frances Yung",
            "Vera Demberg"
        ],
        "paper_abstract": "In many linguistic fields requiring annotated data, multiple interpretations of a single item are possible. Multi-label annotations more accurately reflect this possibility. However, allowing for multi-label annotations also affects the chance that two coders agree with each other. Calculating inter-coder agreement for multi-label datasets is therefore not trivial. In the current contribution, we evaluate different metrics for calculating agreement on multi-label annotations: agreement on the intersection of annotated labels, an augmented version of Cohen\u2019s Kappa, and precision, recall and F1. We propose a bootstrapping method to obtain chance agreement for each measure, which allows us to obtain an adjusted agreement coefficient that is more interpretable. We demonstrate how various measures affect estimates of agreement on simulated datasets and present a case study of discourse relation annotations. We also show how the proportion of double labels, and the entropy of the label distribution, influences the measures outlined above and how a bootstrapped adjusted agreement can make agreement measures more comparable across datasets in multi-label scenarios.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Biographically Relevant Tweets \u2013 a New Dataset, Linguistic Analysis and Classification Experiments",
        "paper_url": "https://aclanthology.org/2022.coling-1.323/",
        "paper_authors": [
            "Michael Wiegand",
            "Rebecca Wilm",
            "Katja Markert"
        ],
        "paper_abstract": "We present a new dataset comprising tweets for the novel task of detecting biographically relevant utterances. Biographically relevant utterances are all those utterances that reveal some persistent and non-trivial information about the author of a tweet, e.g. habits, (dis)likes, family status, physical appearance, employment information, health issues etc. Unlike previous research we do not restrict biographical relevance to a small fixed set of pre-defined relations. Next to classification experiments employing state-of-the-art classifiers to establish strong baselines for future work, we carry out a linguistic analysis that compares the predictiveness of various high-level features. We also show that the task is different from established tasks, such as aspectual classification or sentiment analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BECEL: Benchmark for Consistency Evaluation of Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.324/",
        "paper_authors": [
            "Myeongjun Jang",
            "Deuk Sin Kwon",
            "Thomas Lukasiewicz"
        ],
        "paper_abstract": "Behavioural consistency is a critical condition for a language model (LM) to become trustworthy like humans. Despite its importance, however, there is little consensus on the definition of LM consistency, resulting in different definitions across many studies. In this paper, we first propose the idea of LM consistency based on behavioural consistency and establish a taxonomy that classifies previously studied consistencies into several sub-categories. Next, we create a new benchmark that allows us to evaluate a model on 19 test cases, distinguished by multiple types of consistency and diverse downstream tasks. Through extensive experiments on the new benchmark, we ascertain that none of the modern pre-trained language models (PLMs) performs well in every test case, while exhibiting high inconsistency in many cases. Our experimental results suggest that a unified benchmark that covers broad aspects (i.e., multiple consistency types and tasks) is essential for a more precise evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "KoBEST: Korean Balanced Evaluation of Significant Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.325/",
        "paper_authors": [
            "Myeongjun Jang",
            "Dohyung Kim",
            "Deuk Sin Kwon",
            "Eric Davis"
        ],
        "paper_abstract": "A well-formulated benchmark plays a critical role in spurring advancements in the natural language processing (NLP) field, as it allows objective and precise evaluation of diverse models. As modern language models (LMs) have become more elaborate and sophisticated, more difficult benchmarks that require linguistic knowledge and reasoning have been proposed. However, most of these benchmarks only support English, and great effort is necessary to construct benchmarks for other low resource languages. To this end, we propose a new benchmark named Korean balanced evaluation of significant tasks (KoBEST), which consists of five Korean-language downstream tasks. Professional Korean linguists designed the tasks that require advanced Korean linguistic knowledge. Moreover, our data is purely annotated by humans and thoroughly reviewed to guarantee high data quality. We also provide baseline models and human performance results. Our dataset is available on the Huggingface.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A New Public Corpus for Clinical Section Identification: MedSecId",
        "paper_url": "https://aclanthology.org/2022.coling-1.326/",
        "paper_authors": [
            "Paul Landes",
            "Kunal Patel",
            "Sean S. Huang",
            "Adam Webb",
            "Barbara Di Eugenio",
            "Cornelia Caragea"
        ],
        "paper_abstract": "The process by which sections in a document are demarcated and labeled is known as section identification. Such sections are helpful to the reader when searching for information and contextualizing specific topics. The goal of this work is to segment the sections of clinical medical domain documentation. The primary contribution of this work is MedSecId, a publicly available set of 2,002 fully annotated medical notes from the MIMIC-III. We include several baselines, source code, a pretrained model and analysis of the data showing a relationship between medical concepts across sections using principal component analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Data-driven Approach to Named Entity Recognition for Early Modern French",
        "paper_url": "https://aclanthology.org/2022.coling-1.327/",
        "paper_authors": [
            "Pedro Ortiz Suarez",
            "Simon Gabay"
        ],
        "paper_abstract": "Named entity recognition has become an increasingly useful tool for digital humanities research, specially when it comes to historical texts. However, historical texts pose a wide range of challenges to both named entity recognition and natural language processing in general that are still difficult to address even with modern neural methods. In this article we focus in named entity recognition for historical French, and in particular for Early Modern French (16th-18th c.), i.e. Ancien R\u00e9gime French. However, instead of developing a specialised architecture to tackle the particularities of this state of language, we opt for a data-driven approach by developing a new corpus with fine-grained entity annotation, covering three centuries of literature corresponding to the early modern period; we try to annotate as much data as possible producing a corpus that is many times bigger than the most popular NER evaluation corpora for both Contemporary English and French. We then fine-tune existing state-of-the-art architectures for Early Modern and Contemporary French, obtaining results that are on par with those of the current state-of-the-art NER systems for Contemporary English. Both the corpus and the fine-tuned models are released.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Reproducibility and Automation of the Appraisal Taxonomy",
        "paper_url": "https://aclanthology.org/2022.coling-1.328/",
        "paper_authors": [
            "Pradeesh Parameswaran",
            "Andrew Trotman",
            "Veronica Liesaputra",
            "David Eyers"
        ],
        "paper_abstract": "There is a lack of reproducibility in results from experiments that apply the Appraisal taxonomy. Appraisal is widely used by linguists to study how people judge things or people. Automating Appraisal could be beneficial for use cases such as moderating online comments. Past work in Appraisal annotation has been descriptive in nature and, the lack of publicly available data sets hinders the progress of automation. In this work, we are interested in two things; first, measuring the performance of automated approaches to Appraisal classification in the publicly available Australasian Language Technology Association (ALTA) Shared Task Challenge data set. Second, we are interested in reproducing the annotation of the ALTA data set. Four additional annotators, each with a different linguistics background, were employed to re-annotate the data set. Our results show a poor level of agreement at more detailed Appraisal categories (Fleiss Kappa = 0.059) and a fair level of agreement (Kappa = 0.372) at coarse-level categories. We find similar results when using automated approaches that are available publicly. Our empirical evidence suggests that at present, automating classification is practical only when considering coarse-level categories of the taxonomy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Few-Shot Table Understanding: A Benchmark Dataset and Pre-Training Baseline",
        "paper_url": "https://aclanthology.org/2022.coling-1.329/",
        "paper_authors": [
            "Ruixue Liu",
            "Shaozu Yuan",
            "Aijun Dai",
            "Lei Shen",
            "Tiangang Zhu",
            "Meng Chen",
            "Xiaodong He"
        ],
        "paper_abstract": "Few-shot table understanding is a critical and challenging problem in real-world scenario as annotations over large amount of tables are usually costly. Pre-trained language models (PLMs), which have recently flourished on tabular data, have demonstrated their effectiveness for table understanding tasks. However, few-shot table understanding is rarely explored due to the deficiency of public table pre-training corpus and well-defined downstream benchmark tasks, especially in Chinese. In this paper, we establish a benchmark dataset, FewTUD, which consists of 5 different tasks with human annotations to systematically explore the few-shot table understanding in depth. Since there is no large number of public Chinese tables, we also collect a large-scale, multi-domain tabular corpus to facilitate future Chinese table pre-training, which includes one million tables and related natural language text with auxiliary supervised interaction signals. Finally, we present FewTPT, a novel table PLM with rich interactions over tabular data, and evaluate its performance comprehensively on the benchmark. Our dataset and model will be released to the public soon.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Tafsir Dataset: A Novel Multi-Task Benchmark for Named Entity Recognition and Topic Modeling in Classical Arabic Literature",
        "paper_url": "https://aclanthology.org/2022.coling-1.330/",
        "paper_authors": [
            "Sajawel Ahmed",
            "Rob van der Goot",
            "Misbahur Rehman",
            "Carl Kruse",
            "\u00d6mer \u00d6zsoy",
            "Alexander Mehler",
            "Gemma Roig"
        ],
        "paper_abstract": "Various historical languages, which used to be lingua franca of science and arts, deserve the attention of current NLP research. In this work, we take the first data-driven steps towards this research line for Classical Arabic (CA) by addressing named entity recognition (NER) and topic modeling (TM) on the example of CA literature. We manually annotate the encyclopedic work of Tafsir Al-Tabari with span-based NEs, sentence-based topics, and span-based subtopics, thus creating the Tafsir Dataset with over 51,000 sentences, the first large-scale multi-task benchmark for CA. Next, we analyze our newly generated dataset, which we make open-source available, with current language models (lightweight BiLSTM, transformer-based MaChAmP) along a novel script compression method, thereby achieving state-of-the-art performance for our target task CA-NER. We also show that CA-TM from the perspective of historical topic models, which are central to Arabic studies, is very challenging. With this interdisciplinary work, we lay the foundations for future research on automatic analysis of CA literature.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Resource of Wikipedias in 31 Languages Categorized into Fine-Grained Named Entities",
        "paper_url": "https://aclanthology.org/2022.coling-1.331/",
        "paper_authors": [
            "Satoshi Sekine",
            "Kouta Nakayama",
            "Masako Nomoto",
            "Maya Ando",
            "Asuka Sumida",
            "Koji Matsuda"
        ],
        "paper_abstract": "This paper describes a resource of Wikipedias in 31 languages categorized into Extended Named Entity (ENE), which has 219 fine-grained NE categories. We first categorized 920 K Japanese Wikipedia pages according to the ENE scheme using machine learning, followed by manual validation. We then organized a shared task of Wikipedia categorization into 30 languages. The training data were provided by Japanese categorization and the language links, and the task was to categorize the Wikipedia pages into 30 languages, with no language links from Japanese Wikipedia (20M pages in total). Thirteen groups with 24 systems participated in the 2020 and 2021 tasks, sharing their outputs for resource-building. The Japanese categorization accuracy was 98.5%, and the best performance among the 30 languages ranges from 80 to 93 in F-measure. Using ensemble learning, we created outputs with an average F-measure of 86.8, which is 1.7 better than the best single systems. The total size of the resource is 32.5M pages, including the training data. We call this resource creation scheme \u201cResource by Collaborative Contribution (RbCC)\u201d. We also constructed structuring tasks (attribute extraction and link prediction) using RbCC under our ongoing project, \u201cSHINRA.\u201d",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Accuracy meets Diversity in a News Recommender System",
        "paper_url": "https://aclanthology.org/2022.coling-1.332/",
        "paper_authors": [
            "Shaina Raza",
            "Syed Raza Bashir",
            "Usman Naseem"
        ],
        "paper_abstract": "News recommender systems face certain challenges. These challenges arise due to evolving users\u2019 preferences over dynamically created news articles. The diversity is necessary for a news recommender system to expose users to a variety of information. We propose a deep neural network based on a two-tower architecture that learns news representation through a news item tower and users\u2019 representations through a query tower. We customize an augmented vector for each query and news item to introduce information interaction between the two towers. We introduce diversity in the proposed architecture by considering a category loss function that aligns items\u2019 representation of uneven news categories. Experimental results on two news datasets reveal that our proposed architecture is more effective compared to the state-of-the-art methods and achieves a balance between accuracy and diversity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dynamic Nonlinear Mixup with Distance-based Sample Selection",
        "paper_url": "https://aclanthology.org/2022.coling-1.333/",
        "paper_authors": [
            "Shaokang Zhang",
            "Lei Jiang",
            "Jianlong Tan"
        ],
        "paper_abstract": "Data augmentation with mixup has shown to be effective on the NLP tasks. Although its great success, the mixup still has shortcomings. First, vanilla mixup randomly selects one sample to generate the mixup sample for a given sample. It remains unclear how to best choose the input samples for the mixup. Second, linear interpolation limits the space of synthetic data and its regularization effect. In this paper, we propose the dynamic nonlinear mixup with distance-based sample selection, which not only generates multiple sample pairs based on the distance between each sample but also enlarges the space of synthetic samples. Specifically, we compute the distance between each input data by cosine similarity and select multiple samples for a given sample. Then we use the dynamic nonlinear mixup to fuse sample pairs. It does not use a linear, scalar mixing strategy, but a nonlinear interpolation strategy, where the mixing strategy is adaptively updated for the input and label pairs. Experiments on the multiple public datasets demonstrate that dynamic nonlinear mixup outperforms state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MultiCoNER: A Large-scale Multilingual Dataset for Complex Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.334/",
        "paper_authors": [
            "Shervin Malmasi",
            "Anjie Fang",
            "Besnik Fetahu",
            "Sudipta Kar",
            "Oleg Rokhlenko"
        ],
        "paper_abstract": "We present AnonData, a large multilingual dataset for Named Entity Recognition that covers 3 domains (Wiki sentences, questions, and search queries) across 11 languages, as well as multilingual and code-mixing subsets. This dataset is designed to represent contemporary challenges in NER, including low-context scenarios (short and uncased text), syntactically complex entities like movie titles, and long-tail entity distributions. The 26M token dataset is compiled from public resources using techniques such as heuristic-based sentence sampling, template extraction and slotting, and machine translation. We tested the performance of two NER models on our dataset: a baseline XLM-RoBERTa model, and a state-of-the-art NER GEMNET model that leverages gazetteers. The baseline achieves moderate performance (macro-F1=54%). GEMNET, which uses gazetteers, improvement significantly (average improvement of macro-F1=+30%) and demonstrates the difficulty of our dataset. AnonData poses challenges even for large pre-trained language models, and we believe that it can help further research in building robust NER systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Extracting a Knowledge Base of COVID-19 Events from Social Media",
        "paper_url": "https://aclanthology.org/2022.coling-1.335/",
        "paper_authors": [
            "Shi Zong",
            "Ashutosh Baheti",
            "Wei Xu",
            "Alan Ritter"
        ],
        "paper_abstract": "We present a manually annotated corpus of 10,000 tweets containing public reports of five COVID-19 events, including positive and negative tests, deaths, denied access to testing, claimed cures and preventions. We designed slot-filling questions for each event type and annotated a total of 28 fine-grained slots, such as the location of events, recent travel, and close contacts. We show that our corpus can support fine-tuning BERT-based classifiers to automatically extract publicly reported events, which can be further collected for building a knowledge base. Our knowledge base is constructed over Twitter data covering two years and currently covers over 4.2M events. It can answer complex queries with high precision, such as \u201cWhich organizations have employees that tested positive in Philadelphia?\u201d We believe our proposed methodology could be quickly applied to develop knowledge bases for new domains in response to an emerging crisis, including natural disasters or future disease outbreaks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Accounting for Language Effect in the Evaluation of Cross-lingual AMR Parsers",
        "paper_url": "https://aclanthology.org/2022.coling-1.336/",
        "paper_authors": [
            "Shira Wein",
            "Nathan Schneider"
        ],
        "paper_abstract": "Cross-lingual Abstract Meaning Representation (AMR) parsers are currently evaluated in comparison to gold English AMRs, despite parsing a language other than English, due to the lack of multilingual AMR evaluation metrics. This evaluation practice is problematic because of the established effect of source language on AMR structure. In this work, we present three multilingual adaptations of monolingual AMR evaluation metrics and compare the performance of these metrics to sentence-level human judgments. We then use our most highly correlated metric to evaluate the output of state-of-the-art cross-lingual AMR parsers, finding that Smatch may still be a useful metric in comparison to gold English AMRs, while our multilingual adaptation of S2match (XS2match) is best for comparison with gold in-language AMRs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "QSTS: A Question-Sensitive Text Similarity Measure for Question Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.337/",
        "paper_authors": [
            "Sujatha Das Gollapalli",
            "See-Kiong Ng"
        ],
        "paper_abstract": "While question generation (QG) has received significant focus in conversation modeling and text generation research, the problems of comparing questions and evaluation of QG models have remained inadequately addressed. Indeed, QG models continue to be evaluated using traditional measures such as BLEU, METEOR, and ROUGE scores which were designed for other text generation problems. We propose QSTS, a novel Question-Sensitive Text Similarity measure for comparing two questions by characterizing their target intent based on question class, named-entity, and semantic similarity information from the two questions. We show that QSTS addresses several shortcomings of existing measures that depend on n-gram overlap scores and obtains superior results compared to traditional measures on publicly-available QG datasets. We also collect a novel dataset SimQG, for enabling question similarity research in QG contexts. SimQG contains questions generated by state-of-the-art QG models along with human judgements on their relevance with respect to the passage context they were generated for as well as when compared to the given reference question. Using SimQG, we showcase the key aspect of QSTS that differentiates it from all existing measures. QSTS is not only able to characterize similarity between two questions, but is also able to score questions with respect to passage contexts. Thus QSTS is, to our knowledge, the first metric that enables the measurement of QG performance in a reference-free manner.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Noun-MWP: Math Word Problems Meet Noun Answers",
        "paper_url": "https://aclanthology.org/2022.coling-1.338/",
        "paper_authors": [
            "Taehun Cha",
            "Jaeheun Jung",
            "Donghun Lee"
        ],
        "paper_abstract": "We introduce a new type of problems for math word problem (MWP) solvers, named Noun-MWPs, whose answer is a non-numerical string containing a noun from the problem text. We present a novel method to empower existing MWP solvers to handle Noun-MWPs, and apply the method on Expression-Pointer Transformer (EPT). Our model, N-EPT, solves Noun-MWPs significantly better than other models, and at the same time, solves conventional MWPs as well. Solving Noun-MWPs may lead to bridging MWP solvers and traditional question-answering NLP models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ViNLI: A Vietnamese Corpus for Studies on Open-Domain Natural Language Inference",
        "paper_url": "https://aclanthology.org/2022.coling-1.339/",
        "paper_authors": [
            "Tin Van Huynh",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "paper_abstract": "Over a decade, the research field of computational linguistics has witnessed the growth of corpora and models for natural language inference (NLI) for rich-resource languages such as English and Chinese. A large-scale and high-quality corpus is necessary for studies on NLI for Vietnamese, which can be considered a low-resource language. In this paper, we introduce ViNLI (Vietnamese Natural Language Inference), an open-domain and high-quality corpus for evaluating Vietnamese NLI models, which is created and evaluated with a strict process of quality control. ViNLI comprises over 30,000 human-annotated premise-hypothesis sentence pairs extracted from more than 800 online news articles on 13 distinct topics. In this paper, we introduce the guidelines for corpus creation which take the specific characteristics of the Vietnamese language in expressing entailment and contradiction into account. To evaluate the challenging level of our corpus, we conduct experiments with state-of-the-art deep neural networks and pre-trained models on our dataset. The best system performance is still far from human performance (a 14.20% gap in accuracy). The ViNLI corpus is a challenging corpus to accelerate progress in Vietnamese computational linguistics. Our corpus is available publicly for research purposes.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "InferES : A Natural Language Inference Corpus for Spanish Featuring Negation-Based Contrastive and Adversarial Examples",
        "paper_url": "https://aclanthology.org/2022.coling-1.340/",
        "paper_authors": [
            "Venelin Kovatchev",
            "Mariona Taul\u00e9"
        ],
        "paper_abstract": "In this paper we present InferES - an original corpus for Natural Language Inference (NLI) in European Spanish. We propose, implement, and analyze a variety of corpus-creating strategies utilizing expert linguists and crowd workers. The objectives behind InferES are to provide high-quality data, and at the same time to facilitate the systematic evaluation of automated systems. Specifically, we focus on measuring and improving the performance of machine learning systems on negation-based adversarial examples and their ability to generalize across out-of-distribution topics. We train two transformer models on InferES (8,055 gold examples) in a variety of scenarios. Our best model obtains 72.8% accuracy, leaving a lot of room for improvement. The \u201chypothesis-only\u201d baseline performs only 2%-5% higher than majority, indicating much fewer annotation artifacts than prior work. We show that models trained on InferES generalize very well across topics (both in- and out-of-distribution) and perform moderately well on negation-based adversarial examples.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ParaZh-22M: A Large-Scale Chinese Parabank via Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.341/",
        "paper_authors": [
            "Wenjie Hao",
            "Hongfei Xu",
            "Deyi Xiong",
            "Hongying Zan",
            "Lingling Mu"
        ],
        "paper_abstract": "Paraphrasing, i.e., restating the same meaning in different ways, is an important data augmentation approach for natural language processing (NLP). Zhang et al. (2019b) propose to extract sentence-level paraphrases from multiple Chinese translations of the same source texts, and construct the PKU Paraphrase Bank of 0.5M sentence pairs. However, despite being the largest Chinese parabank to date, the size of PKU parabank is limited by the availability of one-to-many sentence translation data, and cannot well support the training of large Chinese paraphrasers. In this paper, we relieve the restriction with one-to-many sentence translation data, and construct ParaZh-22M, a larger Chinese parabank that is composed of 22M sentence pairs, based on one-to-one bilingual sentence translation data and machine translation (MT). In our data augmentation experiments, we show that paraphrasing based on ParaZh-22M can bring about consistent and significant improvements over several strong baselines on a wide range of Chinese NLP tasks, including a number of Chinese natural language understanding benchmarks (CLUE) and low-resource machine translation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding",
        "paper_url": "https://aclanthology.org/2022.coling-1.342/",
        "paper_authors": [
            "Xing Wu",
            "Chaochen Gao",
            "Liangjun Zang",
            "Jizhong Han",
            "Zhongyuan Wang",
            "Songlin Hu"
        ],
        "paper_abstract": "Contrastive learning has been attracting much attention for learning unsupervised sentence embeddings. The current state-of-the-art unsupervised method is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as a minimal data augmentation method, and passes the same input sentence to a pre-trained Transformer encoder (with dropout turned on) twice to obtain the two corresponding embeddings to build a positive pair. As the length information of a sentence will generally be encoded into the sentence embeddings due to the usage of position embedding in Transformer, each positive pair in unsup-SimCSE actually contains the same length information. And thus unsup-SimCSE trained with these positive pairs is probably biased, which would tend to consider that sentences of the same or similar length are more similar in semantics. Through statistical observations, we find that unsup-SimCSE does have such a problem. To alleviate it, we apply a simple repetition operation to modify the input sentence, and then pass the input sentence and its modified counterpart to the pre-trained Transformer encoder, respectively, to get the positive pair. Additionally, we draw inspiration from the community of computer vision and introduce a momentum contrast, enlarging the number of negative pairs without additional calculations. The proposed two modifications are applied on positive and negative pairs separately, and build a new sentence embedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the proposed ESimCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that ESimCSE outperforms the state-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on BERT-base.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Measuring Robustness for NLP",
        "paper_url": "https://aclanthology.org/2022.coling-1.343/",
        "paper_authors": [
            "Yu Yu",
            "Abdul Rafae Khan",
            "Jia Xu"
        ],
        "paper_abstract": "The quality of Natural Language Processing (NLP) models is typically measured by the accuracy or error rate of a predefined test set. Because the evaluation and optimization of these measures are narrowed down to a specific domain like news and cannot be generalized to other domains like Twitter, we often observe that a system reported with human parity results generates surprising errors in real-life use scenarios. We address this weakness with a new approach that uses an NLP quality measure based on robustness. Unlike previous work that has defined robustness using Minimax to bound worst cases, we measure robustness based on the consistency of cross-domain accuracy and introduce the coefficient of variation and (epsilon, gamma)-Robustness. Our measures demonstrate higher agreements with human evaluation than accuracy scores like BLEU on ranking Machine Translation (MT) systems. Our experiments of sentiment analysis and MT tasks show that incorporating our robustness measures into learning objectives significantly enhances the final NLP prediction accuracy over various domains, such as biomedical and social media.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CSL: A Large-scale Chinese Scientific Literature Dataset",
        "paper_url": "https://aclanthology.org/2022.coling-1.344/",
        "paper_authors": [
            "Yudong Li",
            "Yuqing Zhang",
            "Zhe Zhao",
            "Linlin Shen",
            "Weijie Liu",
            "Weiquan Mao",
            "Hui Zhang"
        ],
        "paper_abstract": "Scientific literature serves as a high-quality corpus, supporting a lot of Natural Language Processing (NLP) research. However, existing datasets are centered around the English language, which restricts the development of Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers. To our knowledge, CSL is the first scientific document dataset in Chinese. The CSL can serve as a Chinese corpus. Also, this semi-structured data is a natural annotation that can constitute many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the performance of models across scientific domain tasks, i.e., summarization, keyword generation and text classification. We analyze the behavior of existing text-to-text models on the evaluation tasks and reveal the challenges for Chinese scientific NLP tasks, which provides a valuable reference for future research. Data and code will be publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Singlish Message Paraphrasing: A Joint Task of Creole Translation and Text Normalization",
        "paper_url": "https://aclanthology.org/2022.coling-1.345/",
        "paper_authors": [
            "Zhengyuan Liu",
            "Shikang Ni",
            "Ai Ti Aw",
            "Nancy F. Chen"
        ],
        "paper_abstract": "Within the natural language processing community, English is by far the most resource-rich language. There is emerging interest in conducting translation via computational approaches to conform its dialects or creole languages back to standard English. This computational approach paves the way to leverage generic English language backbones, which are beneficial for various downstream tasks. However, in practical online communication scenarios, the use of language varieties is often accompanied by noisy user-generated content, making this translation task more challenging. In this work, we introduce a joint paraphrasing task of creole translation and text normalization of Singlish messages, which can shed light on how to process other language varieties and dialects. We formulate the task in three different linguistic dimensions: lexical level normalization, syntactic level editing, and semantic level rewriting. We build an annotated dataset of Singlish-to-Standard English messages, and report performance on a perturbation-resilient sequence-to-sequence model. Experimental results show that the model produces reasonable generation results, and can improve the performance of downstream tasks like stance detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CINO: A Chinese Minority Pre-trained Language Model",
        "paper_url": "https://aclanthology.org/2022.coling-1.346/",
        "paper_authors": [
            "Ziqing Yang",
            "Zihang Xu",
            "Yiming Cui",
            "Baoxin Wang",
            "Min Lin",
            "Dayong Wu",
            "Zhigang Chen"
        ],
        "paper_abstract": "Multilingual pre-trained language models have shown impressive performance on cross-lingual tasks. It greatly facilitates the applications of natural language processing on low-resource languages. However, there are still some languages that the current multilingual models do not perform well on. In this paper, we propose CINO (Chinese Minority Pre-trained Language Model), a multilingual pre-trained language model for Chinese minority languages. It covers Standard Chinese, Yue Chinese, and six other ethnic minority languages. To evaluate the cross-lingual ability of the multilingual model on ethnic minority languages, we collect documents from Wikipedia and news websites, and construct two text classification datasets, WCM (Wiki-Chinese-Minority) and CMNews (Chinese-Minority-News). We show that CINO notably outperforms the baselines on various classification tasks. The CINO model and the datasets are publicly available at http://cino.hfl-rc.com.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "One Word, Two Sides: Traces of Stance in Contextualized Word Representations",
        "paper_url": "https://aclanthology.org/2022.coling-1.347/",
        "paper_authors": [
            "Aina Gar\u00ed Soler",
            "Matthieu Labeau",
            "Chlo\u00e9 Clavel"
        ],
        "paper_abstract": "The way we use words is influenced by our opinion. We investigate whether this is reflected in contextualized word embeddings. For example, is the representation of \u201canimal\u201d different between people who would abolish zoos and those who would not? We explore this question from a Lexical Semantic Change standpoint. Our experiments with BERT embeddings derived from datasets with stance annotations reveal small but significant differences in word representations between opposing stances.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Prepositions Matter in Quantifier Scope Disambiguation",
        "paper_url": "https://aclanthology.org/2022.coling-1.348/",
        "paper_authors": [
            "Aleksander Leczkowski",
            "Justyna Grudzi\u0144ska",
            "Manuel Vargas Guzm\u00e1n",
            "Aleksander Wawer",
            "Aleksandra Siemieniuk"
        ],
        "paper_abstract": "Although it is widely agreed that world knowledge plays a significant role in quantifier scope disambiguation (QSD), there has been only very limited work on how to integrate this knowledge into a QSD model. This paper contributes to this scarce line of research by incorporating into a machine learning model our knowledge about relations, as conveyed by a manageable closed class of function words: prepositions. For data, we use a scope-disambiguated corpus created by AnderBois, Brasoveanu and Henderson, which is additionally annotated with prepositional senses using Schneider et al\u2019s Semantic Network of Adposition and Case Supersenses (SNACS) scheme. By applying Manshadi and Allen\u2019s method to the corpus, we were able to inspect the information gain provided by prepositions for the QSD task. Statistical analysis of the performance of the classifiers, trained in scenarios with and without preposition information, supports the claim that prepositional senses have a strong positive impact on the learnability of automatic QSD systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modelling Commonsense Properties Using Pre-Trained Bi-Encoders",
        "paper_url": "https://aclanthology.org/2022.coling-1.349/",
        "paper_authors": [
            "Amit Gajbhiye",
            "Luis Espinosa-Anke",
            "Steven Schockaert"
        ],
        "paper_abstract": "Grasping the commonsense properties of everyday concepts is an important prerequisite to language understanding. While contextualised language models are reportedly capable of predicting such commonsense properties with human-level accuracy, we argue that such results have been inflated because of the high similarity between training and test concepts. This means that models which capture concept similarity can perform well, even if they do not capture any knowledge of the commonsense properties themselves. In settings where there is no overlap between the properties that are considered during training and testing, we find that the empirical performance of standard language models drops dramatically. To address this, we study the possibility of fine-tuning language models to explicitly model concepts and their properties. In particular, we train separate concept and property encoders on two types of readily available data: extracted hyponym-hypernym pairs and generic sentences. Our experimental results show that the resulting encoders allow us to predict commonsense properties with much higher accuracy than is possible by directly fine-tuning language models. We also present experimental results for the related task of unsupervised hypernym discovery.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "COIN \u2013 an Inexpensive and Strong Baseline for Predicting Out of Vocabulary Word Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.350/",
        "paper_authors": [
            "Andrew Schneider",
            "Lihong He",
            "Zhijia Chen",
            "Arjun Mukherjee",
            "Eduard Dragut"
        ],
        "paper_abstract": "Social media is the ultimate challenge for many natural language processing tools. The constant emergence of linguistic constructs challenge even the most sophisticated NLP tools. Predicting word embeddings for out of vocabulary words is one of those challenges. Word embedding models only include terms that occur a sufficient number of times in their training corpora. Word embedding vector models are unable to directly provide any useful information about a word not in their vocabularies. We propose a fast method for predicting vectors for out of vocabulary terms that makes use of the surrounding terms of the unknown term and the hidden context layer of the word2vec model. We propose this method as a strong baseline in the sense that 1) while it does not surpass all state-of-the-art methods, it surpasses several techniques for vector prediction on benchmark tasks, 2) even when it underperforms, the margin is very small retaining competitive performance in downstream tasks, and 3) it is inexpensive to compute, requiring no additional training stage. We also show that our technique can be incorporated into existing methods to achieve a new state-of-the-art on the word vector prediction problem.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DynGL-SDP: Dynamic Graph Learning for Semantic Dependency Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.351/",
        "paper_authors": [
            "Bin Li",
            "Miao Gao",
            "Yunlong Fan",
            "Yikemaiti Sataer",
            "Zhiqiang Gao",
            "Yaocheng Gui"
        ],
        "paper_abstract": "A recent success in semantic dependency parsing shows that graph neural networks can make significant accuracy improvements, owing to its powerful ability in learning expressive graph representations. However, this work learns graph representations based on a static graph constructed by an existing parser, suffering from two drawbacks: (1) the static graph might be error-prone (e.g., noisy or incomplete), and (2) graph construction stage and graph representation learning stage are disjoint, the errors introduced in the graph construction stage cannot be corrected and might be accumulated to later stages. To address these two drawbacks, we propose a dynamic graph learning framework and apply it to semantic dependency parsing, for jointly learning graph structure and graph representations. Experimental results show that our parser outperforms the previous parsers on the SemEval-2015 Task 18 dataset in three languages (English, Chinese, and Czech).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion",
        "paper_url": "https://aclanthology.org/2022.coling-1.352/",
        "paper_authors": [
            "Chen Chen",
            "Yufei Wang",
            "Bing Li",
            "Kwok-Yan Lam"
        ],
        "paper_abstract": "Knowledge Graph Completion (KGC) has been recently extended to multiple knowledge graph (KG) structures, initiating new research directions, e.g. static KGC, temporal KGC and few-shot KGC. Previous works often design KGC models closely coupled with specific graph structures, which inevitably results in two drawbacks: 1) structure-specific KGC models are mutually incompatible; 2) existing KGC methods are not adaptable to emerging KGs. In this paper, we propose KG-S2S, a Seq2Seq generative framework that could tackle different verbalizable graph structures by unifying the representation of KG facts into \u201cflat\u201d text, regardless of their original form. To remedy the KG structure information loss from the \u201cflat\u201d text, we further improve the input representations of entities and relations, and the inference algorithm in KG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many competitive baselines, setting new state-of-the-art performance. Finally, we analyze KG-S2S\u2019s ability on the different relations and the Non-entity Generations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modelling Frequency, Attestation, and Corpus-Based Information with OntoLex-FrAC",
        "paper_url": "https://aclanthology.org/2022.coling-1.353/",
        "paper_authors": [
            "Christian Chiarcos",
            "Elena-Simona Apostol",
            "Besim Kabashi",
            "Ciprian-Octavian Truic\u0103"
        ],
        "paper_abstract": "OntoLex-Lemon has become a de facto standard for lexical resources in the web of data. This paper provides the first overall description of the emerging OntoLex module for Frequency, Attestations, and Corpus-Based Information (OntoLex-FrAC) that is intended to complement OntoLex-Lemon with the necessary vocabulary to represent major types of information found in or automatically derived from corpora, for applications in both language technology and the language sciences.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Contrast Sets for Stativity of English Verbs in Context",
        "paper_url": "https://aclanthology.org/2022.coling-1.354/",
        "paper_authors": [
            "Daniel Chen",
            "Alexis Palmer"
        ],
        "paper_abstract": "For the task of classifying verbs in context as dynamic or stative, current models approach human performance, but only for particular data sets. To better understand the performance of such models, and how well they are able to generalize beyond particular test sets, we apply the contrast set (Gardner et al., 2020) methodology to stativity classification. We create nearly 300 contrastive pairs by perturbing test set instances just enough to change their labels from one class to the other, while preserving coherence, meaning, and well-formedness. Contrastive evaluation shows that a model with near-human performance on an in-distribution test set degrades substantially when applied to transformed examples, showing that the stative vs. dynamic classification task is more complex than the model performance might otherwise suggest. Code and data are freely available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multilingual and Multimodal Topic Modelling with Pretrained Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.355/",
        "paper_authors": [
            "Elaine Zosa",
            "Lidia Pivovarova"
        ],
        "paper_abstract": "This paper presents M3L-Contrast\u2014a novel multimodal multilingual (M3L) neural topic model for comparable data that maps texts from multiple languages and images into a shared topic space. Our model is trained jointly on texts and images and takes advantage of pretrained document and image embeddings to abstract the complexities between different languages and modalities. As a multilingual topic model, it produces aligned language-specific topics and as multimodal model, it infers textual representations of semantic concepts in images. We demonstrate that our model is competitive with a zero-shot topic model in predicting topic distributions for comparable multilingual data and significantly outperforms a zero-shot model in predicting topic distributions for comparable texts and images. We also show that our model performs almost as well on unaligned embeddings as it does on aligned embeddings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Zero-shot Script Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.356/",
        "paper_authors": [
            "Fangzhou Zhai",
            "Vera Demberg",
            "Alexander Koller"
        ],
        "paper_abstract": "Script knowledge is useful to a variety of NLP tasks. However, existing resources only cover a small number of activities, limiting its practical usefulness. In this work, we propose a zero-shot learning approach to script parsing, the task of tagging texts with scenario-specific event and participant types, which enables us to acquire script knowledge without domain-specific annotations. We (1) learn representations of potential event and participant mentions by promoting cluster consistency according to the annotated data; (2) perform clustering on the event / participant candidates from unannotated texts that belongs to an unseen scenario. The model achieves 68.1/74.4 average F1 for event / participant parsing, respectively, outperforming a previous CRF model that, in contrast, has access to scenario-specific supervision. We also evaluate the model by testing on a different corpus, where it achieved 55.5/54.0 average F1 for event / participant parsing.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Word Sense Disambiguation with Knowledge-Enhanced and Local Self-Attention-based Extractive Sense Comprehension",
        "paper_url": "https://aclanthology.org/2022.coling-1.357/",
        "paper_authors": [
            "Guobiao Zhang",
            "Wenpeng Lu",
            "Xueping Peng",
            "Shoujin Wang",
            "Baoshuo Kan",
            "Rui Yu"
        ],
        "paper_abstract": "Word sense disambiguation (WSD), identifying the most suitable meaning of ambiguous words in the given contexts according to a predefined sense inventory, is one of the most classical and challenging tasks in natural language processing. Benefiting from the powerful ability of deep neural networks, WSD has achieved a great advancement in recent years. Reformulating WSD as a text span extraction task is an effective approach, which accepts a sentence context of an ambiguous word together with all definitions of its candidate senses simultaneously, and requires to extract the text span corresponding with the right sense. However, the approach merely depends on a short definition to learn sense representation, which neglects abundant semantic knowledge from related senses and leads to data-inefficient learning and suboptimal WSD performance. To address the limitations, we propose a novel WSD method with Knowledge-Enhanced and Local Self-Attention-based Extractive Sense Comprehension (KELESC). Specifically, a knowledge-enhanced method is proposed to enrich semantic representation by incorporating additional examples and definitions of the related senses in WordNet. Then, in order to avoid the huge computing complexity induced by the additional information, a local self-attention mechanism is utilized to constrain attention to be local, which allows longer input texts without large-scale computing burdens. Extensive experimental results demonstrate that KELESC achieves better performance than baseline models on public benchmark datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Novel Multi-Task Learning Approach for Context-Sensitive Compound Type Identification in Sanskrit",
        "paper_url": "https://aclanthology.org/2022.coling-1.358/",
        "paper_authors": [
            "Jivnesh Sandhan",
            "Ashish Gupta",
            "Hrishikesh Terdalkar",
            "Tushar Sandhan",
            "Suvendu Samanta",
            "Laxmidhar Behera",
            "Pawan Goyal"
        ],
        "paper_abstract": "The phenomenon of compounding is ubiquitous in Sanskrit. It serves for achieving brevity in expressing thoughts, while simultaneously enriching the lexical and structural formation of the language. In this work, we focus on the Sanskrit Compound Type Identification (SaCTI) task, where we consider the problem of identifying semantic relations between the components of a compound word. Earlier approaches solely rely on the lexical information obtained from the components and ignore the most crucial contextual and syntactic information useful for SaCTI. However, the SaCTI task is challenging primarily due to the implicitly encoded context-sensitive semantic relation between the compound components. Thus, we propose a novel multi-task learning architecture which incorporates the contextual information and enriches the complementary syntactic information using morphological tagging and dependency parsing as two auxiliary tasks. Experiments on the benchmark datasets for SaCTI show 6.1 points (Accuracy) and 7.7 points (F1-score) absolute gain compared to the state-of-the-art system. Further, our multi-lingual experiments demonstrate the efficacy of the proposed architecture in English and Marathi languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment",
        "paper_url": "https://aclanthology.org/2022.coling-1.359/",
        "paper_authors": [
            "Lorenzo Bertolini",
            "Julie Weeds",
            "David Weir"
        ],
        "paper_abstract": "Previous work has demonstrated that pre-trained large language models (LLM) acquire knowledge during pre-training which enables reasoning over relationships between words (e.g, hyponymy) and more complex inferences over larger units of meaning such as sentences. Here, we investigate whether lexical entailment (LE, i.e. hyponymy or the is a relation between words) can be generalised in a compositional manner. Accordingly, we introduce PLANE (Phrase-Level Adjective-Noun Entailment), a new benchmark to test models on fine-grained compositional entailment using adjective-noun phrases. Our experiments show that knowledge extracted via In\u2013Context and transfer learning is not enough to solve PLANE. However, a LLM trained on PLANE can generalise well to out\u2013of\u2013distribution sets, since the required knowledge can be stored in the representations of subwords (SW) tokens.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Does BERT Recognize an Agent? Modeling Dowty\u2019s Proto-Roles with Contextual Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.360/",
        "paper_authors": [
            "Mattia Proietti",
            "Gianluca Lebani",
            "Alessandro Lenci"
        ],
        "paper_abstract": "Contextual embeddings build multidimensional representations of word tokens based on their context of occurrence. Such models have been shown to achieve a state-of-the-art performance on a wide variety of tasks. Yet, the community struggles in understanding what kind of semantic knowledge these representations encode. We report a series of experiments aimed at investigating to what extent one of such models, BERT, is able to infer the semantic relations that, according to Dowty\u2019s Proto-Roles theory, a verbal argument receives by virtue of its role in the event described by the verb. This hypothesis were put to test by learning a linear mapping from the BERT\u2019s verb embeddings to an interpretable space of semantic properties built from the linguistic dataset by White et al. (2016). In a first experiment we tested whether the semantic properties inferred from a typed version of the BERT embeddings would be more linguistically plausible than those produced by relying on static embeddings. We then move to evaluate the semantic properties inferred from the contextual embeddings both against those available in the original dataset, as well as by assessing their ability to model the semantic properties possessed by the agent of the verbs participating in the so-called causative alternation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders",
        "paper_url": "https://aclanthology.org/2022.coling-1.361/",
        "paper_authors": [
            "Qiwei Peng",
            "David Weir",
            "Julie Weeds"
        ],
        "paper_abstract": "Previous works have demonstrated the effectiveness of utilising pre-trained sentence encoders based on their sentence representations for meaning comparison tasks. Though such representations are shown to capture hidden syntax structures, the direct similarity comparison between them exhibits weak sensitivity to word order and structural differences in given sentences. A single similarity score further makes the comparison process hard to interpret. Therefore, we here propose to combine sentence encoders with an alignment component by representing each sentence as a list of predicate-argument spans (where their span representations are derived from sentence encoders), and decomposing the sentence-level meaning comparison into the alignment between their spans for paraphrase identification tasks. Empirical results show that the alignment component brings in both improved performance and interpretability for various sentence encoders. After closer investigation, the proposed approach indicates increased sensitivity to structural difference and enhanced ability to distinguish non-paraphrases with high lexical overlap.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CILex: An Investigation of Context Information for Lexical Substitution Methods",
        "paper_url": "https://aclanthology.org/2022.coling-1.362/",
        "paper_authors": [
            "Sandaru Seneviratne",
            "Elena Daskalaki",
            "Artem Lenskiy",
            "Hanna Suominen"
        ],
        "paper_abstract": "Lexical substitution, which aims to generate substitutes for a target word given a context, is an important natural language processing task useful in many applications. Due to the paucity of annotated data, existing methods for lexical substitution tend to rely on manually curated lexical resources and contextual word embedding models. Methods based on lexical resources are likely to miss relevant substitutes whereas relying only on contextual word embedding models fails to provide adequate information on the impact of a substitute in the entire context and the overall meaning of the input. We proposed CILex, which uses contextual sentence embeddings along with methods that capture additional context information complimenting contextual word embeddings for lexical substitution. This ensured the semantic consistency of a substitute with the target word while maintaining the overall meaning of the sentence. Our experimental comparisons with previously proposed methods indicated that our solution is now the state-of-the-art on both the widely used LS07 and CoInCo datasets with P@1 scores of 55.96% and 57.25% for lexical substitution. The implementation of the proposed approach is available at https://github.com/sandaruSen/CILex under the MIT license.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Emotion Enriched Retrofitted Word Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.363/",
        "paper_authors": [
            "Sapan Shah",
            "Sreedhar Reddy",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "Word embeddings learned using the distributional hypothesis (e.g., GloVe, Word2vec) are good at encoding various lexical-semantic relations. However, they do not capture the emotion aspects of words. We present a novel retrofitting method for updating the vectors of emotion bearing words like fun, offence, angry, etc. The retrofitted embeddings achieve better inter-cluster and intra-cluster distance for words having the same emotions, e.g., the joy cluster containing words like fun, happiness, etc., and the anger cluster with words like offence, rage, etc., as evaluated through different cluster quality metrics. For the downstream tasks on sentiment analysis and sarcasm detection, simple classification models, such as SVM and Attention Net, learned using our retrofitted embeddings perform better than their pre-trained counterparts (about 1.5 % improvement in F1-score) as well as other benchmarks. Furthermore, the difference in performance is more pronounced in the limited data setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Metaphor Detection via Linguistics Enhanced Siamese Network",
        "paper_url": "https://aclanthology.org/2022.coling-1.364/",
        "paper_authors": [
            "Shenglong Zhang",
            "Ying Liu"
        ],
        "paper_abstract": "In this paper we present MisNet, a novel model for word level metaphor detection. MisNet converts two linguistic rules, i.e., Metaphor Identification Procedure (MIP) and Selectional Preference Violation (SPV) into semantic matching tasks. MIP module computes the similarity between the contextual meaning and the basic meaning of a target word. SPV module perceives the incongruity between target words and their contexts. To better represent basic meanings, MisNet utilizes dictionary resources. Empirical results indicate that MisNet achieves competitive performance on several datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fast and Accurate End-to-End Span-based Semantic Role Labeling as Word-based Graph Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.365/",
        "paper_authors": [
            "Shilin Zhou",
            "Qingrong Xia",
            "Zhenghua Li",
            "Yu Zhang",
            "Yu Hong",
            "Min Zhang"
        ],
        "paper_abstract": "This paper proposes to cast end-to-end span-based SRL as a word-based graph parsing task. The major challenge is how to represent spans at the word level. Borrowing ideas from research on Chinese word segmentation and named entity recognition, we propose and compare four different schemata of graph representation, i.e., BES, BE, BIES, and BII, among which we find that the BES schema performs the best. We further gain interesting insights through detailed analysis. Moreover, we propose a simple constrained Viterbi procedure to ensure the legality of the output graph according to the constraints of the SRL structure. We conduct experiments on two widely used benchmark datasets, i.e., CoNLL05 and CoNLL12. Results show that our word-based graph parsing approach achieves consistently better performance than previous results, under all settings of end-to-end and predicate-given, without and with pre-trained language models (PLMs). More importantly, our model can parse 669/252 sentences per second, without and with PLMs respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Lexical Substitution with Decontextualised Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.366/",
        "paper_authors": [
            "Takashi Wada",
            "Timothy Baldwin",
            "Yuji Matsumoto",
            "Jey Han Lau"
        ],
        "paper_abstract": "We propose a new unsupervised method for lexical substitution using pre-trained language models. Compared to previous approaches that use the generative capability of language models to predict substitutes, our method retrieves substitutes based on the similarity of contextualised and decontextualised word embeddings, i.e. the average contextual representation of a word in multiple contexts. We conduct experiments in English and Italian, and show that our method substantially outperforms strong baselines and establishes a new state-of-the-art without any explicit supervision or fine-tuning. We further show that our method performs particularly well at predicting low-frequency substitutes, and also generates a diverse list of substitute candidates, reducing morphophonetic or morphosyntactic biases induced by article-noun agreement.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Transparent Semantic Parsing with Universal Dependencies Using Graph Transformations",
        "paper_url": "https://aclanthology.org/2022.coling-1.367/",
        "paper_authors": [
            "Wessel Poelman",
            "Rik van Noord",
            "Johan Bos"
        ],
        "paper_abstract": "Even though many recent semantic parsers are based on deep learning methods, we should not forget that rule-based alternatives might offer advantages over neural approaches with respect to transparency, portability, and explainability. Taking advantage of existing off-the-shelf Universal Dependency parsers, we present a method that maps a syntactic dependency tree to a formal meaning representation based on Discourse Representation Theory. Rather than using lambda calculus to manage variable bindings, our approach is novel in that it consists of using a series of graph transformations. The resulting UD semantic parser shows good performance for English, German, Italian and Dutch, with F-scores over 75%, outperforming a neural semantic parser for the lower-resourced languages. Unlike neural semantic parsers, our UD semantic parser does not hallucinate output, is relatively easy to port to other languages, and is completely transparent.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multilingual Word Sense Disambiguation with Unified Sense Representation",
        "paper_url": "https://aclanthology.org/2022.coling-1.368/",
        "paper_authors": [
            "Ying Su",
            "Hongming Zhang",
            "Yangqiu Song",
            "Tong Zhang"
        ],
        "paper_abstract": "As a key natural language processing (NLP) task, word sense disambiguation (WSD) evaluates how well NLP models can understand the fine-grained semantics of words under specific contexts. Benefited from the large-scale annotation, current WSD systems have achieved impressive performances in English by combining supervised learning with lexical knowledge. However, such success is hard to be replicated in other languages, where we only have very limited annotations. In this paper, based on that the multilingual lexicon BabelNet describing the same set of concepts across languages, we propose to build knowledge and supervised based Multilingual Word Sense Disambiguation (MWSD) systems. We build unified sense representations for multiple languages and address the annotation scarcity problem for MWSD by transferring annotations from rich sourced languages. With the unified sense representations, annotations from multiple languages can be jointly trained to benefit the MWSD tasks. Evaluations of SemEval-13 and SemEval-15 datasets demonstrate the effectiveness of our methodology.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Transition-based Method for Complex Question Understanding",
        "paper_url": "https://aclanthology.org/2022.coling-1.369/",
        "paper_authors": [
            "Yu Xia",
            "Wenbin Jiang",
            "Yajuan Lyu",
            "Sujian Li"
        ],
        "paper_abstract": "Complex Question Understanding (CQU) parses complex questions to Question Decomposition Meaning Representation (QDMR) which is a sequence of atomic operators. Existing works are based on end-to-end neural models which do not explicitly model the intermediate states and lack interpretability for the parsing process. Besides, they predict QDMR in a mismatched granularity and do not model the step-wise information which is an essential characteristic of QDMR. To alleviate the issues, we treat QDMR as a computational graph and propose a transition-based method where a decider predicts a sequence of actions to build the graph node-by-node. In this way, the partial graph at each step enables better representation of the intermediate states and better interpretability. At each step, the decider encodes the intermediate state with specially designed encoders and predicts several candidates of the next action and its confidence. For inference, a searcher seeks the optimal graph based on the predictions of the decider to alleviate the error propagation. Experimental results demonstrate the parsing accuracy of our method against several strong baselines. Moreover, our method has transparent and human-readable intermediate results, showing improved interpretability.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures inside Arguments",
        "paper_url": "https://aclanthology.org/2022.coling-1.370/",
        "paper_authors": [
            "Yu Zhang",
            "Qingrong Xia",
            "Shilin Zhou",
            "Yong Jiang",
            "Guohong Fu",
            "Min Zhang"
        ],
        "paper_abstract": "Semantic role labeling (SRL) is a fundamental yet challenging task in the NLP community. Recent works of SRL mainly fall into two lines: 1) BIO-based; 2) span-based. Despite ubiquity, they share some intrinsic drawbacks of not considering internal argument structures, potentially hindering the model\u2019s expressiveness. The key challenge is arguments are flat structures, and there are no determined subtree realizations for words inside arguments. To remedy this, in this paper, we propose to regard flat argument spans as latent subtrees, accordingly reducing SRL to a tree parsing task. In particular, we equip our formulation with a novel span-constrained TreeCRF to make tree structures span-aware and further extend it to the second-order case. We conduct extensive experiments on CoNLL05 and CoNLL12 benchmarks. Results reveal that our methods perform favorably better than all previous syntax-agnostic works, achieving new state-of-the-art under both end-to-end and w/ gold predicates settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Noisy Label Regularisation for Textual Regression",
        "paper_url": "https://aclanthology.org/2022.coling-1.371/",
        "paper_authors": [
            "Yuxia Wang",
            "Timothy Baldwin",
            "Karin Verspoor"
        ],
        "paper_abstract": "Training with noisy labelled data is known to be detrimental to model performance, especially for high-capacity neural network models in low-resource domains. Our experiments suggest that standard regularisation strategies, such as weight decay and dropout, are ineffective in the face of noisy labels. We propose a simple noisy label detection method that prevents error propagation from the input layer. The approach is based on the observation that the projection of noisy labels is learned through memorisation at advanced stages of learning, and that the Pearson correlation is sensitive to outliers. Extensive experiments over real-world human-disagreement annotations as well as randomly-corrupted and data-augmented labels, across various tasks and domains, demonstrate that our method is effective, regularising noisy labels and improving generalisation performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting Suicide Risk in Online Counseling Services: A Study in a Low-Resource Language",
        "paper_url": "https://aclanthology.org/2022.coling-1.372/",
        "paper_authors": [
            "Amir Bialer",
            "Daniel Izmaylov",
            "Avi Segal",
            "Oren Tsur",
            "Yossi Levi-Belz",
            "Kobi Gal"
        ],
        "paper_abstract": "With the increased awareness of situations of mental crisis and their societal impact, online services providing emergency support are becoming commonplace in many countries. Computational models, trained on discussions between help-seekers and providers, can support suicide prevention by identifying at-risk individuals. However, the lack of domain-specific models, especially in low-resource languages, poses a significant challenge for the automatic detection of suicide risk. We propose a model that combines pre-trained language models (PLM) with a fixed set of manually crafted (and clinically approved) set of suicidal cues, followed by a two-stage fine-tuning process. Our model achieves 0.91 ROC-AUC and an F2-score of 0.55, significantly outperforming an array of strong baselines even early on in the conversation, which is critical for real-time detection in the field. Moreover, the model performs well across genders and age groups.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Does Meta-learning Help mBERT for Few-shot Question Generation in a Cross-lingual Transfer Setting for Indic Languages?",
        "paper_url": "https://aclanthology.org/2022.coling-1.373/",
        "paper_authors": [
            "Aniruddha Roy",
            "Rupak Kumar Thakur",
            "Isha Sharma",
            "Ashim Gupta",
            "Amrith Krishna",
            "Sudeshna Sarkar",
            "Pawan Goyal"
        ],
        "paper_abstract": "Few-shot Question Generation (QG) is an important and challenging problem in the Natural Language Generation (NLG) domain. Multilingual BERT (mBERT) has been successfully used in various Natural Language Understanding (NLU) applications. However, the question of how to utilize mBERT for few-shot QG, possibly with cross-lingual transfer, remains. In this paper, we try to explore how mBERT performs in few-shot QG (cross-lingual transfer) and also whether applying meta-learning on mBERT further improves the results. In our setting, we consider mBERT as the base model and fine-tune it using a seq-to-seq language modeling framework in a cross-lingual setting. Further, we apply the model agnostic meta-learning approach to our base model. We evaluate our model for two low-resource Indian languages, Bengali and Telugu, using the TyDi QA dataset. The proposed approach consistently improves the performance of the base model in few-shot settings and even works better than some heavily parameterized models. Human evaluation also confirms the effectiveness of our approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Revisiting Syllables in Language Modelling and Their Application on Low-Resource Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.374/",
        "paper_authors": [
            "Arturo Oncevay",
            "Kervy Dante Rivas Rojas",
            "Liz Karen Chavez Sanchez",
            "Roberto Zariquiey"
        ],
        "paper_abstract": "Language modelling and machine translation tasks mostly use subword or character inputs, but syllables are seldom used. Syllables provide shorter sequences than characters, require less-specialised extracting rules than morphemes, and their segmentation is not impacted by the corpus size. In this study, we first explore the potential of syllables for open-vocabulary language modelling in 21 languages. We use rule-based syllabification methods for six languages and address the rest with hyphenation, which works as a syllabification proxy. With a comparable perplexity, we show that syllables outperform characters and other subwords. Moreover, we study the importance of syllables on neural machine translation for a non-related and low-resource language-pair (Spanish\u2013Shipibo-Konibo). In pairwise and multilingual systems, syllables outperform unsupervised subwords, and further morphological segmentation methods, when translating into a highly synthetic language with a transparent orthography (Shipibo-Konibo). Finally, we perform some human evaluation, and discuss limitations and opportunities.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Aligning Multilingual Embeddings for Improved Code-switched Natural Language Understanding",
        "paper_url": "https://aclanthology.org/2022.coling-1.375/",
        "paper_authors": [
            "Barah Fazili",
            "Preethi Jyothi"
        ],
        "paper_abstract": "Multilingual pretrained models, while effective on monolingual data, need additional training to work well with code-switched text. In this work, we present a novel idea of training multilingual models with alignment objectives using parallel text so as to explicitly align word representations with the same underlying semantics across languages. Such an explicit alignment step has a positive downstream effect and improves performance on multiple code-switched NLP tasks. We explore two alignment strategies and report improvements of up to 7.32%, 0.76% and 1.9% on Hindi-English Sentiment Analysis, Named Entity Recognition and Question Answering tasks compared to a competitive baseline model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fashioning Local Designs from Generic Speech Technologies in an Australian Aboriginal Community",
        "paper_url": "https://aclanthology.org/2022.coling-1.376/",
        "paper_authors": [
            "\u00c9ric Le Ferrand",
            "Steven Bird",
            "Laurent Besacier"
        ],
        "paper_abstract": "An increasing number of papers have been addressing issues related to low-resource languages and the transcription bottleneck paradigm. After several years spent in Northern Australia, where some of the strongest Aboriginal languages are spoken, we could observe a gap between the motivations depicted in research contributions in this space and the Northern Australian context. In this paper, we address this gap in research by exploring the potential of speech recognition in an Aboriginal community. We describe our work from training a spoken term detection system to its implementation in an activity with Aboriginal participants. We report here on one side how speech recognition technologies can find their place in an Aboriginal context and, on the other, methodological paths that allowed us to reach better comprehension and engagement from Aboriginal participants.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Few-Shot Pidgin Text Adaptation via Contrastive Fine-Tuning",
        "paper_url": "https://aclanthology.org/2022.coling-1.377/",
        "paper_authors": [
            "Ernie Chang",
            "Jesujoba O. Alabi",
            "David Ifeoluwa Adelani",
            "Vera Demberg"
        ],
        "paper_abstract": "The surging demand for multilingual dialogue systems often requires a costly labeling process for each language addition. For low resource languages, human annotators are continuously tasked with the adaptation of resource-rich language utterances for each new domain. However, this prohibitive and impractical process can often be a bottleneck for low resource languages that are still without proper translation systems nor parallel corpus. In particular, it is difficult to obtain task-specific low resource language annotations for the English-derived creoles (e.g. Nigerian and Cameroonian Pidgin). To address this issue, we utilize the pretrained language models i.e. BART which has shown great potential in language generation/understanding \u2013 we propose to finetune the BART model to generate utterances in Pidgin by leveraging the proximity of the source and target languages, and utilizing positive and negative examples in constrastive training objectives. We collected and released the first parallel Pidgin-English conversation corpus in two dialogue domains and showed that this simple and effective technique is suffice to yield impressive results for English-to-Pidgin generation, which are two closely-related languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Penalizing Divergence: Multi-Parallel Translation for Low-Resource Languages of North America",
        "paper_url": "https://aclanthology.org/2022.coling-1.378/",
        "paper_authors": [
            "Garrett Nicolai",
            "Changbing Yang",
            "Miikka Silfverberg"
        ],
        "paper_abstract": "This paper explores a special case in multilingual machine translation: so called multi-parallel translation, where the target data for all language pairs are identical. While multi-parallelism offers benefits which are not available in a standard translation setting, translation models can easily overfit when training data are limited. We introduce a regularizer, the divergence penalty, which penalizes the translation model when it represents source sentences with identical target translations in divergent ways. Experiments on very low-resourced Indigenous North American languages show that an initially deficient multilingual translator can improve by 4.9 BLEU through mBART pre-training, and 5.5 BLEU points with the strategic addition of monolingual data, and that a divergence penalty leads to further increases of 0.4 BLEU. Further experiments on Germanic languages demonstrate a improvement of 0.5 BLEU when applying the divergence penalty. An investigation of the neural encoder representations learned by our translation models shows that the divergence penalty encourages models to learn a unified neural interlingua.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Assessing Digital Language Support on a Global Scale",
        "paper_url": "https://aclanthology.org/2022.coling-1.379/",
        "paper_authors": [
            "Gary F. Simons",
            "Abbey L. L. Thomas",
            "Chad K. K. White"
        ],
        "paper_abstract": "The users of endangered languages struggle to thrive in a digitally-mediated world. We have developed an automated method for assessing how well every language recognized by ISO 639 is faring in terms of digital language support. The assessment is based on scraping the names of supported languages from the websites of 143 digital tools selected to represent a full range of ways that digital technology can support languages. The method uses Mokken scale analysis to produce an explainable model for quantifying digital language support and monitoring it on a global scale.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Persian Natural Language Inference: A Meta-learning Approach",
        "paper_url": "https://aclanthology.org/2022.coling-1.380/",
        "paper_authors": [
            "Heydar Soudani",
            "Mohammad Hassan Mojab",
            "Hamid Beigy"
        ],
        "paper_abstract": "Incorporating information from other languages can improve the results of tasks in low-resource languages. A powerful method of building functional natural language processing systems for low-resource languages is to combine multilingual pre-trained representations with cross-lingual transfer learning. In general, however, shared representations are learned separately, either across tasks or across languages. This paper proposes a meta-learning approach for inferring natural language in Persian. Alternately, meta-learning uses different task information (such as QA in Persian) or other language information (such as natural language inference in English). Also, we investigate the role of task augmentation strategy for forming additional high-quality tasks. We evaluate the proposed method using four languages and an auxiliary task. Compared to the baseline approach, the proposed model consistently outperforms it, improving accuracy by roughly six percent. We also examine the effect of finding appropriate initial parameters using zero-shot evaluation and CCA similarity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Global Readiness of Language Technology for Healthcare: What Would It Take to Combat the Next Pandemic?",
        "paper_url": "https://aclanthology.org/2022.coling-1.381/",
        "paper_authors": [
            "Ishani Mondal",
            "Kabir Ahuja",
            "Mohit Jain",
            "Jacki O\u2019Neill",
            "Kalika Bali",
            "Monojit Choudhury"
        ],
        "paper_abstract": "The COVID-19 pandemic has brought out both the best and worst of language technology (LT). On one hand, conversational agents for information dissemination and basic diagnosis have seen widespread use, and arguably, had an important role in fighting against the pandemic. On the other hand, it has also become clear that such technologies are readily available for a handful of languages, and the vast majority of the global south is completely bereft of these benefits. What is the state of LT, especially conversational agents, for healthcare across the world\u2019s languages? And, what would it take to ensure global readiness of LT before the next pandemic? In this paper, we try to answer these questions through survey of existing literature and resources, as well as through a rapid chatbot building exercise for 15 Asian and African languages with varying amount of resource-availability. The study confirms the pitiful state of LT even for languages with large speaker bases, such as Sinhala and Hausa, and identifies the gaps that could help us prioritize research and investment strategies in LT for healthcare.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning",
        "paper_url": "https://aclanthology.org/2022.coling-1.382/",
        "paper_authors": [
            "Jesujoba O. Alabi",
            "David Ifeoluwa Adelani",
            "Marius Mosbach",
            "Dietrich Klakow"
        ],
        "paper_abstract": "Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) \u2014 fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Noun Class Disambiguation in Runyankore and Related Languages",
        "paper_url": "https://aclanthology.org/2022.coling-1.383/",
        "paper_authors": [
            "Joan Byamugisha"
        ],
        "paper_abstract": "Bantu languages are spoken by communities in more than half of the countries on the African continent by an estimated third of a billion people. Despite this populous and the amount of high quality linguistic research done over the years, Bantu languages are still computationally under-resourced. The biggest limitation to the development of computational methods for processing Bantu language text is their complex grammatical structure, chiefly in the system of noun classes. We investigated the use of a combined syntactic and semantic method to disambiguate among singular nouns with the same class prefix but belonging to different noun classes. This combination uses the semantic generalizations of the types of nouns in each class to overcome the limitations of relying only on the prefixes they take. We used the nearest neighbors of a query word as semantic generalizations, and developed a tool to determine the noun class based on resources in Runyankore, a Bantu language indigenous to Uganda. We also investigated whether, with the same Runyankore resources, our method had utility in other Bantu languages, Luganda, indigenous to Uganda, and Kinyarwanda, indigenous to Rwanda. For all three languages, the combined approach resulted in an improvement in accuracy, as compared to using only the syntactic or the semantic approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Low-resource RRG Parsing with Cross-lingual Self-training",
        "paper_url": "https://aclanthology.org/2022.coling-1.384/",
        "paper_authors": [
            "Kilian Evang",
            "Laura Kallmeyer",
            "Jakub Waszczuk",
            "Kilu von Prince",
            "Tatiana Bladier",
            "Simon Petitjean"
        ],
        "paper_abstract": "This paper considers the task of parsing low-resource languages in a scenario where parallel English data and also a limited seed of annotated sentences in the target language are available, as for example in bootstrapping parallel treebanks. We focus on constituency parsing using Role and Reference Grammar (RRG), a theory that has so far been understudied in computational linguistics but that is widely used in typological research, i.e., in particular in the context of low-resource languages. Starting from an existing RRG parser, we propose two strategies for low-resource parsing: first, we extend the parsing model into a cross-lingual parser, exploiting the parallel data in the high-resource language and unsupervised word alignments by providing internal states of the source-language parser to the target-language parser. Second, we adopt self-training, thereby iteratively expanding the training data, starting from the seed, by including the most confident new parses in each round. Both in simulated scenarios and with a real low-resource language (Daakaka), we find substantial and complementary improvements from both self-training and cross-lingual parsing. Moreover, we also experimented with using gloss embeddings in addition to token embeddings in the target language, and this also improves results. Finally, starting from what we have for Daakaka, we also consider parsing a related language (Dalkalaen) where glosses and English translations are available but no annotated trees at all, i.e., a no-resource scenario wrt. syntactic annotations. We start with cross-lingual parser trained on Daakaka with glosses and use self-training to adapt it to Dalkalaen. The results are surprisingly good.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Simple and Effective Method to Improve Zero-Shot Cross-Lingual Transfer Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.385/",
        "paper_authors": [
            "Kunbo Ding",
            "Weijie Liu",
            "Yuejian Fang",
            "Weiquan Mao",
            "Zhe Zhao",
            "Tao Zhu",
            "Haoyan Liu",
            "Rong Tian",
            "Yiren Chen"
        ],
        "paper_abstract": "Existing zero-shot cross-lingual transfer methods rely on parallel corpora or bilingual dictionaries, which are expensive and impractical for low-resource languages. To disengage from these dependencies, researchers have explored training multilingual models on English-only resources and transferring them to low-resource languages. However, its effect is limited by the gap between embedding clusters of different languages. To address this issue, we propose Embedding-Push, Attention-Pull, and Robust targets to transfer English embeddings to virtual multilingual embeddings without semantic loss, thereby improving cross-lingual transferability. Experimental results on mBERT and XLM-R demonstrate that our method significantly outperforms previous works on the zero-shot cross-lingual text classification task and can obtain a better multilingual alignment.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings",
        "paper_url": "https://aclanthology.org/2022.coling-1.386/",
        "paper_authors": [
            "Linlin Liu",
            "Thien Hai Nguyen",
            "Shafiq Joty",
            "Lidong Bing",
            "Luo Si"
        ],
        "paper_abstract": "Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual tasks. However, most existing approaches to learn CLWE including the ones with contextual embeddings are sense agnostic. In this work, we propose a novel framework to align contextual embeddings at the sense level by leveraging cross-lingual signal from bilingual dictionaries only. We operationalize our framework by first proposing a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate significant performance improvement for word sense disambiguation tasks. We then propose a sense alignment objective on top of the sense-aware cross entropy loss for cross-lingual model pretraining, and pretrain cross-lingual models for several language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best baseline results, our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "How to Parse a Creole: When Martinican Creole Meets French",
        "paper_url": "https://aclanthology.org/2022.coling-1.387/",
        "paper_authors": [
            "Ludovic Mompelat",
            "Daniel Dakota",
            "Sandra K\u00fcbler"
        ],
        "paper_abstract": "We investigate methods to develop a parser for Martinican Creole, a highly under-resourced language, using a French treebank. We compare transfer learning and multi-task learning models and examine different input features and strategies to handle the massive size imbalance between the treebanks. Surprisingly, we find that a simple concatenated (French + Martinican Creole) baseline yields optimal results even though it has access to only 80 Martinican Creole sentences. POS embeddings work better than lexical ones, but they suffer from negative transfer.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Byte-based Multilingual NMT for Endangered Languages",
        "paper_url": "https://aclanthology.org/2022.coling-1.388/",
        "paper_authors": [
            "Mengjiao Zhang",
            "Jia Xu"
        ],
        "paper_abstract": "Multilingual neural machine translation (MNMT) jointly trains a shared model for translation with multiple language pairs. However, traditional subword-based MNMT approaches suffer from out-of-vocabulary (OOV) issues and representation bottleneck, which often degrades translation performance on certain language pairs. While byte tokenization is used to tackle the OOV problems in neural machine translation (NMT), until now its capability has not been validated in MNMT. Additionally, existing work has not studied how byte encoding can benefit endangered language translation to our knowledge. We propose a byte-based multilingual neural machine translation system (BMNMT) to alleviate the representation bottleneck and improve translation performance in endangered languages. Furthermore, we design a random byte mapping method with an ensemble prediction to enhance our model robustness. Experimental results show that our BMNMT consistently and significantly outperforms subword/word-based baselines on twelve language pairs up to +18.5 BLEU points, an 840% relative improvement.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BRCC and SentiBahasaRojak: The First Bahasa Rojak Corpus for Pretraining and Sentiment Analysis Dataset",
        "paper_url": "https://aclanthology.org/2022.coling-1.389/",
        "paper_authors": [
            "Nanda Putri Romadhona",
            "Sin-En Lu",
            "Bo-Han Lu",
            "Richard Tzong-Han Tsai"
        ],
        "paper_abstract": "Code-mixing refers to the mixed use of multiple languages. It is prevalent in multilingual societies and is also one of the most challenging natural language processing tasks. In this paper, we study Bahasa Rojak, a dialect popular in Malaysia that consists of English, Malay, and Chinese. Aiming to establish a model to deal with the code-mixing phenomena of Bahasa Rojak, we use data augmentation to automatically construct the first Bahasa Rojak corpus for pre-training language models, which we name the Bahasa Rojak Crawled Corpus (BRCC). We also develop a new pre-trained model called \u201cMixed XLM\u201d. The model can tag the language of the input token automatically to process code-mixing input. Finally, to test the effectiveness of the Mixed XLM model pre-trained on BRCC for social media scenarios where code-mixing is found frequently, we compile a new Bahasa Rojak sentiment analysis dataset, SentiBahasaRojak, with a Kappa value of 0.77.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "WordNet-QU: Development of a Lexical Database for Quechua Varieties",
        "paper_url": "https://aclanthology.org/2022.coling-1.390/",
        "paper_authors": [
            "Nelsi Melgarejo",
            "Rodolfo Zevallos",
            "Hector Gomez",
            "John E. Ortega"
        ],
        "paper_abstract": "In the effort to minimize the risk of extinction of a language, linguistic resources are fundamental. Quechua, a low-resource language from South America, is a language spoken by millions but, despite several efforts in the past, still lacks the resources necessary to build high-performance computational systems. In this article, we present WordNet-QU which signifies the inclusion of Quechua in a well-known lexical database called wordnet. We propose WordNet-QU to be included as an extension to wordnet after demonstrating a manually-curated collection of multiple digital resources for lexical use in Quechua. Our work uses the synset alignment algorithm to compare Quechua to its geographically nearest high-resource language, Spanish. Altogether, we propose a total of 28,582 unique synset IDs divided according to region like so: 20510 for Southern Quechua, 5993 for Central Quechua, 1121 for Northern Quechua, and 958 for Amazonian Quechua.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "When the Student Becomes the Master: Learning Better and Smaller Monolingual Models from mBERT",
        "paper_url": "https://aclanthology.org/2022.coling-1.391/",
        "paper_authors": [
            "Pranaydeep Singh",
            "Els Lefever"
        ],
        "paper_abstract": "In this research, we present pilot experiments to distil monolingual models from a jointly trained model for 102 languages (mBERT). We demonstrate that it is possible for the target language to outperform the original model, even with a basic distillation setup. We evaluate our methodology for 6 languages with varying amounts of resources and language families.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Zero-shot Disfluency Detection for Indian Languages",
        "paper_url": "https://aclanthology.org/2022.coling-1.392/",
        "paper_authors": [
            "Rohit Kundu",
            "Preethi Jyothi",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "Disfluencies that appear in the transcriptions from automatic speech recognition systems tend to impair the performance of downstream NLP tasks. Disfluency correction models can help alleviate this problem. However, the unavailability of labeled data in low-resource languages impairs progress. We propose using a pretrained multilingual model, finetuned only on English disfluencies, for zero-shot disfluency detection in Indian languages. We present a detailed pipeline to synthetically generate disfluent text and create evaluation datasets for four Indian languages: Bengali, Hindi, Malayalam, and Marathi. Even in the zero-shot setting, we obtain F1 scores of 75 and higher on five disfluency types across all four languages. We also show the utility of synthetically generated disfluencies by evaluating on real disfluent text in Bengali, Hindi, and Marathi. Finetuning the multilingual model on additional synthetic Hindi disfluent text nearly doubles the number of exact matches and yields a 20-point boost in F1 scores when evaluated on real Hindi disfluent text, compared to training with only English disfluent text.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating Word Embeddings in Extremely Under-Resourced Languages: A Case Study in Bribri",
        "paper_url": "https://aclanthology.org/2022.coling-1.393/",
        "paper_authors": [
            "Rolando Coto-Solano"
        ],
        "paper_abstract": "Word embeddings are critical for numerous NLP tasks but their evaluation in actual under-resourced settings needs further examination. This paper presents a case study in Bribri, a Chibchan language from Costa Rica. Four experiments were adapted from English: Word similarities, WordSim353 correlations, odd-one-out tasks and analogies. Here we discuss their adaptation to an under-resourced Indigenous language and we use them to measure semantic and morphological learning. We trained 96 word2vec models with different hyperparameter combinations. The best models for this under-resourced scenario were Skip-grams with an intermediate size (100 dimensions) and large window sizes (10). These had an average correlation of r=0.28 with WordSim353, a 76% accuracy in semantic odd-one-out and 70% accuracy in structural/morphological odd-one-out. The performance was lower for the analogies: The best models could find the appropriate semantic target amongst the first 25 results approximately 60% of the times, but could only find the morphological/structural target 11% of the times. Future research needs to further explore the patterns of morphological/structural learning, to examine the behavior of deep learning embeddings, and to establish a human baseline. This project seeks to improve Bribri NLP and ultimately help in its maintenance and revitalization.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Applying Natural Annotation and Curriculum Learning to Named Entity Recognition for Under-Resourced Languages",
        "paper_url": "https://aclanthology.org/2022.coling-1.394/",
        "paper_authors": [
            "Valeriy Lobov",
            "Alexandra Ivoylova",
            "Serge Sharoff"
        ],
        "paper_abstract": "Current practices in building new NLP models for low-resourced languages rely either on Machine Translation of training sets from better resourced languages or on cross-lingual transfer from them. Still we can see a considerable performance gap between the models originally trained within better resourced languages and the models transferred from them. In this study we test the possibility of (1) using natural annotation to build synthetic training sets from resources not initially designed for the target downstream task and (2) employing curriculum learning methods to select the most suitable examples from synthetic training sets. We test this hypothesis across seven Slavic languages and across three curriculum learning strategies on Named Entity Recognition as the downstream task. We also test the possibility of fine-tuning the synthetic resources to reflect linguistic properties, such as the grammatical case and gender, both of which are important for the Slavic languages. We demonstrate the possibility to achieve the mean F1 score of 0.78 across the three basic entities types for Belarusian starting from zero resources in comparison to the baseline of 0.63 using the zero-shot transfer from English. For comparison, the English model trained on the original set achieves the mean F1-score of 0.75. The experimental results are available from https://github.com/ValeraLobov/SlavNER",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Taking Actions Separately: A Bidirectionally-Adaptive Transfer Learning Method for Low-Resource Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.395/",
        "paper_authors": [
            "Xiaolin Xing",
            "Yu Hong",
            "Minhan Xu",
            "Jianmin Yao",
            "Guodong Zhou"
        ],
        "paper_abstract": "Training Neural Machine Translation (NMT) models suffers from sparse parallel data, in the infrequent translation scenarios towards low-resource source languages. The existing solutions primarily concentrate on the utilization of Parent-Child (PC) transfer learning. It transfers well-trained NMT models on high-resource languages (namely Parent NMT) to low-resource languages, so as to produce Child NMT models by fine-tuning. It has been carefully demonstrated that a variety of PC variants yield significant improvements for low-resource NMT. In this paper, we intend to enhance PC-based NMT by a bidirectionally-adaptive learning strategy. Specifically, we divide inner constituents (6 transformers) of Parent encoder into two \u201cteams\u201d, i.e., T1 and T2. During representation learning, T1 learns to encode low-resource languages conditioned on bilingual shareable latent space. Generative adversarial network and masked language modeling are used for space-shareable encoding. On the other hand, T2 is straightforwardly transferred to low-resource languages, and fine-tuned together with T1 for low-resource translation. Briefly, T1 and T2 take actions separately for different goals. The former aims to adapt to characteristics of low-resource languages during encoding, while the latter adapts to translation experiences learned from high-resource languages. We experiment on benchmark corpora SETIMES, conducting low-resource NMT for Albanian (Sq), Macedonian (Mk), Croatian (Hr) and Romanian (Ro). Experimental results show that our method yields substantial improvements, which allows the NMT performance to reach BLEU4-scores of 62.24%, 56.93%, 50.53% and 54.65% for Sq, Mk, Hr and Ro, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "HCLD: A Hierarchical Framework for Zero-shot Cross-lingual Dialogue System",
        "paper_url": "https://aclanthology.org/2022.coling-1.396/",
        "paper_authors": [
            "Zhanyu Ma",
            "Jian Ye",
            "Xurui Yang",
            "Jianfeng Liu"
        ],
        "paper_abstract": "Recently, many task-oriented dialogue systems need to serve users in different languages. However, it is time-consuming to collect enough data of each language for training. Thus, zero-shot adaptation of cross-lingual task-oriented dialog systems has been studied. Most of existing methods consider the word-level alignments to conduct two main tasks for task-oriented dialogue system, i.e., intent detection and slot filling, and they rarely explore the dependency relations among these two tasks. In this paper, we propose a hierarchical framework to classify the pre-defined intents in the high-level and fulfill slot filling under the guidance of intent in the low-level. Particularly, we incorporate sentence-level alignment among different languages to enhance the performance of intent detection. The extensive experiments report that our proposed method achieves the SOTA performance on a public task-oriented dialog dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
        "paper_url": "https://aclanthology.org/2022.coling-1.397/",
        "paper_authors": [
            "Adyasha Maharana",
            "Mohit Bansal"
        ],
        "paper_abstract": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Eureka: Neural Insight Learning for Knowledge Graph Reasoning",
        "paper_url": "https://aclanthology.org/2022.coling-1.398/",
        "paper_authors": [
            "Alex X. Zhang",
            "Xun Liang",
            "Bo Wu",
            "Xiangping Zheng",
            "Sensen Zhang",
            "Yuhui Guo",
            "Jun Wang",
            "Xinyao Liu"
        ],
        "paper_abstract": "The human recognition system has presented the remarkable ability to effortlessly learn novel knowledge from only a few trigger events based on prior knowledge, which is called insight learning. Mimicking such behavior on Knowledge Graph Reasoning (KGR) is an interesting and challenging research problem with many practical applications. Simultaneously, existing works, such as knowledge embedding and few-shot learning models, have been limited to conducting KGR in either \u201cseen-to-seen\u201d or \u201cunseen-to-unseen\u201d scenarios. To this end, we propose a neural insight learning framework named Eureka to bridge the \u201cseen\u201d to \u201cunseen\u201d gap. Eureka is empowered to learn the seen relations with sufficient training triples while providing the flexibility of learning unseen relations given only one trigger without sacrificing its performance on seen relations. Eureka meets our expectation of the model to acquire seen and unseen relations at no extra cost, and eliminate the need to retrain when encountering emerging unseen relations. Experimental results on two real-world datasets demonstrate that the proposed framework also outperforms various state-of-the-art baselines on datasets of both seen and unseen relations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CitRet: A Hybrid Model for Cited Text Span Retrieval",
        "paper_url": "https://aclanthology.org/2022.coling-1.399/",
        "paper_authors": [
            "Amit Pandey",
            "Avani Gupta",
            "Vikram Pudi"
        ],
        "paper_abstract": "The paper aims to identify cited text spans in the reference paper related to the given citance in the citing paper. We refer to it as cited text span retrieval (CTSR). Most current methods attempt this task by relying on pre-trained off-the-shelf deep learning models like SciBERT. Though these models are pre-trained on large datasets, they under-perform in out-of-domain settings. We introduce CitRet, a novel hybrid model for CTSR that leverages unique semantic and syntactic structural characteristics of scientific documents. This enables us to use significantly less data for finetuning. We use only 1040 documents for finetuning. Our model augments mildly-trained SBERT-based contextual embeddings with pre-trained non-contextual Word2Vec embeddings to calculate semantic textual similarity. We demonstrate the performance of our model on the CLSciSumm shared tasks. It improves the state-of-the-art results by over 15% on the F1 score evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Weak Supervision Approach for Predicting Difficulty of Technical Interview Questions",
        "paper_url": "https://aclanthology.org/2022.coling-1.400/",
        "paper_authors": [
            "Arpita Kundu",
            "Subhasish Ghosh",
            "Pratik Saini",
            "Tapas Nayak",
            "Indrajit Bhattacharya"
        ],
        "paper_abstract": "Predicting difficulty of questions is crucial for technical interviews. However, such questions are long-form and more open-ended than factoid and multiple choice questions explored so far for question difficulty prediction. Existing models also require large volumes of candidate response data for training. We study weak-supervision and use unsupervised algorithms for both question generation and difficulty prediction. We create a dataset of interview questions with difficulty scores for deep learning and use it to evaluate SOTA models for question difficulty prediction trained using weak supervision. Our analysis brings out the task\u2019s difficulty as well as the promise of weak supervision for it.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Reinforcement Learning with Large Action Spaces for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.401/",
        "paper_authors": [
            "Asaf Yehudai",
            "Leshem Choshen",
            "Lior Fox",
            "Omri Abend"
        ],
        "paper_abstract": "Applying Reinforcement learning (RL) following maximum likelihood estimation (MLE) pre-training is a versatile method for enhancing neural machine translation (NMT) performance. However, recent work has argued that the gains produced by RL for NMT are mostly due to promoting tokens that have already received a fairly high probability in pre-training. We hypothesize that the large action space is a main obstacle to RL\u2019s effectiveness in MT, and conduct two sets of experiments that lend support to our hypothesis. First, we find that reducing the size of the vocabulary improves RL\u2019s effectiveness. Second, we find that effectively reducing the dimension of the action space without changing the vocabulary also yields notable improvement as evaluated by BLEU, semantic similarity, and human evaluation. Indeed, by initializing the network\u2019s final fully connected layer (that maps the network\u2019s internal dimension to the vocabulary dimension), with a layer that generalizes over similar actions, we obtain a substantial improvement in RL performance: 1.5 BLEU points on average.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Noise Learning for Text Classification: A Benchmark",
        "paper_url": "https://aclanthology.org/2022.coling-1.402/",
        "paper_authors": [
            "Bo Liu",
            "Wandi Xu",
            "Yuejia Xiang",
            "Xiaojun Wu",
            "Lejian He",
            "Bowen Zhang",
            "Li Zhu"
        ],
        "paper_abstract": "Noise Learning is important in the task of text classification which depends on massive labeled data that could be error-prone. However, we find that noise learning in text classification is relatively underdeveloped: 1. many methods that have been proven effective in the image domain are not explored in text classification, 2. it is difficult to conduct a fair comparison between previous studies as they do experiments in different noise settings. In this work, we adapt four state-of-the-art methods of noise learning from the image domain to text classification. Moreover, we conduct comprehensive experiments on our benchmark of noise learning with seven commonly-used methods, four datasets, and five noise modes. Additionally, most previous works are based on an implicit hypothesis that the commonly-used datasets such as TREC, Ag-News and Chnsenticorp contain no errors. However, these datasets indeed contain 0.61% to 15.77% noise labels which we define as intrinsic noise that can cause inaccurate evaluation. Therefore, we build a new dataset Golden-Chnsenticorp( G-Chnsenticorp) without intrinsic noise to more accurately compare the effects of different noise learning methods. To the best of our knowledge, this is the first benchmark of noise learning for text classification.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mitigating the Diminishing Effect of Elastic Weight Consolidation",
        "paper_url": "https://aclanthology.org/2022.coling-1.403/",
        "paper_authors": [
            "Canasai Kruengkrai",
            "Junichi Yamagishi"
        ],
        "paper_abstract": "Elastic weight consolidation (EWC, Kirkpatrick et al. 2017) is a promising approach to addressing catastrophic forgetting in sequential training. We find that the effect of EWC can diminish when fine-tuning large-scale pre-trained language models on different datasets. We present two simple objective functions to mitigate this problem by rescaling the components of EWC. Experiments on natural language inference and fact-checking tasks indicate that our methods require much smaller values for the trade-off parameters to achieve results comparable to EWC.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Token and Head Adaptive Transformers for Efficient Natural Language Processing",
        "paper_url": "https://aclanthology.org/2022.coling-1.404/",
        "paper_authors": [
            "Chonghan Lee",
            "Md Fahim Faysal Khan",
            "Rita Brugarolas Brufau",
            "Ke Ding",
            "Vijaykrishnan Narayanan"
        ],
        "paper_abstract": "While pre-trained language models like BERT have achieved impressive results on various natural language processing tasks, deploying them on resource-restricted devices is challenging due to their intensive computational cost and memory footprint. Previous approaches mainly focused on training smaller versions of a BERT model with competitive accuracy under limited computational resources. In this paper, we extend Length Adaptive Transformer and propose to design Token and Head Adaptive Transformer, which can compress and accelerate various BERT-based models via simple fine-tuning. We train a transformer with a progressive token and head pruning scheme, eliminating a large number of redundant tokens and attention heads in the later layers. Then, we conduct a multi-objective evolutionary search with the overall number of floating point operations (FLOPs) as its efficiency constraint to find joint token and head pruning strategies that maximize accuracy and efficiency under various computational budgets. Empirical studies show that a large portion of tokens and attention heads could be pruned while achieving superior performance compared to the baseline BERT-based models and Length Adaptive Transformers in various downstream NLP tasks. MobileBERT trained with our joint token and head pruning scheme achieves a GLUE score of 83.0, which is 1.4 higher than Length Adaptive Transformer and 2.9 higher than the original model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Don\u2019t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling",
        "paper_url": "https://aclanthology.org/2022.coling-1.405/",
        "paper_authors": [
            "Dongsuk Oh",
            "Yejin Kim",
            "Hodong Lee",
            "H. Howie Huang",
            "Heuiseok Lim"
        ],
        "paper_abstract": "Recent pre-trained language models (PLMs) achieved great success on many natural language processing tasks through learning linguistic features and contextualized sentence representation. Since attributes captured in stacked layers of PLMs are not clearly identified, straightforward approaches such as embedding the last layer are commonly preferred to derive sentence representations from PLMs. This paper introduces the attention-based pooling strategy, which enables the model to preserve layer-wise signals captured in each layer and learn digested linguistic features for downstream tasks. The contrastive learning objective can adapt the layer-wise attention pooling to both unsupervised and supervised manners. It results in regularizing the anisotropic space of pre-trained embeddings and being more uniform. We evaluate our model on standard semantic textual similarity (STS) and semantic search tasks. As a result, our method improved the performance of the base contrastive learned BERTbase and variants.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SHAP-Based Explanation Methods: A Review for NLP Interpretability",
        "paper_url": "https://aclanthology.org/2022.coling-1.406/",
        "paper_authors": [
            "Edoardo Mosca",
            "Ferenc Szigeti",
            "Stella Tragianni",
            "Daniel Gallagher",
            "Georg Groh"
        ],
        "paper_abstract": "Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The SHapley Additive exPlanations (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature\u2014presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Simple Log-based Loss Function for Ordinal Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.407/",
        "paper_authors": [
            "Fran\u00e7ois Castagnos",
            "Martin Mihelich",
            "Charles Dognin"
        ],
        "paper_abstract": "The cross-entropy loss function is widely used and generally considered the default loss function for text classification. When it comes to ordinal text classification where there is an ordinal relationship between labels, the cross-entropy is not optimal as it does not incorporate the ordinal character into its feedback. In this paper, we propose a new simple loss function called ordinal log-loss (OLL). We show that this loss function outperforms state-of-the-art previously introduced losses on four benchmark text classification datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Ask Question First for Enhancing Lifelong Language Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.408/",
        "paper_authors": [
            "Han Wang",
            "Ruiliu Fu",
            "Xuejun Zhang",
            "Jun Zhou",
            "Qingwei Zhao"
        ],
        "paper_abstract": "Lifelong language learning aims to stream learning NLP tasks while retaining knowledge of previous tasks. Previous works based on the language model and following data-free constraint approaches have explored formatting all data as \u201cbegin token (B) + context (C) + question (Q) + answer (A)\u201d for different tasks. However, they still suffer from catastrophic forgetting and are exacerbated when the previous task\u2019s pseudo data is insufficient for the following reasons: (1) The model has difficulty generating task-corresponding pseudo data, and (2) A is prone to error when A and C are separated by Q because the information of the C is diminished before generating A. Therefore, we propose the Ask Question First and Replay Question (AQF-RQ), including a novel data format \u201cBQCA\u201d and a new training task to train pseudo questions of previous tasks. Experimental results demonstrate that AQF-RQ makes it easier for the model to generate more pseudo data that match corresponding tasks, and is more robust to both sufficient and insufficient pseudo-data when the task boundary is both clear and unclear. AQF-RQ can achieve only 0.36% lower performance than multi-task learning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DoubleMix: Simple Interpolation-Based Data Augmentation for Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.409/",
        "paper_authors": [
            "Hui Chen",
            "Wei Han",
            "Diyi Yang",
            "Soujanya Poria"
        ],
        "paper_abstract": "This paper proposes a simple yet effective interpolation-based data augmentation approach termed DoubleMix, to improve the robustness of models in text classification. DoubleMix first leverages a couple of simple augmentation operations to generate several perturbed samples for each training data, and then uses the perturbed data and original data to carry out a two-step interpolation in the hidden space of neural models. Concretely, it first mixes up the perturbed data to a synthetic sample and then mixes up the original data and the synthetic perturbed data. DoubleMix enhances models\u2019 robustness by learning the \u201cshifted\u201d features in hidden space. On six text classification benchmark datasets, our approach outperforms several popular text augmentation methods including token-level, sentence-level, and hidden-level data augmentation techniques. Also, experiments in low-resource settings show our approach consistently improves models\u2019 performance when the training data is scarce. Extensive ablation studies and case studies confirm that each component of our approach contributes to the final performance and show that our approach exhibits superior performance on challenging counterexamples. Additionally, visual analysis shows that text features generated by our approach are highly interpretable.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Large Sequence Representation Learning via Multi-Stage Latent Transformers",
        "paper_url": "https://aclanthology.org/2022.coling-1.410/",
        "paper_authors": [
            "Ionut-Catalin Sandu",
            "Daniel Voinea",
            "Alin-Ionut Popa"
        ],
        "paper_abstract": "We present LANTERN, a multi-stage transformer architecture for named-entity recognition (NER) designed to operate on indefinitely large text sequences (i.e. > 512 elements). For a given image of a form with structured text, our method uses language and spatial features to predict the entity tags of each text element. It breaks the quadratic computational constraints of the attention mechanism by operating over a learned latent space representation which encodes the input sequence via the cross-attention mechanism while having the multi-stage encoding component as a refinement over the NER predictions. As a proxy task, we propose RADAR, an LSTM classifier operating at character level, which predicts the relevance of a word with respect to the entity-recognition task. Additionally, we formulate a challenging novel NER use case, nutritional information extraction from food product labels. We created a dataset with 11,926 images depicting food product labels entitled TREAT dataset, with fully detailed annotations. Our method achieves superior performance against two competitive models designed for long sequences on the proposed TREAT dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MockingBERT: A Method for Retroactively Adding Resilience to NLP Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.411/",
        "paper_authors": [
            "Jan Jezabek",
            "Akash Singh"
        ],
        "paper_abstract": "Protecting NLP models against misspellings whether accidental or adversarial has been the object of research interest for the past few years. Existing remediations have typically either compromised accuracy or required full model re-training with each new class of attacks. We propose a novel method of retroactively adding resilience to misspellings to transformer-based NLP models. This robustness can be achieved without the need for re-training of the original NLP model and with only a minimal loss of language understanding performance on inputs without misspellings. Additionally we propose a new efficient approximate method of generating adversarial misspellings, which significantly reduces the cost needed to evaluate a model\u2019s resilience to adversarial attacks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Equivariant Transduction through Invariant Alignment",
        "paper_url": "https://aclanthology.org/2022.coling-1.412/",
        "paper_authors": [
            "Jennifer C. White",
            "Ryan Cotterell"
        ],
        "paper_abstract": "The ability to generalize compositionally is key to understanding the potentially infinite number of sentences that can be constructed in a human language from only a finite number of words. Investigating whether NLP models possess this ability has been a topic of interest: SCAN (Lake and Baroni, 2018) is one task specifically proposed to test for this property. Previous work has achieved impressive empirical results using a group-equivariant neural network that naturally encodes a useful inductive bias for SCAN (Gordon et al., 2020). Inspired by this, we introduce a novel group-equivariant architecture that incorporates a group-invariant hard alignment mechanism. We find that our network\u2019s structure allows it to develop stronger equivariance properties than existing group-equivariant approaches. We additionally find that it outperforms previous group-equivariant networks empirically on the SCAN task. Our results suggest that integrating group-equivariance into a variety of neural architectures is a potentially fruitful avenue of research, and demonstrate the value of careful analysis of the theoretical properties of such architectures.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers",
        "paper_url": "https://aclanthology.org/2022.coling-1.413/",
        "paper_authors": [
            "Jenny Kunz",
            "Marco Kuhlmann"
        ],
        "paper_abstract": "Probing studies have extensively explored where in neural language models linguistic information is located. The standard approach to interpreting the results of a probing classifier is to focus on the layers whose representations give the highest performance on the probing task. We propose an alternative method that asks where the task-relevant information emerges in the model. Our framework consists of a family of metrics that explicitly model local information gain relative to the previous layer and each layer\u2019s contribution to the model\u2019s overall performance. We apply the new metrics to two pairs of syntactic probing tasks with different degrees of complexity and find that the metrics confirm the expected ordering only for one of the pairs. Our local metrics show a massive dominance of the first layers, indicating that the features that contribute the most to our probing tasks are not as high-level as global metrics suggest.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting",
        "paper_url": "https://aclanthology.org/2022.coling-1.414/",
        "paper_authors": [
            "Jun Kong",
            "Jin Wang",
            "Liang-Chih Yu",
            "Xuejie Zhang"
        ],
        "paper_abstract": "Conditional computation algorithms, such as the early exiting (EE) algorithm, can be applied to accelerate the inference of pretrained language models (PLMs) while maintaining competitive performance on resource-constrained devices. However, this approach is only applied to the vertical architecture to decide which layers should be used for inference. Conversely, the operation of the horizontal perspective is ignored, and the determination of which tokens in each layer should participate in the computation fails, leading to a high redundancy for adaptive inference. To address this limitation, a unified horizontal and vertical multi-perspective early exiting (MPEE) framework is proposed in this study to accelerate the inference of transformer-based models. Specifically, the vertical architecture uses recycling EE classifier memory and weighted self-distillation to enhance the performance of the EE classifiers. Then, the horizontal perspective uses recycling class attention memory to emphasize the informative tokens. Conversely, the tokens with less information are truncated by weighted fusion and isolated from the following computation. Based on this, both horizontal and vertical EE are unified to obtain a better tradeoff between performance and efficiency. Extensive experimental results show that MPEE can achieve higher acceleration inference with competent performance than existing competitive methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Topology Imbalance and Relation Inauthenticity Aware Hierarchical Graph Attention Networks for Fake News Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.415/",
        "paper_authors": [
            "Li Gao",
            "Lingyun Song",
            "Jie Liu",
            "Bolin Chen",
            "Xuequn Shang"
        ],
        "paper_abstract": "Fake news detection is a challenging problem due to its tremendous real-world political and social impacts. Recent fake news detection works focus on learning news features from News Propagation Graph (NPG). However, little attention is paid to the issues of both authenticity of the relationships and topology imbalance in the structure of NPG, which trick existing methods and thus lead to incorrect prediction results. To tackle these issues, in this paper, we propose a novel Topology imbalance and Relation inauthenticity aware Hierarchical Graph Attention Networks (TR-HGAN) to identify fake news on social media. Specifically, we design a new topology imbalance smoothing strategy to measure the topology weight of each node. Besides, we adopt a hierarchical-level attention mechanism for graph convolutional learning, which can adaptively identify the authenticity of relationships by assigning appropriate weights to each of them. Experiments on real-world datasets demonstrate that TR-HGAN significantly outperforms state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Temporal Knowledge Graph Completion with Approximated Gaussian Process Embedding",
        "paper_url": "https://aclanthology.org/2022.coling-1.416/",
        "paper_authors": [
            "Linhai Zhang",
            "Deyu Zhou"
        ],
        "paper_abstract": "Knowledge Graphs (KGs) stores world knowledge that benefits various reasoning-based applications. Due to their incompleteness, a fundamental task for KGs, which is known as Knowledge Graph Completion (KGC), is to perform link prediction and infer new facts based on the known facts. Recently, link prediction on the temporal KGs becomes an active research topic. Numerous Temporal Knowledge Graph Completion (TKGC) methods have been proposed by mapping the entities and relations in TKG to the high-dimensional representations. However, most existing TKGC methods are mainly based on deterministic vector embeddings, which are not flexible and expressive enough. In this paper, we propose a novel TKGC method, TKGC-AGP, by mapping the entities and relations in TKG to the approximations of multivariate Gaussian processes (MGPs). Equipped with the flexibility and capacity of MGP, the global trends as well as the local fluctuations in the TKGs can be simultaneously modeled. Moreover, the temporal uncertainties can be also captured with the kernel function and the covariance matrix of MGP. Moreover, a first-order Markov assumption-based training algorithm is proposed to effective optimize the proposed method. Experimental results show the effectiveness of the proposed approach on two real-world benchmark datasets compared with some state-of-the-art TKGC methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CILDA: Contrastive Data Augmentation Using Intermediate Layer Knowledge Distillation",
        "paper_url": "https://aclanthology.org/2022.coling-1.417/",
        "paper_authors": [
            "Md Akmal Haidar",
            "Mehdi Rezagholizadeh",
            "Abbas Ghaddar",
            "Khalil Bibi",
            "Phillippe Langlais",
            "Pascal Poupart"
        ],
        "paper_abstract": "Knowledge distillation (KD) is an efficient framework for compressing large-scale pre-trained language models. Recent years have seen a surge of research aiming to improve KD by leveraging Contrastive Learning, Intermediate Layer Distillation, Data Augmentation, and Adversarial Training. In this work, we propose a learning-based data augmentation technique tailored for knowledge distillation, called CILDA. To the best of our knowledge, this is the first time that intermediate layer representations of the main task are used in improving the quality of augmented samples. More precisely, we introduce an augmentation technique for KD based on intermediate layer matching using contrastive loss to improve masked adversarial data augmentation. CILDA outperforms existing state-of-the-art KD approaches on the GLUE benchmark, as well as in an out-of-domain evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pro-KD: Progressive Distillation by Following the Footsteps of the Teacher",
        "paper_url": "https://aclanthology.org/2022.coling-1.418/",
        "paper_authors": [
            "Mehdi Rezagholizadeh",
            "Aref Jafari",
            "Puneeth S.M. Saladi",
            "Pranav Sharma",
            "Ali Saheb Pasand",
            "Ali Ghodsi"
        ],
        "paper_abstract": "With the ever growing scale of neural models, knowledge distillation (KD) attracts more attention as a prominent tool for neural model compression. However, there are counter intuitive observations in the literature showing some challenging limitations of KD. A case in point is that the best performing checkpoint of the teacher might not necessarily be the best teacher for training the student in KD. Therefore, one important question would be how to find the best checkpoint of the teacher for distillation? Searching through the checkpoints of the teacher would be a very tedious and computationally expensive process, which we refer to as the checkpoint-search problem. Moreover, another observation is that larger teachers might not necessarily be better teachers in KD, which is referred to as the capacity-gap problem. To address these challenging problems, in this work, we introduce our progressive knowledge distillation (Pro-KD) technique which defines a smoother training path for the student by following the training footprints of the teacher instead of solely relying on distilling from a single mature fully-trained teacher. We demonstrate that our technique is quite effective in mitigating the capacity-gap problem and the checkpoint search problem. We evaluate our technique using a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and CIFAR-100), natural language understanding tasks of the GLUE benchmark, and question answering (SQuAD 1.1 and 2.0) using BERT-based models and consistently got superior results over state-of-the-art techniques.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Classical Sequence Match Is a Competitive Few-Shot One-Class Learner",
        "paper_url": "https://aclanthology.org/2022.coling-1.419/",
        "paper_authors": [
            "Mengting Hu",
            "Hang Gao",
            "Yinhao Bai",
            "Mingming Liu"
        ],
        "paper_abstract": "Nowadays, transformer-based models gradually become the default choice for artificial intelligence pioneers. The models also show superiority even in the few-shot scenarios. In this paper, we revisit the classical methods and propose a new few-shot alternative. Specifically, we investigate the few-shot one-class problem, which actually takes a known sample as a reference to detect whether an unknown instance belongs to the same class. This problem can be studied from the perspective of sequence match. It is shown that with meta-learning, the classical sequence match method, i.e. Compare-Aggregate, significantly outperforms transformer ones. The classical approach requires much less training cost. Furthermore, we perform an empirical comparison between two kinds of sequence match approaches under simple fine-tuning and meta-learning. Meta-learning causes the transformer models\u2019 features to have high-correlation dimensions. The reason is closely related to the number of layers and heads of transformer models. Experimental codes and data are available at https://github.com/hmt2014/FewOne.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Domain Adaptation for Text Classification via Meta Self-Paced Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.420/",
        "paper_authors": [
            "Nghia Ngo Trung",
            "Linh Ngo Van",
            "Thien Huu Nguyen"
        ],
        "paper_abstract": "A shift in data distribution can have a significant impact on performance of a text classification model. Recent methods addressing unsupervised domain adaptation for textual tasks typically extracted domain-invariant representations through balancing between multiple objectives to align feature spaces between source and target domains. While effective, these methods induce various new domain-sensitive hyperparameters, thus are impractical as large-scale language models are drastically growing bigger to achieve optimal performance. To this end, we propose to leverage meta-learning framework to train a neural network-based self-paced learning procedure in an end-to-end manner. Our method, called Meta Self-Paced Domain Adaption (MSP-DA), follows a novel but intuitive domain-shift variation of cluster assumption to derive the meta train-test dataset split based on the self-pacing difficulties of source domain\u2019s examples. As a result, MSP-DA effectively leverages self-training and self-tuning domain-specific hyperparameters simultaneously throughout the learning process. Extensive experiments demonstrate our framework substantially improves performance on target domains, surpassing state-of-the-art approaches. Detailed analyses validate our method and provide insight into how each domain affects the learned hyperparameters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "WARM: A Weakly (+Semi) Supervised Math Word Problem Solver",
        "paper_url": "https://aclanthology.org/2022.coling-1.421/",
        "paper_authors": [
            "Oishik Chatterjee",
            "Isha Pandey",
            "Aashish Waikar",
            "Vishwajeet Kumar",
            "Ganesh Ramakrishnan"
        ],
        "paper_abstract": "Solving math word problems (MWPs) is an important and challenging problem in natural language processing. Existing approaches to solving MWPs require full supervision in the form of intermediate equations. However, labeling every MWP with its corresponding equations is a time-consuming and expensive task. In order to address this challenge of equation annotation, we propose a weakly supervised model for solving MWPs by requiring only the final answer as supervision. We approach this problem by first learning to generate the equation using the problem description and the final answer, which we subsequently use to train a supervised MWP solver. We propose and compare various weakly supervised techniques to learn to generate equations directly from the problem description and answer. Through extensive experiments, we demonstrate that without using equations for supervision, our approach achieves accuracy gains of 4.5% and 32% over the current state-of-the-art weakly-supervised approach, on the standard Math23K and AllArith datasets respectively. Additionally, we curate and release new datasets of roughly 10k MWPs each in English and in Hindi (a low-resource language). These datasets are suitable for training weakly supervised models. We also present an extension of our model to semi-supervised learning and present further improvements on results, along with insights.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Attention Networks for Augmenting Clinical Text with Support Sets for Diagnosis Prediction",
        "paper_url": "https://aclanthology.org/2022.coling-1.422/",
        "paper_authors": [
            "Paul Grundmann",
            "Tom Oberhauser",
            "Felix Gers",
            "Alexander L\u00f6ser"
        ],
        "paper_abstract": "Diagnosis prediction on admission notes is a core clinical task. However, these notes may incompletely describe the patient. Also, clinical language models may suffer from idiosyncratic language or imbalanced vocabulary for describing diseases or symptoms. We tackle the task of diagnosis prediction, which consists of predicting future patient diagnoses from clinical texts at the time of admission. We improve the performance on this task by introducing an additional signal from support sets of diagnostic codes from prior admissions or as they emerge during differential diagnosis. To enhance the robustness of diagnosis prediction methods, we propose to augment clinical text with potentially complementary set data from diagnosis codes from previous patient visits or from codes that emerge from the current admission as they become available through diagnostics. We discuss novel attention network architectures and augmentation strategies to solve this problem. Our experiments reveal that support sets improve the performance drastically to predict less common diagnosis codes. Our approach clearly outperforms the previous state-of-the-art PubMedBERT baseline by up 3% points. Furthermore, we find that support sets drastically improve the performance for pregnancy- and gynecology-related diagnoses up to 32.9% points compared to the baseline.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PARSE: An Efficient Search Method for Black-box Adversarial Text Attacks",
        "paper_url": "https://aclanthology.org/2022.coling-1.423/",
        "paper_authors": [
            "Pengwei Zhan",
            "Chao Zheng",
            "Jing Yang",
            "Yuxiang Wang",
            "Liming Wang",
            "Yang Wu",
            "Yunjian Zhang"
        ],
        "paper_abstract": "Neural networks are vulnerable to adversarial examples. The adversary can successfully attack a model even without knowing model architecture and parameters, i.e., under a black-box scenario. Previous works on word-level attacks widely use word importance ranking (WIR) methods and complex search methods, including greedy search and heuristic algorithms, to find optimal substitutions. However, these methods fail to balance the attack success rate and the cost of attacks, such as the number of queries to the model and the time consumption. In this paper, We propose PAthological woRd Saliency sEarch (PARSE) that performs the search under dynamic search space following the subarea importance. Experiments show that PARSE can achieve comparable attack success rates to complex search methods while saving numerous queries and time, e.g., saving at most 74% of queries and 90% of time compared with greedy search when attacking the examples from Yelp dataset. The adversarial examples crafted by PARSE are also of high quality, highly transferable, and can effectively improve model robustness in adversarial training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Closer Look at Parameter Contributions When Training Neural Language and Translation Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.424/",
        "paper_authors": [
            "Ra\u00fal V\u00e1zquez",
            "Hande Celikkanat",
            "Vinit Ravishankar",
            "Mathias Creutz",
            "J\u00f6rg Tiedemann"
        ],
        "paper_abstract": "We analyze the learning dynamics of neural language and translation models using Loss Change Allocation (LCA), an indicator that enables a fine-grained analysis of parameter updates when optimizing for the loss function. In other words, we can observe the contributions of different network components at training time. In this article, we systematically study masked language modeling, causal language modeling, and machine translation. We show that the choice of training objective leads to distinctive optimization procedures, even when performed on comparable Transformer architectures. We demonstrate how the various Transformer parameters are used during training, supporting that the feed-forward components of each layer are the main contributors to the optimization procedure. Finally, we find that the learning dynamics are not affected by data size and distribution but rather determined by the learning objective.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "KNOT: Knowledge Distillation Using Optimal Transport for Solving NLP Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.425/",
        "paper_authors": [
            "Rishabh Bhardwaj",
            "Tushar Vaidya",
            "Soujanya Poria"
        ],
        "paper_abstract": "We propose a new approach, Knowledge Distillation using Optimal Transport (KNOT), to distill the natural language semantic knowledge from multiple teacher networks to a student network. KNOT aims to train a (global) student model by learning to minimize the optimal transport cost of its assigned probability distribution over the labels to the weighted sum of probabilities predicted by the (local) teacher models, under the constraints that the student model does not have access to teacher models\u2019 parameters or training data. To evaluate the quality of knowledge transfer, we introduce a new metric, Semantic Distance (SD), that measures semantic closeness between the predicted and ground truth label distributions. The proposed method shows improvements in the global model\u2019s SD performance over the baseline across three NLP tasks while performing on par with Entropy-based distillation on standard accuracy and F1 metrics. The implementation pertaining to this work is publicly available at https://github.com/declare-lab/KNOT.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Information Minimization Based Contrastive Learning Model for Unsupervised Sentence Embeddings Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.426/",
        "paper_authors": [
            "Shaobin Chen",
            "Jie Zhou",
            "Yuling Sun",
            "Liang He"
        ],
        "paper_abstract": "Unsupervised sentence embeddings learning has been recently dominated by contrastive learning methods (e.g., SimCSE), which keep positive pairs similar and push negative pairs apart. The contrast operation aims to keep as much information as possible by maximizing the mutual information between positive instances, which leads to redundant information in sentence embedding. To address this problem, we present an information minimization based contrastive learning InforMin-CL model to retain the useful information and discard the redundant information by maximizing the mutual information and minimizing the information entropy between positive instances meanwhile for unsupervised sentence representation learning. Specifically, we find that information minimization can be achieved by simple contrast and reconstruction objectives. The reconstruction operation reconstitutes the positive instance via the other positive instance to minimize the information entropy between positive instances. We evaluate our model on fourteen downstream tasks, including both supervised and unsupervised (semantic textual similarity) tasks. Extensive experimental results show that our InforMin-CL obtains a state-of-the-art performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learn2Weight: Parameter Adaptation against Similar-domain Adversarial Attacks",
        "paper_url": "https://aclanthology.org/2022.coling-1.427/",
        "paper_authors": [
            "Siddhartha Datta"
        ],
        "paper_abstract": "Recent work in black-box adversarial attacks for NLP systems has attracted attention. Prior black-box attacks assume that attackers can observe output labels from target models based on selected inputs. In this work, inspired by adversarial transferability, we propose a new type of black-box NLP adversarial attack that an attacker can choose a similar domain and transfer the adversarial examples to the target domain and cause poor performance in target model. Based on domain adaptation theory, we then propose a defensive strategy, called Learn2Weight, which trains to predict the weight adjustments for target model in order to defense the attack of similar-domain adversarial examples. Using Amazon multi-domain sentiment classification dataset, we empirically show that Learn2Weight model is effective against the attack compared to standard black-box defense methods such as adversarial training and defense distillation. This work contributes to the growing literature on machine learning safety.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Sentence-aware Adversarial Meta-Learning for Few-Shot Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.428/",
        "paper_authors": [
            "Suhe Wang",
            "Xiaoyuan Liu",
            "Bo Liu",
            "Diwen Dong"
        ],
        "paper_abstract": "Meta-learning has emerged as an effective approach for few-shot text classification. However, current studies fail to realize the importance of the semantic interaction between sentence features and neglect to enhance the generalization ability of the model to new tasks. In this paper, we integrate an adversarial network architecture into the meta-learning system and leverage cost-effective modules to build a novel few-shot classification framework named SaAML. Significantly, our approach can exploit the temporal convolutional network to encourage more discriminative representation learning and explore the attention mechanism to promote more comprehensive feature expression, thus resulting in better adaptation for new classes. Through a series of experiments on four benchmark datasets, we demonstrate that our new framework acquires considerable superiority over state-of-the-art methods in all datasets, increasing the performance of 1-shot classification and 5-shot classification by 7.15% and 2.89%, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Reweighting Strategy Based on Synthetic Data Identification for Sentence Similarity",
        "paper_url": "https://aclanthology.org/2022.coling-1.429/",
        "paper_authors": [
            "TaeHee Kim",
            "ChaeHun Park",
            "Jimin Hong",
            "Radhika Dua",
            "Edward Choi",
            "Jaegul Choo"
        ],
        "paper_abstract": "Semantically meaningful sentence embeddings are important for numerous tasks in natural language processing. To obtain such embeddings, recent studies explored the idea of utilizing synthetically generated data from pretrained language models(PLMs) as a training corpus. However, PLMs often generate sentences different from the ones written by human. We hypothesize that treating all these synthetic examples equally for training can have an adverse effect on learning semantically meaningful embeddings. To analyze this, we first train a classifier that identifies machine-written sentences and observe that the linguistic features of the sentences identified as written by a machine are significantly different from those of human-written sentences. Based on this, we propose a novel approach that first trains the classifier to measure the importance of each sentence. The distilled information from the classifier is then used to train a reliable sentence embedding model. Through extensive evaluation on four real-world datasets, we demonstrate that our model trained on synthetic data generalizes well and outperforms the baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MaxMatch-Dropout: Subword Regularization for WordPiece",
        "paper_url": "https://aclanthology.org/2022.coling-1.430/",
        "paper_authors": [
            "Tatsuya Hiraoka"
        ],
        "paper_abstract": "We present a subword regularization method for WordPiece, which uses a maximum matching algorithm for tokenization. The proposed method, MaxMatch-Dropout, randomly drops words in a search using the maximum matching algorithm. It realizes finetuning with subword regularization for popular pretrained language models such as BERT-base. The experimental results demonstrate that MaxMatch-Dropout improves the performance of text classification and machine translation tasks as well as other subword regularization methods. Moreover, we provide a comparative analysis of subword regularization methods: subword regularization with SentencePiece (Unigram), BPE-Dropout, and MaxMatch-Dropout.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.431/",
        "paper_authors": [
            "Tianyi Lei",
            "Honghui Hu",
            "Qiaoyang Luo",
            "Dezhong Peng",
            "Xu Wang"
        ],
        "paper_abstract": "Few-shot text classification aims to classify the text under the few-shot scenario. Most of the previous methods adopt optimization-based meta learning to obtain task distribution. However, due to the neglect of matching between the few amount of samples and complicated models, as well as the distinction between useful and useless task features, these methods suffer from the overfitting issue. To address this issue, we propose a novel Adaptive Meta-learner via Gradient Similarity (AMGS) method to improve the model generalization ability to a new task. Specifically, the proposed AMGS alleviates the overfitting based on two aspects: (i) acquiring the potential semantic representation of samples and improving model generalization through the self-supervised auxiliary task in the inner loop, (ii) leveraging the adaptive meta-learner via gradient similarity to add constraints on the gradient obtained by base-learner in the outer loop. Moreover, we make a systematic analysis of the influence of regularization on the entire framework. Experimental results on several benchmarks demonstrate that the proposed AMGS consistently improves few-shot text classification performance compared with the state-of-the-art optimization-based meta-learning approaches. The code is available at: https://github.com/Tianyi-Lei.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Vocabulary-informed Language Encoding",
        "paper_url": "https://aclanthology.org/2022.coling-1.432/",
        "paper_authors": [
            "Xi Ai",
            "Bin Fang"
        ],
        "paper_abstract": "A Multilingual model relies on language encodings to identify input languages because the multilingual model has to distinguish between the input and output languages or among all the languages for cross-lingual tasks. Furthermore, we find that language encodings potentially refine multiple morphologies of different languages to form a better isomorphic space for multilinguality. To leverage this observation, we present a method to compute a vocabulary-informed language encoding as the language representation, for a required language, considering a local vocabulary covering an acceptable amount of the most frequent word embeddings in this language. In our experiments, our method can consistently improve the performance of multilingual models on unsupervised neural machine translation and cross-lingual embedding.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "OpticE: A Coherence Theory-Based Model for Link Prediction",
        "paper_url": "https://aclanthology.org/2022.coling-1.433/",
        "paper_authors": [
            "Xiangyu Gui",
            "Feng Zhao",
            "Langjunqing Jin",
            "Hai Jin"
        ],
        "paper_abstract": "Knowledge representation learning is a key step required for link prediction tasks with knowledge graphs (KGs). During the learning process, the semantics of each entity are embedded by a vector or a point in a feature space. The distance between these points is a measure of semantic similarity. However, in a KG, while two entities may have similar semantics in some relations, they have different semantics in others. It is ambiguous to assign a fixed distance to depict the variant semantic similarity of entities. To alleviate the semantic ambiguity in KGs, we design a new embedding approach named OpticE, which is derived from the well-known physical phenomenon of optical interference. It is a lightweight and relation-adaptive model based on coherence theory, in which each entity\u2019s semantics vary automatically regarding different relations. In addition, a unique negative sampling method is proposed to combine the multimapping properties and self-adversarial learning during the training process. The experimental results obtained on practical KG benchmarks show that the OpticE model, with elegant structures, can compete with existing link prediction methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Smoothed Contrastive Learning for Unsupervised Sentence Embedding",
        "paper_url": "https://aclanthology.org/2022.coling-1.434/",
        "paper_authors": [
            "Xing Wu",
            "Chaochen Gao",
            "Yipeng Su",
            "Jizhong Han",
            "Zhongyuan Wang",
            "Songlin Hu"
        ],
        "paper_abstract": "Unsupervised contrastive sentence embedding models, e.g., unsupervised SimCSE, use the InfoNCE loss function in training. Theoretically, we expect to use larger batches to get more adequate comparisons among samples and avoid overfitting. However, increasing batch size leads to performance degradation when it exceeds a threshold, which is probably due to the introduction of false-negative pairs through statistical observation. To alleviate this problem, we introduce a simple smoothing strategy upon the InfoNCE loss function, termed Gaussian Smoothed InfoNCE (GS-InfoNCE). In other words, we add random Gaussian noise as an extension to the negative pairs without increasing the batch size. Through experiments on the semantic text similarity tasks, though simple, the proposed smoothing strategy brings improvements to unsupervised SimCSE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Knowledge Distillation with Reptile Meta-Learning for Pretrained Language Model Compression",
        "paper_url": "https://aclanthology.org/2022.coling-1.435/",
        "paper_authors": [
            "Xinge Ma",
            "Jin Wang",
            "Liang-Chih Yu",
            "Xuejie Zhang"
        ],
        "paper_abstract": "The billions, and sometimes even trillions, of parameters involved in pre-trained language models significantly hamper their deployment in resource-constrained devices and real-time applications. Knowledge distillation (KD) can transfer knowledge from the original model (i.e., teacher) into a compact model (i.e., student) to achieve model compression. However, previous KD methods have usually frozen the teacher and applied its immutable output feature maps as soft labels to guide the student\u2019s training. Moreover, the goal of the teacher is to achieve the best performance on downstream tasks rather than knowledge transfer. Such a fixed architecture may limit the teacher\u2019s teaching and student\u2019s learning abilities. Herein, a knowledge distillation method with reptile meta-learning is proposed to facilitate the transfer of knowledge from the teacher to the student. The teacher can continuously meta-learn the student\u2019s learning objective to adjust its parameters for maximizing the student\u2019s performance throughout the distillation process. In this way, the teacher learns to teach, produces more suitable soft labels, and transfers more appropriate knowledge to the student, resulting in improved performance. Unlike previous KD using meta-learning, the proposed method only needs to calculate the first-order derivatives to update the teacher, leading to lower computational cost but better convergence. Extensive experiments on the GLUE benchmark show the competitive performance achieved by the proposed method. For reproducibility, the code for this paper is available at: https://github.com/maxinge8698/ReptileDistil.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space",
        "paper_url": "https://aclanthology.org/2022.coling-1.436/",
        "paper_authors": [
            "Yao Dong",
            "Lei Wang",
            "Ji Xiang",
            "Xiaobo Guo",
            "Yuqiang Xie"
        ],
        "paper_abstract": "Knowledge graph embedding, which aims to learn representations of entities and relations in knowledge graphs, finds applications in various downstream tasks. The key to success of knowledge graph embedding models are the ability to model relation patterns including symmetry/antisymmetry, inversion, commutative composition and non-commutative composition. Although existing methods fail in modeling the non-commutative composition patterns, several approaches support this pattern by modeling beyond Euclidean space and complex space. Nevertheless, expanding to complicated spaces such as quaternion can easily lead to a substantial increase in the amount of parameters, which greatly reduces the computational efficiency. In this paper, we propose a new knowledge graph embedding method called RotateCT, which first transforms the coordinates of each entity, and then represents each relation as a rotation from head entity to tail entity in complex space. By design, RotateCT can infer the non-commutative composition patterns and improve the computational efficiency. Experiments on multiple datasets empirically show that RotateCT outperforms most state-of-the-art methods on link prediction and path query answering.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Can Data Diversity Enhance Learning Generalization?",
        "paper_url": "https://aclanthology.org/2022.coling-1.437/",
        "paper_authors": [
            "Yu Yu",
            "Shahram Khadivi",
            "Jia Xu"
        ],
        "paper_abstract": "This paper introduces our Diversity Advanced Actor-Critic reinforcement learning (A2C) framework (DAAC) to improve the generalization and accuracy of Natural Language Processing (NLP). We show that the diversification of training samples alleviates overfitting and improves model generalization and accuracy. We quantify diversity on a set of samples using the max dispersion, convex hull volume, and graph entropy based on sentence embeddings in high-dimensional metric space. We also introduce A2C to select such a diversified training subset efficiently. Our experiments achieve up to +23.8 accuracy increase (38.0% relatively) in sentiment analysis, -44.7 perplexity decrease (37.9% relatively) in language modeling, and consistent improvements in named entity recognition over various domains. In particular, our method outperforms both domain adaptation and generalization baselines without using any target domain knowledge.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.438/",
        "paper_authors": [
            "Yury Zemlyanskiy",
            "Michiel de Jong",
            "Joshua Ainslie",
            "Panupong Pasupat",
            "Peter Shaw",
            "Linlu Qiu",
            "Sumit Sanghai",
            "Fei Sha"
        ],
        "paper_abstract": "A common recent approach to semantic parsing augments sequence-to-sequence models by retrieving and appending a set of training samples, called exemplars. The effectiveness of this recipe is limited by the ability to retrieve informative exemplars that help produce the correct parse, which is especially challenging in low-resource settings. Existing retrieval is commonly based on similarity of query and exemplar inputs. We propose GandR, a retrieval procedure that retrieves exemplars for which outputs are also similar. GandR first generates a preliminary prediction with input-based retrieval. Then, it retrieves exemplars with outputs similar to the preliminary prediction which are used to generate a final prediction. GandR sets the state of the art on multiple low-resource semantic parsing tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Coarse-to-Fine: Hierarchical Multi-task Learning for Natural Language Understanding",
        "paper_url": "https://aclanthology.org/2022.coling-1.439/",
        "paper_authors": [
            "Zhaoye Fei",
            "Yu Tian",
            "Yongkang Wu",
            "Xinyu Zhang",
            "Yutao Zhu",
            "Zheng Liu",
            "Jiawen Wu",
            "Dejiang Kong",
            "Ruofei Lai",
            "Zhao Cao",
            "Zhicheng Dou",
            "Xipeng Qiu"
        ],
        "paper_abstract": "Generalized text representations are the foundation of many natural language understanding tasks. To fully utilize the different corpus, it is inevitable that models need to understand the relevance among them. However, many methods ignore the relevance and adopt a single-channel model (a coarse paradigm) directly for all tasks, which lacks enough rationality and interpretation. In addition, some existing works learn downstream tasks by stitches skill block (a fine paradigm), which might cause irrational results due to its redundancy and noise. In this work, we first analyze the task correlation through three different perspectives, , data property, manual design, and model-based relevance, based on which the similar tasks are grouped together. Then, we propose a hierarchical framework with a coarse-to-fine paradigm, with the bottom level shared to all the tasks, the mid-level divided to different groups, and the top-level assigned to each of the tasks. This allows our model to learn basic language properties from all tasks, boost performance on relevant tasks, and reduce the negative impact from irrelevant tasks. Our experiments on 13 benchmark datasets across five natural language understanding tasks demonstrate the superiority of our method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automatic Label Sequence Generation for Prompting Sequence-to-sequence Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.440/",
        "paper_authors": [
            "Zichun Yu",
            "Tianyu Gao",
            "Zhengyan Zhang",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "paper_abstract": "Prompting, which casts downstream applications as language modeling tasks, has shown to be sample efficient compared to standard fine-tuning with pre-trained models. However, one pitfall of prompting is the need of manually-designed patterns, whose outcome can be unintuitive and requires large validation sets to tune. To tackle the challenge, we propose AutoSeq, a fully automatic prompting method: (1) We adopt natural language prompts on sequence-to-sequence models, enabling free-form generation and larger label search space; (2) We propose label sequences \u2013 phrases with indefinite lengths to verbalize the labels \u2013 which eliminate the need of manual templates and are more expressive than single label words; (3) We use beam search to automatically generate a large amount of label sequence candidates and propose contrastive re-ranking to get the best combinations. AutoSeq significantly outperforms other no-manual-design methods, such as soft prompt tuning, adapter tuning, and automatic search on single label words; the generated label sequences are even better than curated manual ones on a variety of tasks. Our method reveals the potential of sequence-to-sequence models in few-shot learning and sheds light on a path to generic and automatic prompting. The source code of this paper can be obtained from https://github.com/thunlp/Seq2Seq-Prompt.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics",
        "paper_url": "https://aclanthology.org/2022.coling-1.441/",
        "paper_authors": [
            "Zihao Wang",
            "Jiaheng Dou",
            "Yong Zhang"
        ],
        "paper_abstract": "Measuring Sentence Textual Similarity (STS) is a classic task that can be applied to many downstream NLP applications such as text generation and retrieval. In this paper, we focus on unsupervised STS that works on various domains but only requires minimal data and computational resources. Theoretically, we propose a light-weighted Expectation-Correction (EC) formulation for STS computation. EC formulation unifies unsupervised STS approaches including the cosine similarity of Additively Composed (AC) sentence embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the compositional phrase semantics by composing multiple recursive EC formulations. ROTS finishes in linear time and is faster than its predecessors. ROTS is empirically more effective and scalable than previous approaches. Extensive experiments on 29 STS tasks under various settings show the clear advantage of ROTS over existing approaches. Detailed ablation studies prove the effectiveness of our approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Generalized Method for Automated Multilingual Loanword Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.442/",
        "paper_authors": [
            "Abhijnan Nath",
            "Sina Mahdipour Saravani",
            "Ibrahim Khebour",
            "Sheikh Mannan",
            "Zihui Li",
            "Nikhil Krishnaswamy"
        ],
        "paper_abstract": "Loanwords are words incorporated from one language into another without translation. Suppose two words from distantly-related or unrelated languages sound similar and have a similar meaning. In that case, this is evidence of likely borrowing. This paper presents a method to automatically detect loanwords across various language pairs, accounting for differences in script, pronunciation and phonetic transformation by the borrowing language. We incorporate edit distance, semantic similarity measures, and phonetic alignment. We evaluate on 12 language pairs and achieve performance comparable to or exceeding state of the art methods on single-pair loanword detection tasks. We also demonstrate that multilingual models perform the same or often better than models trained on single language pairs and can potentially generalize to unseen language pairs with sufficient data, and that our method can exceed human performance on loanword detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "FeatureBART: Feature Based Sequence-to-Sequence Pre-Training for Low-Resource NMT",
        "paper_url": "https://aclanthology.org/2022.coling-1.443/",
        "paper_authors": [
            "Abhisek Chakrabarty",
            "Raj Dabre",
            "Chenchen Ding",
            "Hideki Tanaka",
            "Masao Utiyama",
            "Eiichiro Sumita"
        ],
        "paper_abstract": "In this paper we present FeatureBART, a linguistically motivated sequence-to-sequence monolingual pre-training strategy in which syntactic features such as lemma, part-of-speech and dependency labels are incorporated into the span prediction based pre-training framework (BART). These automatically extracted features are incorporated via approaches such as concatenation and relevance mechanisms, among which the latter is known to be better than the former. When used for low-resource NMT as a downstream task, we show that these feature based models give large improvements in bilingual settings and modest ones in multilingual settings over their counterparts that do not use features.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-level Community-awareness Graph Neural Networks for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.444/",
        "paper_authors": [
            "Binh Nguyen",
            "Long Nguyen",
            "Dien Dinh"
        ],
        "paper_abstract": "Neural Machine Translation (NMT) aims to translate the source- to the target-language while preserving the original meaning. Linguistic information such as morphology, syntactic, and semantics shall be grasped in token embeddings to produce a high-quality translation. Recent works have leveraged the powerful Graph Neural Networks (GNNs) to encode such language knowledge into token embeddings. Specifically, they use a trained parser to construct semantic graphs given sentences and then apply GNNs. However, most semantic graphs are tree-shaped and too sparse for GNNs which cause the over-smoothing problem. To alleviate this problem, we propose a novel Multi-level Community-awareness Graph Neural Network (MC-GNN) layer to jointly model local and global relationships between words and their linguistic roles in multiple communities. Intuitively, the MC-GNN layer substitutes a self-attention layer at the encoder side of a transformer-based machine translation model. Extensive experiments on four language-pair datasets with common evaluation metrics show the remarkable improvements of our method while reducing the time complexity in very long sentences.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On the Complementarity between Pre-Training and Random-Initialization for Resource-Rich Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.445/",
        "paper_authors": [
            "Changtong Zan",
            "Liang Ding",
            "Li Shen",
            "Yu Cao",
            "Weifeng Liu",
            "Dacheng Tao"
        ],
        "paper_abstract": "Pre-Training (PT) of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains (some- times, even worse) on resource-rich NMT on par with its Random-Initialization (RI) counterpart. We take the first step to investigate the complementarity between PT and RI in resource-rich scenarios via two probing analyses, and find that: 1) PT improves NOT the accuracy, but the generalization by achieving flatter loss landscapes than that of RI; 2) PT improves NOT the confidence of lexical choice, but the negative diversity by assigning smoother lexical probability distributions than that of RI. Based on these insights, we propose to combine their complementarities with a model fusion algorithm that utilizes optimal transport to align neurons between PT and RI. Experiments on two resource-rich translation benchmarks, WMT\u201917 English-Chinese (20M) and WMT\u201919 English-German (36M), show that PT and RI could be nicely complementary to each other, achieving substantial improvements considering both translation accuracy, generalization, and negative diversity. Probing tools and code are released at: https://github.com/zanchangtong/PTvsRI.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.446/",
        "paper_authors": [
            "Cunxiao Du",
            "Zhaopeng Tu",
            "Longyue Wang",
            "Jing Jiang"
        ],
        "paper_abstract": "Recently, a new training oaxe loss has proven effective to ameliorate the effect of multimodality for non-autoregressive translation (NAT), which removes the penalty of word order errors in the standard cross-entropy loss. Starting from the intuition that reordering generally occurs between phrases, we extend oaxe by only allowing reordering between ngram phrases and still requiring a strict match of word order within the phrases. Extensive experiments on NAT benchmarks across language pairs and data scales demonstrate the effectiveness and universality of our approach. Further analyses show that ngram noaxe indeed improves the translation of ngram phrases, and produces more fluent translation with a better modeling of sentence structure.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Language Branch Gated Multilingual Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.447/",
        "paper_authors": [
            "Haoran Sun",
            "Deyi Xiong"
        ],
        "paper_abstract": "Knowledge transfer across languages is crucial for multilingual neural machine translation. In this paper, we propose language branch (LB) gated multilingual neural machine translation that encourages knowledge transfer within the same language branch with a LB-gated module that is integrated into both the encoder and decoder. The LB-gated module distinguishes LB-specific parameters from global parameters shared by all languages and routes languages from the same LB to the corresponding LB-specific network. Comprehensive experiments on the OPUS-100 dataset show that the proposed approach substantially improves translation quality on both middle- and low-resource languages over previous methods. Further analysis demonstrates its ability in learning similarities between language branches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Iterative Constrained Back-Translation for Unsupervised Domain Adaptation of Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.448/",
        "paper_authors": [
            "Hongxiao Zhang",
            "Hui Huang",
            "Jiale Gao",
            "Yufeng Chen",
            "Jinan Xu",
            "Jian Liu"
        ],
        "paper_abstract": "Back-translation has been proven to be effective in unsupervised domain adaptation of neural machine translation (NMT). However, the existing back-translation methods mainly improve domain adaptability by generating in-domain pseudo-parallel data that contains sentence-structural knowledge, paying less attention to the in-domain lexical knowledge, which may lead to poor translation of unseen in-domain words. In this paper, we propose an Iterative Constrained Back-Translation (ICBT) method to incorporate in-domain lexical knowledge on the basis of BT for unsupervised domain adaptation of NMT. Specifically, we apply lexical constraints into back-translation to generate pseudo-parallel data with in-domain lexical knowledge, and then perform round-trip iterations to incorporate more lexical knowledge. Based on this, we further explore sampling strategies of constrained words in ICBT to introduce more targeted lexical knowledge, via domain specificity and confidence estimation. Experimental results on four domains show that our approach achieves state-of-the-art results, improving the BLEU score by up to 3.08 compared to the strongest baseline, which demonstrates the effectiveness of our approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Linguistically-Motivated Yor\u00f9b\u00e1-English Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.449/",
        "paper_authors": [
            "Ife Adebara",
            "Muhammad Abdul-Mageed",
            "Miikka Silfverberg"
        ],
        "paper_abstract": "Translating between languages where certain features are marked morphologically in one but absent or marked contextually in the other is an important test case for machine translation. When translating into English which marks (in)definiteness morphologically, from Yor\u00f9b\u00e1 which uses bare nouns but marks these features contextually, ambiguities arise. In this work, we perform fine-grained analysis on how an SMT system compares with two NMT systems (BiLSTM and Transformer) when translating bare nouns in Yor\u00f9b\u00e1 into English. We investigate how the systems what extent they identify BNs, correctly translate them, and compare with human translation patterns. We also analyze the type of errors each model makes and provide a linguistic description of these errors. We glean insights for evaluating model performance in low-resource settings. In translating bare nouns, our results show the transformer model outperforms the SMT and BiLSTM models for 4 categories, the BiLSTM outperforms the SMT model for 3 categories while the SMT outperforms the NMT models for 1 category.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dynamic Position Encoding for Transformers",
        "paper_url": "https://aclanthology.org/2022.coling-1.450/",
        "paper_authors": [
            "Joyce Zheng",
            "Mehdi Rezagholizadeh",
            "Peyman Passban"
        ],
        "paper_abstract": "Recurrent models have been dominating the field of neural machine translation (NMT) for the past few years. Transformers have radically changed it by proposing a novel architecture that relies on a feed-forward backbone and self-attention mechanism. Although Transformers are powerful, they could fail to properly encode sequential/positional information due to their non-recurrent nature. To solve this problem, position embeddings are defined exclusively for each time step to enrich word information. However, such embeddings are fixed after training regardless of the task and word ordering system of the source and target languages. In this paper, we address this shortcoming by proposing a novel architecture with new position embeddings that take the order of the target words into consideration. Instead of using predefined position embeddings, our solution generates new embeddings to refine each word\u2019s position information. Since we do not dictate the position of the source tokens and we learn them in an end-to-end fashion, we refer to our method as dynamic position encoding (DPE). We evaluated the impact of our model on multiple datasets to translate from English to German, French, and Italian and observed meaningful improvements in comparison to the original Transformer.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.451/",
        "paper_authors": [
            "Juncheng Wan",
            "Jian Yang",
            "Shuming Ma",
            "Dongdong Zhang",
            "Weinan Zhang",
            "Yong Yu",
            "Zhoujun Li"
        ],
        "paper_abstract": "While end-to-end neural machine translation (NMT) has achieved impressive progress, noisy input usually leads models to become fragile and unstable. Generating adversarial examples as the augmented data has been proved to be useful to alleviate this problem. Existing methods for adversarial example generation (AEG) are word-level or character-level, which ignore the ubiquitous phrase structure. In this paper, we propose a Phrase-level Adversarial Example Generation (PAEG) framework to enhance the robustness of the translation model. Our method further improves the gradient-based word-level AEG method by adopting a phrase-level substitution strategy. We verify our method on three benchmarks, including LDC Chinese-English, IWSLT14 German-English, and WMT14 English-German tasks. Experimental results demonstrate that our approach significantly improves translation performance and robustness to noise compared to previous strong baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Noise-robust Cross-modal Interactive Learning with Text2Image Mask for Multi-modal Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.452/",
        "paper_authors": [
            "Junjie Ye",
            "Junjun Guo",
            "Yan Xiang",
            "Kaiwen Tan",
            "Zhengtao Yu"
        ],
        "paper_abstract": "Multi-modal neural machine translation (MNMT) aims to improve textual level machine translation performance in the presence of text-related images. Most of the previous works on MNMT focus on multi-modal fusion methods with full visual features. However, text and its corresponding image may not match exactly, visual noise is generally inevitable. The irrelevant image regions may mislead or distract the textual attention and cause model performance degradation. This paper proposes a noise-robust multi-modal interactive fusion approach with cross-modal relation-aware mask mechanism for MNMT. A text-image relation-aware attention module is constructed through the cross-modal interaction mask mechanism, and visual features are extracted based on the text-image interaction mask knowledge. Then a noise-robust multi-modal adaptive fusion approach is presented by fusion the relevant visual and textual features for machine translation. We validate our method on the Multi30K dataset. The experimental results show the superiority of our proposed model, and achieve the state-of-the-art scores in all En-De, En-Fr and En-Cs translation tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Speeding up Transformer Decoding via an Attention Refinement Network",
        "paper_url": "https://aclanthology.org/2022.coling-1.453/",
        "paper_authors": [
            "Kaixin Wu",
            "Yue Zhang",
            "Bojie Hu",
            "Tong Zhang"
        ],
        "paper_abstract": "Despite the revolutionary advances made by Transformer in Neural Machine Translation (NMT), inference efficiency remains an obstacle due to the heavy use of attention operations in auto-regressive decoding. We thereby propose a lightweight attention structure called Attention Refinement Network (ARN) for speeding up Transformer. Specifically, we design a weighted residual network, which reconstructs the attention by reusing the features across layers. To further improve the Transformer efficiency, we merge the self-attention and cross-attention components for parallel computing. Extensive experiments on ten WMT machine translation tasks show that the proposed model yields an average of 1.35x faster (with almost no decrease in BLEU) over the state-of-the-art inference implementation. Results on widely used WMT14 En-De machine translation tasks demonstrate that our model achieves a higher speed-up, giving highly competitive performance compared to AAN and SAN models with fewer parameter numbers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Interactive Post-Editing for Verbosity Controlled Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.454/",
        "paper_authors": [
            "Prabhakar Gupta",
            "Anil Nelakanti",
            "Grant M. Berry",
            "Abhishek Sharma"
        ],
        "paper_abstract": "We explore Interactive Post-Editing (IPE) models for human-in-loop translation to help correct translation errors and rephrase it with a desired style variation. We specifically study verbosity for style variations and build on top of multi-source transformers that can read source and hypothesis to improve the latter with user inputs. Token-level interaction inputs for error corrections and length interaction inputs for verbosity control are used by the model to generate a suitable translation. We report BERTScore to evaluate semantic quality with other relevant metrics for translations from English to German, French and Spanish languages. Our model achieves superior BERTScore over state-of-the-art machine translation models while maintaining the desired token-level and verbosity preference.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Addressing Asymmetry in Multilingual Neural Machine Translation with Fuzzy Task Clustering",
        "paper_url": "https://aclanthology.org/2022.coling-1.455/",
        "paper_authors": [
            "Qian Wang",
            "Jiajun Zhang"
        ],
        "paper_abstract": "Multilingual neural machine translation (NMT) enables positive knowledge transfer among multiple translation tasks with a shared underlying model, but a unified multilingual model usually suffers from capacity bottleneck when tens or hundreds of languages are involved. A possible solution is to cluster languages and train individual model for each cluster. However, the existing clustering methods based on language similarity cannot handle the asymmetric problem in multilingual NMT, i.e., one translation task A can benefit from another translation task B but task B will be harmed by task A. To address this problem, we propose a fuzzy task clustering method for multilingual NMT. Specifically, we employ task affinity, defined as the loss change of one translation task caused by the training of another, as the clustering criterion. Next, we cluster the translation tasks based on the task affinity, such that tasks from the same cluster can benefit each other. For each cluster, we further find out a set of auxiliary translation tasks that benefit the tasks in this cluster. In this way, the model for each cluster is trained not only on the tasks in the cluster but also on the auxiliary tasks. We conduct extensive experiments for one-to-many, manyto-one, and many-to-many translation scenarios to verify the effectiveness of our method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.456/",
        "paper_authors": [
            "Qiang Wang",
            "Rongxiang Weng",
            "Ming Chen"
        ],
        "paper_abstract": "K-Nearest Neighbor Neural Machine Translation (kNNMT) successfully incorporates external corpus by retrieving word-level representations at test time. Generally, kNNMT borrows the off-the-shelf context representation in the translation task, e.g., the output of the last decoder layer, as the query vector of the retrieval task. In this work, we highlight that coupling the representations of these two tasks is sub-optimal for fine-grained retrieval. To alleviate it, we leverage supervised contrastive learning to learn the distinctive retrieval representation derived from the original context representation. We also propose a fast and effective approach to constructing hard negative samples. Experimental results on five domains show that our approach improves the retrieval accuracy and BLEU score compared to vanilla kNNMT.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantically Consistent Data Augmentation for Neural Machine Translation via Conditional Masked Language Model",
        "paper_url": "https://aclanthology.org/2022.coling-1.457/",
        "paper_authors": [
            "Qiao Cheng",
            "Jin Huang",
            "Yitao Duan"
        ],
        "paper_abstract": "This paper introduces a new data augmentation method for neural machine translation that can enforce stronger semantic consistency both within and across languages. Our method is based on Conditional Masked Language Model (CMLM) which is bi-directional and can be conditional on both left and right context, as well as the label. We demonstrate that CMLM is a good technique for generating context-dependent word distributions. In particular, we show that CMLM is capable of enforcing semantic consistency by conditioning on both source and target during substitution. In addition, to enhance diversity, we incorporate the idea of soft word substitution for data augmentation which replaces a word with a probabilistic distribution over the vocabulary. Experiments on four translation datasets of different scales show that the overall solution results in more realistic data augmentation and better translation quality. Our approach consistently achieves the best performance in comparison with strong and recent works and yields improvements of up to 1.90 BLEU points over the baseline.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Informative Language Representation Learning for Massively Multilingual Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.458/",
        "paper_authors": [
            "Renren Jin",
            "Deyi Xiong"
        ],
        "paper_abstract": "In a multilingual neural machine translation model that fully shares parameters across all languages, an artificial language token is usually used to guide translation into the desired target language. However, recent studies show that prepending language tokens sometimes fails to navigate the multilingual neural machine translation models into right translation directions, especially on zero-shot translation. To mitigate this issue, we propose two methods, language embedding embodiment and language-aware multi-head attention, to learn informative language representations to channel translation into right directions. The former embodies language embeddings into different critical switching points along the information flow from the source to the target, aiming at amplifying translation direction guiding signals. The latter exploits a matrix, instead of a vector, to represent a language in the continuous space. The matrix is chunked into multiple heads so as to learn language representations in multiple subspaces. Experiment results on two datasets for massively multilingual neural machine translation demonstrate that language-aware multi-head attention benefits both supervised and zero-shot translation and significantly alleviates the off-target translation issue. Further linguistic typology prediction experiments show that matrix-based language representations learned by our methods are capable of capturing rich linguistic typology features.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Rare but Severe Neural Machine Translation Errors Induced by Minimal Deletion: An Empirical Study on Chinese and English",
        "paper_url": "https://aclanthology.org/2022.coling-1.459/",
        "paper_authors": [
            "Ruikang Shi",
            "Alvin Grissom II",
            "Duc Minh Trinh"
        ],
        "paper_abstract": "We examine the inducement of rare but severe errors in English-Chinese and Chinese-English in-domain neural machine translation by minimal deletion of source text with character-based models. By deleting a single character, we can induce severe translation errors. We categorize these errors and compare the results of deleting single characters and single words. We also examine the effect of training data size on the number and types of pathological cases induced by these minimal perturbations, finding significant variation. We find that deleting a word hurts overall translation score more than deleting a character, but certain errors are more likely to occur when deleting characters, with language direction also influencing the effect.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.460/",
        "paper_authors": [
            "Sugyeong Eo",
            "Chanjun Park",
            "Hyeonseok Moon",
            "Jaehyung Seo",
            "Gyeongmin Kim",
            "Jungseob Lee",
            "Heuiseok Lim"
        ],
        "paper_abstract": "With the recent advance in neural machine translation demonstrating its importance, research on quality estimation (QE) has been steadily progressing. QE aims to automatically predict the quality of machine translation (MT) output without reference sentences. Despite its high utility in the real world, there remain several limitations concerning manual QE data creation: inevitably incurred non-trivial costs due to the need for translation experts, and issues with data scaling and language expansion. To tackle these limitations, we present QUAK, a Korean-English synthetic QE dataset generated in a fully automatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and QUAK-H, produced through three strategies that are relatively free from language constraints. Since each strategy requires no human effort, which facilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M for QUAK-M. As an experiment, we quantitatively analyze word-level QE results in various ways while performing statistical analysis. Moreover, we show that datasets scaled in an efficient way also contribute to performance improvements by observing meaningful performance gains in QUAK-M, P when adding data up to 1.58M.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Both Domain Robustness and Domain Adaptability in Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.461/",
        "paper_authors": [
            "Wen Lai",
            "Jind\u0159ich Libovick\u00fd",
            "Alexander Fraser"
        ],
        "paper_abstract": "We consider two problems of NMT domain adaptation using meta-learning. First, we want to reach domain robustness, i.e., we want to reach high quality on both domains seen in the training data and unseen domains. Second, we want our systems to be adaptive, i.e., making it possible to finetune systems with just hundreds of in-domain parallel sentences. We study the domain adaptability of meta-learning when improving the domain robustness of the model. In this paper, we propose a novel approach, RMLNMT (Robust Meta-Learning Framework for Neural Machine Translation Domain Adaptation), which improves the robustness of existing meta-learning models. More specifically, we show how to use a domain classifier in curriculum learning and we integrate the word-level domain mixing model into the meta-learning framework with a balanced sampling strategy. Experiments on English-German and English-Chinese translation show that RMLNMT improves in terms of both domain robustness and domain adaptability in seen and unseen domains.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CoDoNMT: Modeling Cohesion Devices for Document-Level Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.462/",
        "paper_authors": [
            "Yikun Lei",
            "Yuqi Ren",
            "Deyi Xiong"
        ],
        "paper_abstract": "Cohesion devices, e.g., reiteration, coreference, are crucial for building cohesion links across sentences. In this paper, we propose a document-level neural machine translation framework, CoDoNMT, which models cohesion devices from two perspectives: Cohesion Device Masking (CoDM) and Cohesion Attention Focusing (CoAF). In CoDM, we mask cohesion devices in the current sentence and force NMT to predict them with inter-sentential context information. A prediction task is also introduced to be jointly trained with NMT. In CoAF, we attempt to guide the model to pay exclusive attention to relevant cohesion devices in the context when translating cohesion devices in the current sentence. Such a cohesion attention focusing strategy is softly applied to the self-attention layer. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art document-level neural machine translation baselines. Further linguistic evaluation validates the effectiveness of the proposed model in producing cohesive translations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Non-Autoregressive Neural Machine Translation via Modeling Localness",
        "paper_url": "https://aclanthology.org/2022.coling-1.463/",
        "paper_authors": [
            "Yong Wang",
            "Xinwei Geng"
        ],
        "paper_abstract": "Non-autoregressive translation (NAT) models, which eliminate the sequential dependencies within the target sentence, have achieved remarkable inference speed, but suffer from inferior translation quality. Towards exploring the underlying causes, we carry out a thorough preliminary study on the attention mechanism, which demonstrates the serious weakness in capturing localness compared with conventional autoregressive translation (AT). In response to this problem, we propose to improve the localness of NAT models by explicitly introducing the information about surrounding words. Specifically, temporal convolutions are incorporated into both encoder and decoder sides to obtain localness-aware representations. Extensive experiments on several typical translation datasets show that the proposed method can achieve consistent and significant improvements over strong NAT baselines. Further analyses on the WMT14 En-De translation task reveal that compared with baselines, our approach accelerates the convergence in training and can achieve equivalent performance with a reduction of 70% training steps.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Categorizing Semantic Representations for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.464/",
        "paper_authors": [
            "Yongjing Yin",
            "Yafu Li",
            "Fandong Meng",
            "Jie Zhou",
            "Yue Zhang"
        ],
        "paper_abstract": "Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks. However, they have recently been shown to suffer limitation in compositional generalization, failing to effectively learn the translation of atoms (e.g., words) and their semantic composition (e.g., modification) from seen compounds (e.g., phrases), and thus suffering from significantly weakened translation performance on unseen compounds during inference. We address this issue by introducing categorization to the source contextualized representations. The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding. Experiments on a dedicated MT dataset (i.e., CoGnition) show that our method reduces compositional generalization error rates by 24% error reduction. In addition, our conceptually simple method gives consistently better results than the Transformer baseline on a range of general MT datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adversarial Training on Disentangling Meaning and Language Representations for Unsupervised Quality Estimation",
        "paper_url": "https://aclanthology.org/2022.coling-1.465/",
        "paper_authors": [
            "Yuto Kuroda",
            "Tomoyuki Kajiwara",
            "Yuki Arase",
            "Takashi Ninomiya"
        ],
        "paper_abstract": "We propose a method to distill language-agnostic meaning embeddings from multilingual sentence encoders for unsupervised quality estimation of machine translation. Our method facilitates that the meaning embeddings focus on semantics by adversarial training that attempts to eliminate language-specific information. Experimental results on unsupervised quality estimation reveal that our method achieved higher correlations with human evaluations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Alleviating the Inequality of Attention Heads for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.466/",
        "paper_authors": [
            "Zewei Sun",
            "Shujian Huang",
            "Xinyu Dai",
            "Jiajun Chen"
        ],
        "paper_abstract": "Recent studies show that the attention heads in Transformer are not equal. We relate this phenomenon to the imbalance training of multi-head attention and the model dependence on specific heads. To tackle this problem, we propose a simple masking method: HeadMask, in two specific ways. Experiments show that translation improvements are achieved on multiple language pairs. Subsequent empirical analyses also support our assumption and confirm the effectiveness of the method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adapting to Non-Centered Languages for Zero-shot Multilingual Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.467/",
        "paper_authors": [
            "Zhi Qu",
            "Taro Watanabe"
        ],
        "paper_abstract": "Multilingual neural machine translation can translate unseen language pairs during training, i.e. zero-shot translation. However, the zero-shot translation is always unstable. Although prior works attributed the instability to the domination of central language, e.g. English, we supplement this viewpoint with the strict dependence of non-centered languages. In this work, we propose a simple, lightweight yet effective language-specific modeling method by adapting to non-centered languages and combining the shared information and the language-specific information to counteract the instability of zero-shot translation. Experiments with Transformer on IWSLT17, Europarl, TED talks, and OPUS-100 datasets show that our method not only performs better than strong baselines in centered data conditions but also can easily fit non-centered data conditions. By further investigating the layer attribution, we show that our proposed method can disentangle the coupled representation in the correct direction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Robust Neural Machine Translation with Iterative Scheduled Data-Switch Training",
        "paper_url": "https://aclanthology.org/2022.coling-1.468/",
        "paper_authors": [
            "Zhongjian Miao",
            "Xiang Li",
            "Liyan Kang",
            "Wen Zhang",
            "Chulun Zhou",
            "Yidong Chen",
            "Bin Wang",
            "Min Zhang",
            "Jinsong Su"
        ],
        "paper_abstract": "Most existing methods on robust neural machine translation (NMT) construct adversarial examples by injecting noise into authentic examples and indiscriminately exploit two types of examples. They require the model to translate both the authentic source sentence and its adversarial counterpart into the identical target sentence within the same training stage, which may be a suboptimal choice to achieve robust NMT. In this paper, we first conduct a preliminary study to confirm this claim and further propose an Iterative Scheduled Data-switch Training Framework to mitigate this problem. Specifically, we introduce two training stages, iteratively switching between authentic and adversarial examples. Compared with previous studies, our model focuses more on just one type of examples at each single stage, which can better exploit authentic and adversarial examples, and thus obtaining a better robust NMT model. Moreover, we introduce an improved curriculum learning method with a sampling strategy to better schedule the process of noise injection. Experimental results show that our model significantly surpasses several competitive baselines on four translation benchmarks. Our source code is available at https://github.com/DeepLearnXMU/RobustNMT-ISDST.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-lingual Feature Extraction from Monolingual Corpora for Low-resource Unsupervised Bilingual Lexicon Induction",
        "paper_url": "https://aclanthology.org/2022.coling-1.469/",
        "paper_authors": [
            "Zihao Feng",
            "Hailong Cao",
            "Tiejun Zhao",
            "Weixuan Wang",
            "Wei Peng"
        ],
        "paper_abstract": "Despite their progress in high-resource language settings, unsupervised bilingual lexicon induction (UBLI) models often fail on corpora with low-resource distant language pairs due to insufficient initialization. In this work, we propose a cross-lingual feature extraction (CFE) method to learn the cross-lingual features from monolingual corpora for low-resource UBLI, enabling representations of words with the same meaning leveraged by the initialization step. By integrating cross-lingual representations with pre-trained word embeddings in a fully unsupervised initialization on UBLI, the proposed method outperforms existing state-of-the-art methods on low-resource language pairs (EN-VI, EN-TH, EN-ZH, EN-JA). The ablation study also proves that the learned cross-lingual features can enhance the representational ability and robustness of the existing embedding model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Language-Independent Approach for Morphological Disambiguation",
        "paper_url": "https://aclanthology.org/2022.coling-1.470/",
        "paper_authors": [
            "Alymzhan Toleu",
            "Gulmira Tolegen",
            "Rustam Mussabayev"
        ],
        "paper_abstract": "This paper presents a language-independent approach for morphological disambiguation which has been regarded as an extension of POS tagging, jointly predicting complex morphological tags. In the proposed approach, all words, roots, POS and morpheme tags are embedded into vectors, and contexts representations from surface word and morphological contexts are calculated. Then the inner products between analyses and the context\u2019s representations are computed to perform the disambiguation. The underlying hypothesis is that the correct morphological analysis should be closer to the context in a vector space. Experimental results show that the proposed approach outperforms the existing models on seven different language datasets. Concretely, compared with the baselines of MarMot and a sophisticated neural model (Seq2Seq), the proposed approach achieves around 6% improvement in average accuracy for all languages while running about 6 and 33 times faster than MarMot and Seq2Seq, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers",
        "paper_url": "https://aclanthology.org/2022.coling-1.471/",
        "paper_authors": [
            "Bowen Qin",
            "Lihan Wang",
            "Binyuan Hui",
            "Bowen Li",
            "Xiangpeng Wei",
            "Binhua Li",
            "Fei Huang",
            "Luo Si",
            "Min Yang",
            "Yongbin Li"
        ],
        "paper_abstract": "This paper aims to improve the performance of text-to-SQL parsing by exploring the intrinsic uncertainties in the neural network based approaches (called SUN). From the data uncertainty perspective, it is indisputable that a single SQL can be learned from multiple semantically-equivalent questions. Different from previous methods that are limited to one-to-one mapping, we propose a data uncertainty constraint to explore the underlying complementary semantic information among multiple semantically-equivalent questions (many-to-one) and learn the robust feature representations with reduced spurious associations. In this way, we can reduce the sensitivity of the learned representations and improve the robustness of the parser. From the model uncertainty perspective, there is often structural information (dependence) among the weights of neural networks. To improve the generalizability and stability of neural text-to-SQL parsers, we propose a model uncertainty constraint to refine the query representations by enforcing the output representations of different perturbed encoding networks to be consistent with each other. Extensive experiments on five benchmark datasets demonstrate that our method significantly outperforms strong competitors and achieves new state-of-the-art results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Deciphering and Characterizing Out-of-Vocabulary Words for Morphologically Rich Languages",
        "paper_url": "https://aclanthology.org/2022.coling-1.472/",
        "paper_authors": [
            "Georgie Botev",
            "Arya D. McCarthy",
            "Winston Wu",
            "David Yarowsky"
        ],
        "paper_abstract": "This paper presents a detailed foundational empirical case study of the nature of out-of-vocabulary words encountered in modern text in a moderate-resource language such as Bulgarian, and a multi-faceted distributional analysis of the underlying word-formation processes that can aid in their compositional translation, tagging, parsing, language modeling, and other NLP tasks. Given that out-of-vocabulary (OOV) words generally present a key open challenge to NLP and machine translation systems, especially toward the lower limit of resource availability, there are useful practical insights, as well as corpus-linguistic insights, from both a detailed manual and automatic taxonomic analysis of the types, multidimensional properties, and processing potential for multiple representative OOV data samples.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling",
        "paper_url": "https://aclanthology.org/2022.coling-1.473/",
        "paper_authors": [
            "Guanting Dong",
            "Daichi Guo",
            "Liwen Wang",
            "Xuefeng Li",
            "Zechen Wang",
            "Chen Zeng",
            "Keqing He",
            "Jinzheng Zhao",
            "Hao Lei",
            "Xinyue Cui",
            "Yi Huang",
            "Junlan Feng",
            "Weiran Xu"
        ],
        "paper_abstract": "Most existing slot filling models tend to memorize inherent patterns of entities and corresponding contexts from training data. However, these models can lead to system failure or undesirable outputs when being exposed to spoken language perturbation or variation in practice. We propose a perturbed semantic structure awareness transferring method for training perturbation-robust slot filling models. Specifically, we introduce two MLM-based training strategies to respectively learn contextual semantic structure and word distribution from unsupervised language perturbation corpus. Then, we transfer semantic knowledge learned from upstream training procedure into the original samples and filter generated data by consistency processing. These procedures aims to enhance the robustness of slot filling models. Experimental results show that our method consistently outperforms the previous basic methods and gains strong generalization while preventing the model from memorizing inherent patterns of entities and contexts.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "String Editing Based Chinese Grammatical Error Diagnosis",
        "paper_url": "https://aclanthology.org/2022.coling-1.474/",
        "paper_authors": [
            "Haihua Xie",
            "Xiaoqing Lyu",
            "Xuefei Chen"
        ],
        "paper_abstract": "Chinese Grammatical Error Diagnosis (CGED) suffers the problems of numerous types of grammatical errors and insufficiency of training data. In this paper, we propose a string editing based CGED model that requires less training data by using a unified workflow to handle various types of grammatical errors. Two measures are proposed in our model to enhance the performance of CGED. First, the detection and correction of grammatical errors are divided into different stages. In the stage of error detection, the model only outputs the types of grammatical errors so that the tag vocabulary size is significantly reduced compared with other string editing based models. Secondly, the correction of some grammatical errors is converted to the task of masked character inference, which has plenty of training data and mature solutions. Experiments on datasets of NLPTEA-CGED demonstrate that our model outperforms other CGED models in many aspects.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The Fragility of Multi-Treebank Parsing Evaluation",
        "paper_url": "https://aclanthology.org/2022.coling-1.475/",
        "paper_authors": [
            "Iago Alonso-Alonso",
            "David Vilares",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "paper_abstract": "Treebank selection for parsing evaluation and the spurious effects that might arise from a biased choice have not been explored in detail. This paper studies how evaluating on a single subset of treebanks can lead to weak conclusions. First, we take a few contrasting parsers, and run them on subsets of treebanks proposed in previous work, whose use was justified (or not) on criteria such as typology or data scarcity. Second, we run a large-scale version of this experiment, create vast amounts of random subsets of treebanks, and compare on them many parsers whose scores are available. The results show substantial variability across subsets and that although establishing guidelines for good treebank selection is hard, some inadequate strategies can be easily avoided.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "FactMix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition",
        "paper_url": "https://aclanthology.org/2022.coling-1.476/",
        "paper_authors": [
            "Linyi Yang",
            "Lifan Yuan",
            "Leyang Cui",
            "Wenyang Gao",
            "Yue Zhang"
        ],
        "paper_abstract": "Few-shot Named Entity Recognition (NER) is imperative for entity tagging in limited resource domains and thus received proper attention in recent years. Existing approaches for few-shot NER are evaluated mainly under in-domain settings. In contrast, little is known about how these inherently faithful models perform in cross-domain NER using a few labeled in-domain examples. This paper proposes a two-step rationale-centric data augmentation method to improve the model\u2019s generalization ability. Results on several datasets show that our model-agnostic method significantly improves the performance of cross-domain NER tasks compared to previous state-of-the-art methods compared to the counterfactual data augmentation and prompt-tuning methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Speaker-Aware Discourse Parsing on Multi-Party Dialogues",
        "paper_url": "https://aclanthology.org/2022.coling-1.477/",
        "paper_authors": [
            "Nan Yu",
            "Guohong Fu",
            "Min Zhang"
        ],
        "paper_abstract": "Discourse parsing on multi-party dialogues is an important but difficult task in dialogue systems and conversational analysis. It is believed that speaker interactions are helpful for this task. However, most previous research ignores speaker interactions between different speakers. To this end, we present a speaker-aware model for this task. Concretely, we propose a speaker-context interaction joint encoding (SCIJE) approach, using the interaction features between different speakers. In addition, we propose a second-stage pre-training task, same speaker prediction (SSP), enhancing the conversational context representations by predicting whether two utterances are from the same speaker. Experiments on two standard benchmark datasets show that the proposed model achieves the best-reported performance in the literature. We will release the codes of this paper to facilitate future research.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Iterative Span Selection: Self-Emergence of Resolving Orders in Semantic Role Labeling",
        "paper_url": "https://aclanthology.org/2022.coling-1.478/",
        "paper_authors": [
            "Shuhei Kurita",
            "Hiroki Ouchi",
            "Kentaro Inui",
            "Satoshi Sekine"
        ],
        "paper_abstract": "Semantic Role Labeling (SRL) is the task of labeling semantic arguments for marked semantic predicates. Semantic arguments and their predicates are related in various distinct manners, of which certain semantic arguments are a necessity while others serve as an auxiliary to their predicates. To consider such roles and relations of the arguments in the labeling order, we introduce iterative argument identification (IAI), which combines global decoding and iterative identification for the semantic arguments. In experiments, we first realize that the model with random argument labeling orders outperforms other heuristic orders such as the conventional left-to-right labeling order. Combined with simple reinforcement learning, the proposed model spontaneously learns the optimized labeling orders that are different from existing heuristic orders. The proposed model with the IAI algorithm achieves competitive or outperforming results from the existing models in the standard benchmark datasets of span-based SRL: CoNLL-2005 and CoNLL-2012.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.479/",
        "paper_authors": [
            "Taeuk Kim"
        ],
        "paper_abstract": "Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a recent paradigm that attempts to induce constituency parse trees relying only on the internal knowledge of pre-trained language models. While attractive in the perspective that similar to in-context learning, it does not require task-specific fine-tuning, the practical effectiveness of such an approach still remains unclear, except that it can function as a probe for investigating language models\u2019 inner workings. In this work, we mathematically reformulate CPE-PLM and propose two advanced ensemble methods tailored for it, demonstrating that the new parsing paradigm can be competitive with common unsupervised parsers by introducing a set of heterogeneous PLMs combined using our techniques. Furthermore, we explore some scenarios where the trees generated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM is more effective than typical supervised parsers in few-shot settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Position Offset Label Prediction for Grammatical Error Correction",
        "paper_url": "https://aclanthology.org/2022.coling-1.480/",
        "paper_authors": [
            "Xiuyu Wu",
            "Jingsong Yu",
            "Xu Sun",
            "Yunfang Wu"
        ],
        "paper_abstract": "We introduce a novel position offset label prediction subtask to the encoder-decoder architecture for grammatical error correction (GEC) task. To keep the meaning of the input sentence unchanged, only a few words should be inserted or deleted during correction, and most of tokens in the erroneous sentence appear in the paired correct sentence with limited position movement. Inspired by this observation, we design an auxiliary task to predict position offset label (POL) of tokens, which is naturally capable of integrating different correction editing operations into a unified framework. Based on the predicted POL, we further propose a new copy mechanism (P-copy) to replace the vanilla copy module. Experimental results on Chinese, English and Japanese datasets demonstrate that our proposed POL-Pc framework obviously improves the performance of baseline models. Moreover, our model yields consistent performance gain over various data augmentation methods. Especially, after incorporating synthetic data, our model achieves a 38.95 F-0.5 score on Chinese GEC dataset, which outperforms the previous state-of-the-art by a wide margin of 1.98 points.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Parsing Natural Language into Propositional and First-Order Logic with Dual Reinforcement Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.481/",
        "paper_authors": [
            "Xuantao Lu",
            "Jingping Liu",
            "Zhouhong Gu",
            "Hanwen Tong",
            "Chenhao Xie",
            "Junyang Huang",
            "Yanghua Xiao",
            "Wenguang Wang"
        ],
        "paper_abstract": "Semantic parsing converts natural language utterances into structured logical expressions. We consider two such formal representations: Propositional Logic (PL) and First-order Logic (FOL). The paucity of labeled data is a major challenge in this field. In previous works, dual reinforcement learning has been proposed as an approach to reduce dependence on labeled data. However, this method has the following limitations: 1) The reward needs to be set manually and is not applicable to all kinds of logical expressions. 2) The training process easily collapses when models are trained with only the reward from dual reinforcement learning. In this paper, we propose a scoring model to automatically learn a model-based reward, and an effective training strategy based on curriculum learning is further proposed to stabilize the training process. In addition to the technical contribution, a Chinese-PL/FOL dataset is constructed to compensate for the paucity of labeled data in this field. Experimental results show that the proposed method outperforms competitors on several datasets. Furthermore, by introducing PL/FOL generated by our model, the performance of existing Natural Language Inference (NLI) models is further enhanced.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Yet Another Format of Universal Dependencies for Korean",
        "paper_url": "https://aclanthology.org/2022.coling-1.482/",
        "paper_authors": [
            "Yige Chen",
            "Eunkyul Leah Jo",
            "Yundong Yao",
            "KyungTae Lim",
            "Miikka Silfverberg",
            "Francis M. Tyers",
            "Jungyeul Park"
        ],
        "paper_abstract": "In this study, we propose a morpheme-based scheme for Korean dependency parsing and adopt the proposed scheme to Universal Dependencies. We present the linguistic rationale that illustrates the motivation and the necessity of adopting the morpheme-based format, and develop scripts that convert between the original format used by Universal Dependencies and the proposed morpheme-based format automatically. The effectiveness of the proposed format for Korean dependency parsing is then testified by both statistical and neural models, including UDPipe and Stanza, with our carefully constructed morpheme-based word embedding for Korean. morphUD outperforms parsing results for all Korean UD treebanks, and we also present detailed error analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enhancing Structure-aware Encoder with Extremely Limited Data for Graph-based Dependency Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.483/",
        "paper_authors": [
            "Yuanhe Tian",
            "Yan Song",
            "Fei Xia"
        ],
        "paper_abstract": "Dependency parsing is an important fundamental natural language processing task which analyzes the syntactic structure of an input sentence by illustrating the syntactic relations between words. To improve dependency parsing, leveraging existing dependency parsers and extra data (e.g., through semi-supervised learning) has been demonstrated to be effective, even though the final parsers are trained on inaccurate (but massive) data. In this paper, we propose a frustratingly easy approach to improve graph-based dependency parsing, where a structure-aware encoder is pre-trained on auto-parsed data by predicting the word dependencies and then fine-tuned on gold dependency trees, which differs from the usual pre-training process that aims to predict the context words along dependency paths. Experimental results and analyses demonstrate the effectiveness and robustness of our approach to benefit from the data (even with noise) processed by different parsers, where our approach outperforms strong baselines under different settings with different dependency standards and model architectures used in pre-training and fine-tuning. More importantly, further analyses find that only 2K auto-parsed sentences are required to obtain improvement when pre-training vanilla BERT-large based parser without requiring extra parameters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Simple and Effective Graph-to-Graph Annotation Conversion",
        "paper_url": "https://aclanthology.org/2022.coling-1.484/",
        "paper_authors": [
            "Yuxuan Wang",
            "Zhilin Lei",
            "Yuqiu Ji",
            "Wanxiang Che"
        ],
        "paper_abstract": "Annotation conversion is an effective way to construct datasets under new annotation guidelines based on existing datasets with little human labour. Previous work has been limited in conversion between tree-structured datasets and mainly focused on feature-based models which are not easily applicable to new conversions. In this paper, we propose two simple and effective graph-to-graph annotation conversion approaches, namely Label Switching and Graph2Graph Linear Transformation, which use pseudo data and inherit parameters to guide graph conversions respectively. These methods are able to deal with conversion between graph-structured annotations and require no manually designed features. To verify their effectiveness, we manually construct a graph-structured parallel annotated dataset and evaluate the proposed approaches on it as well as other existing parallel annotated datasets. Experimental results show that the proposed approaches outperform strong baselines with higher conversion score. To further validate the quality of converted graphs, we utilize them to train the target parser and find graphs generated by our approaches lead to higher parsing score than those generated by the baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BiBL: AMR Parsing and Generation with Bidirectional Bayesian Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.485/",
        "paper_authors": [
            "Ziming Cheng",
            "Zuchao Li",
            "Hai Zhao"
        ],
        "paper_abstract": "Abstract Meaning Representation (AMR) offers a unified semantic representation for natural language sentences. Thus transformation between AMR and text yields two transition tasks in opposite directions, i.e., Text-to-AMR parsing and AMR-to-Text generation. Existing AMR studies only focus on one-side improvements despite the duality of the two tasks, and their improvements are greatly attributed to the inclusion of large extra training data or complex structure modifications which harm the inference speed. Instead, we propose data-efficient Bidirectional Bayesian learning (BiBL) to facilitate bidirectional information transition by adopting a single-stage multitasking strategy so that the resulting model may enjoy much lighter training at the same time. Evaluation on benchmark datasets shows that our proposed BiBL outperforms strong previous seq2seq refinements without the help of extra data which is indispensable in existing counterpart models. We release the codes of BiBL at: https://github.com/KHAKhazeus/BiBL.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Layer Pseudo-Siamese Biaffine Model for Dependency Parsing",
        "paper_url": "https://aclanthology.org/2022.coling-1.486/",
        "paper_authors": [
            "Ziyao Xu",
            "Houfeng Wang",
            "Bingdong Wang"
        ],
        "paper_abstract": "Biaffine method is a strong and efficient method for graph-based dependency parsing. However, previous work only used the biaffine method at the end of the dependency parser as a scorer, and its application in multi-layer form is ignored. In this paper, we propose a multi-layer pseudo-Siamese biaffine model for neural dependency parsing. In this model, we modify the biaffine method so that it can be utilized in multi-layer form, and use pseudo-Siamese biaffine module to construct arc weight matrix for final prediction. In our proposed multi-layer architecture, the biaffine method plays important roles in both scorer and attention mechanism at the same time in each layer. We evaluate our model on PTB, CTB, and UD. The model achieves state-of-the-art results on these datasets. Further experiments show the benefits of introducing multi-layer form and pseudo-Siamese module into the biaffine method with low efficiency loss.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Belief Revision Based Caption Re-ranker with Visual Semantic Information",
        "paper_url": "https://aclanthology.org/2022.coling-1.487/",
        "paper_authors": [
            "Ahmed Sabir",
            "Francesc Moreno-Noguer",
            "Pranava Madhyastha",
            "Llu\u00eds Padr\u00f3"
        ],
        "paper_abstract": "In this work, we focus on improving the captions generated by image-caption generation systems. We propose a novel re-ranking approach that leverages visual-semantic measures to identify the ideal caption that maximally captures the visual information in the image. Our re-ranker utilizes the Belief Revision framework (Blok et al., 2003) to calibrate the original likelihood of the top-n captions by explicitly exploiting semantic relatedness between the depicted caption and the visual context. Our experiments demonstrate the utility of our approach, where we observe that our re-ranker can enhance the performance of a typical image-captioning system without necessity of any additional training or fine-tuning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Understanding the Relation between Gestures and Language",
        "paper_url": "https://aclanthology.org/2022.coling-1.488/",
        "paper_authors": [
            "Artem Abzaliev",
            "Andrew Owens",
            "Rada Mihalcea"
        ],
        "paper_abstract": "In this paper, we explore the relation between gestures and language. Using a multimodal dataset, consisting of Ted talks where the language is aligned with the gestures made by the speakers, we adapt a semi-supervised multimodal model to learn gesture embeddings. We show that gestures are predictive of the native language of the speaker, and that gesture embeddings further improve language prediction result. In addition, gesture embeddings might contain some linguistic information, as we show by probing embeddings for psycholinguistic categories. Finally, we analyze the words that lead to the most expressive gestures and find that function words drive the expressiveness of gestures.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Building Joint Relationship Attention Network for Image-Text Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.489/",
        "paper_authors": [
            "Changzhi Wang",
            "Xiaodong Gu"
        ],
        "paper_abstract": "Attention based methods for image-text generation often focus on visual features individually, while ignoring relationship information among image features that provides important guidance for generating sentences. To alleviate this issue, in this work we propose the Joint Relationship Attention Network (JRAN) that novelly explores the relationships among the features. Specifically, different from the previous relationship based approaches that only explore the single relationship in the image, our JRAN can effectively learn two relationships, the visual relationships among region features and the visual-semantic relationships between region features and semantic features, and further make a dynamic trade-off between them during outputting the relationship representation. Moreover, we devise a new relationship based attention, which can adaptively focus on the output relationship representation when predicting different words. Extensive experiments on large-scale MSCOCO and small-scale Flickr30k datasets show that JRAN achieves state-of-the-art performance. More remarkably, JRAN achieves new 28.3% and 58.2% performance in terms of BLEU4 and CIDEr metric on Flickr30k dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Focus on the Foreground for Temporal Sentence Grounding",
        "paper_url": "https://aclanthology.org/2022.coling-1.490/",
        "paper_authors": [
            "Daizong Liu",
            "Wei Hu"
        ],
        "paper_abstract": "Temporal sentence grounding (TSG) is crucial and fundamental for video understanding. Previous works typically model the target activity referred to the sentence query in a video by extracting the appearance information from each whole frame. However, these methods fail to distinguish visually similar background noise and capture subtle details of small objects. Although a few recent works additionally adopt a detection model to filter out the background contents and capture local appearances of foreground objects, they rely on the quality of the detection model and suffer from the time-consuming detection process. To this end, we propose a novel detection-free framework for TSG\u2014Grounding with Learnable Foreground (GLF), which efficiently learns to locate the foreground regions related to the query in consecutive frames for better modelling the target activity. Specifically, we first split each video frame into multiple patch candidates of equal size, and reformulate the foreground detection problem as a patch localization task. Then, we develop a self-supervised coarse-to-fine paradigm to learn to locate the most query-relevant patch in each frame and aggregate them among the video for final grounding. Further, we employ a multi-scale patch reasoning strategy to capture more fine-grained foreground information. Extensive experiments on three challenging datasets (Charades-STA, TACoS, ActivityNet) show that the proposed GLF outperforms state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Are Visual-Linguistic Models Commonsense Knowledge Bases?",
        "paper_url": "https://aclanthology.org/2022.coling-1.491/",
        "paper_authors": [
            "Hsiu-Yu Yang",
            "Carina Silberer"
        ],
        "paper_abstract": "Despite the recent success of pretrained language models as on-the-fly knowledge sources for various downstream tasks, they are shown to inadequately represent trivial common facts that vision typically captures. This limits their application to natural language understanding tasks that require commonsense knowledge. We seek to determine the capability of pretrained visual-linguistic models as knowledge sources on demand. To this end, we systematically compare language-only and visual-linguistic models in a zero-shot commonsense question answering inference task. We find that visual-linguistic models are highly promising regarding their benefit for text-only tasks on certain types of commonsense knowledge associated with the visual world. Surprisingly, this knowledge can be activated even when no visual input is given during inference, suggesting an effective multimodal fusion during pretraining. However, we reveal that there is still a huge space for improvement towards better cross-modal reasoning abilities and pretraining strategies for event understanding.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Visual Prompt Tuning for Few-Shot Text Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.492/",
        "paper_authors": [
            "Jingyuan Wen",
            "Yutian Luo",
            "Nanyi Fei",
            "Guoxing Yang",
            "Zhiwu Lu",
            "Hao Jiang",
            "Jie Jiang",
            "Zhao Cao"
        ],
        "paper_abstract": "Deploying large-scale pre-trained models in the prompt-tuning paradigm has demonstrated promising performance in few-shot learning. Particularly, vision-language pre-training models (VL-PTMs) have been intensively explored in various few-shot downstream tasks. However, most existing works only apply VL-PTMs to visual tasks like image classification, with few attempts being made on language tasks like text classification. In few-shot text classification, a feasible paradigm for deploying VL-PTMs is to align the input samples and their category names via the text encoders. However, it leads to the waste of visual information learned by the image encoders of VL-PTMs. To overcome this drawback, we propose a novel method named Visual Prompt Tuning (VPT). To our best knowledge, this method is the first attempt to deploy VL-PTM in few-shot text classification task. The main idea is to generate the image embeddings w.r.t. category names as visual prompt and then add them to the aligning process. Extensive experiments show that our VPT can achieve significant improvements under both zero-shot and few-shot settings. Importantly, our VPT even outperforms the most recent prompt-tuning methods on five public text classification datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Systematic Analysis of Image Schemas in Natural Language through Explainable Multilingual Neural Language Processing",
        "paper_url": "https://aclanthology.org/2022.coling-1.493/",
        "paper_authors": [
            "Lennart Wachowiak",
            "Dagmar Gromann"
        ],
        "paper_abstract": "In embodied cognition, physical experiences are believed to shape abstract cognition, such as natural language and reasoning. Image schemas were introduced as spatio-temporal cognitive building blocks that capture these recurring sensorimotor experiences. The few existing approaches for automatic detection of image schemas in natural language rely on specific assumptions about word classes as indicators of spatio-temporal events. Furthermore, the lack of sufficiently large, annotated datasets makes evaluation and supervised learning difficult. We propose to build on the recent success of large multilingual pretrained language models and a small dataset of examples from image schema literature to train a supervised classifier that classifies natural language expressions of varying lengths into image schemas. Despite most of the training data being in English with few examples for German, the model performs best in German. Additionally, we analyse the model\u2019s zero-shot performance in Russian, French, and Mandarin. To further investigate the model\u2019s behaviour, we utilize local linear approximations for prediction probabilities that indicate which words in a sentence the model relies on for its final classification decision. Code and dataset are publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?",
        "paper_url": "https://aclanthology.org/2022.coling-1.494/",
        "paper_authors": [
            "Lovisa Hagstr\u00f6m",
            "Richard Johansson"
        ],
        "paper_abstract": "Current language models have been criticised for learning language from text alone without connection between words and their meaning. Consequently, multimodal training has been proposed as a way for creating models with better language understanding by providing the lacking connection. We focus on pre-trained multimodal vision-and-language (VL) models for which there already are some results on their language understanding capabilities. An unresolved issue with evaluating the linguistic skills of these models, however, is that there is no established method for adapting them to text-only input without out-of-distribution uncertainty. To find the best approach, we investigate and compare seven possible methods for adapting three different pre-trained VL models to text-only input. Our evaluations on both GLUE and Visual Property Norms (VPN) show that care should be put into adapting VL models to zero-shot text-only tasks, while the models are less sensitive to how we adapt them to non-zero-shot tasks. We also find that the adaptation methods perform differently for different models and that unimodal model counterparts perform on par with the VL models regardless of adaptation, indicating that current VL models do not necessarily gain better language understanding from their multimodal training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ACT-Thor: A Controlled Benchmark for Embodied Action Understanding in Simulated Environments",
        "paper_url": "https://aclanthology.org/2022.coling-1.495/",
        "paper_authors": [
            "Michael Hanna",
            "Federico Pedeni",
            "Alessandro Suglia",
            "Alberto Testoni",
            "Raffaella Bernardi"
        ],
        "paper_abstract": "Artificial agents are nowadays challenged to perform embodied AI tasks. To succeed, agents must understand the meaning of verbs and how their corresponding actions transform the surrounding world. In this work, we propose ACT-Thor, a novel controlled benchmark for embodied action understanding. We use the AI2-THOR simulated environment to produce a controlled setup in which an agent, given a before-image and an associated action command, has to determine what the correct after-image is among a set of possible candidates. First, we assess the feasibility of the task via a human evaluation that resulted in 81.4% accuracy, and very high inter-annotator agreement (84.9%). Second, we design both unimodal and multimodal baselines, using state-of-the-art visual feature extractors. Our evaluation and error analysis suggest that only models that have a very structured representation of the actions together with powerful visual features can perform well on the task. However, they still fall behind human performance in a zero-shot scenario where the model is exposed to unseen (action, object) pairs. This paves the way for a systematic way of evaluating embodied AI agents that understand grounded actions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "In-the-Wild Video Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.496/",
        "paper_authors": [
            "Santiago Castro",
            "Naihao Deng",
            "Pingxuan Huang",
            "Mihai Burzo",
            "Rada Mihalcea"
        ],
        "paper_abstract": "Existing video understanding datasets mostly focus on human interactions, with little attention being paid to the \u201cin the wild\u201d settings, where the videos are recorded outdoors. We propose WILDQA, a video understanding dataset of videos recorded in outside settings. In addition to video question answering (Video QA), we also introduce the new task of identifying visual support for a given question and answer (Video Evidence Selection). Through evaluations using a wide range of baseline models, we show that WILDQA poses new challenges to the vision and language research communities. The dataset is available at https: //lit.eecs.umich.edu/wildqa/.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Better Semantic Understanding of Mobile Interfaces",
        "paper_url": "https://aclanthology.org/2022.coling-1.497/",
        "paper_authors": [
            "Srinivas Sunkara",
            "Maria Wang",
            "Lijuan Liu",
            "Gilles Baechler",
            "Yu-Chung Hsiao",
            "Jindong Chen",
            "Abhanshu Sharma",
            "James W. W. Stout"
        ],
        "paper_abstract": "Improving the accessibility and automation capabilities of mobile devices can have a significant positive impact on the daily lives of countless users. To stimulate research in this direction, we release a human-annotated dataset with approximately 500k unique annotations aimed at increasing the understanding of the functionality of UI elements. This dataset augments images and view hierarchies from RICO, a large dataset of mobile UIs, with annotations for icons based on their shapes and semantics, and associations between different elements and their corresponding text labels, resulting in a significant increase in the number of UI elements and the categories assigned to them. We also release models using image-only and multimodal inputs; we experiment with various architectures and study the benefits of using multimodal inputs on the new dataset. Our models demonstrate strong performance on an evaluation set of unseen apps, indicating their generalizability to newer screens. These models, combined with the new dataset, can enable innovative functionalities like referring to UI elements by their labels, improved coverage and better semantics for icons etc., which would go a long way in making UIs more usable for everyone.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "End-to-end Dense Video Captioning as Sequence Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.498/",
        "paper_authors": [
            "Wanrong Zhu",
            "Bo Pang",
            "Ashish V. Thapliyal",
            "William Yang Wang",
            "Radu Soricut"
        ],
        "paper_abstract": "Dense video captioning aims to identify the events of interest in an input video, and generate descriptive captions for each event. Previous approaches usually follow a two-stage generative process, which first proposes a segment for each event, then renders a caption for each identified segment. Recent advances in large-scale sequence generation pretraining have seen great success in unifying task formulation for a great variety of tasks, but so far, more complex tasks such as dense video captioning are not able to fully utilize this powerful paradigm. In this work, we show how to model the two subtasks of dense video captioning jointly as one sequence generation task, and simultaneously predict the events and the corresponding descriptions. Experiments on YouCook2 and ViTT show encouraging results and indicate the feasibility of training complex tasks such as end-to-end dense video captioning integrated into large-scale pretrained models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SANCL: Multimodal Review Helpfulness Prediction with Selective Attention and Natural Contrastive Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.499/",
        "paper_authors": [
            "Wei Han",
            "Hui Chen",
            "Zhen Hai",
            "Soujanya Poria",
            "Lidong Bing"
        ],
        "paper_abstract": "With the boom of e-commerce, Multimodal Review Helpfulness Prediction (MRHP) that identifies the helpfulness score of multimodal product reviews has become a research hotspot. Previous work on this task focuses on attention-based modality fusion, information integration, and relation modeling, which primarily exposes the following drawbacks: 1) the model may fail to capture the really essential information due to its indiscriminate attention formulation; 2) lack appropriate modeling methods that takes full advantage of correlation among provided data. In this paper, we propose SANCL: Selective Attention and Natural Contrastive Learning for MRHP. SANCL adopts a probe-based strategy to enforce high attention weights on the regions of greater significance. It also constructs a contrastive learning framework based on natural matching properties in the dataset. Experimental results on two benchmark datasets with three categories show that SANCL achieves state-of-the-art baseline performance with lower memory consumption.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.500/",
        "paper_authors": [
            "Weidong Tian",
            "Haodong Li",
            "Zhong-Qiu Zhao"
        ],
        "paper_abstract": "A Visual Question Answering (VQA) model processes images and questions simultaneously with rich semantic information. The attention mechanism can highlight fine-grained features with critical information, thus ensuring that feature extraction emphasizes the objects related to the questions. However, unattended coarse-grained information is also essential for questions involving global elements. We believe that global coarse-grained information and local fine-grained information can complement each other to provide richer comprehensive information. In this paper, we propose a dual capsule attention mask network with mutual learning for VQA. Specifically, it contains two branches processing coarse-grained features and fine-grained features, respectively. We also design a novel stackable dual capsule attention module to fuse features and locate evidence. The two branches are combined to make final predictions for VQA. Experimental results show that our method outperforms the baselines in terms of VQA performance and interpretability and achieves new SOTA performance on the VQA-v2 dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Emergence of Hierarchical Reference Systems in Multi-agent Communication",
        "paper_url": "https://aclanthology.org/2022.coling-1.501/",
        "paper_authors": [
            "Xenia Ohmer",
            "Marko Duda",
            "Elia Bruni"
        ],
        "paper_abstract": "In natural language, referencing objects at different levels of specificity is a fundamental pragmatic mechanism for efficient communication in context. We develop a novel communication game, the hierarchical reference game, to study the emergence of such reference systems in artificial agents. We consider a simplified world, in which concepts are abstractions over a set of primitive attributes (e.g., color, style, shape). Depending on how many attributes are combined, concepts are more general (\u201ccircle\u201d) or more specific (\u201cred dotted circle\u201d). Based on the context, the agents have to communicate at different levels of this hierarchy. Our results show that the agents learn to play the game successfully and can even generalize to novel concepts. To achieve abstraction, they use implicit (omitting irrelevant information) and explicit (indicating that attributes are irrelevant) strategies. In addition, the compositional structure underlying the concept hierarchy is reflected in the emergent protocols, indicating that the need to develop hierarchical reference systems supports the emergence of compositionality.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Scene Graph Modification as Incremental Structure Expanding",
        "paper_url": "https://aclanthology.org/2022.coling-1.502/",
        "paper_authors": [
            "Xuming Hu",
            "Zhijiang Guo",
            "Yu Fu",
            "Lijie Wen",
            "Philip S. Yu"
        ],
        "paper_abstract": "A scene graph is a semantic representation that expresses the objects, attributes, and relationships between objects in a scene. Scene graphs play an important role in many cross modality tasks, as they are able to capture the interactions between images and texts. In this paper, we focus on scene graph modification (SGM), where the system is required to learn how to update an existing scene graph based on a natural language query. Unlike previous approaches that rebuilt the entire scene graph, we frame SGM as a graph expansion task by introducing the incremental structure expanding (ISE). ISE constructs the target graph by incrementally expanding the source graph without changing the unmodified structure. Based on ISE, we further propose a model that iterates between nodes prediction and edges prediction, inferring more accurate and harmonious expansion decisions progressively. In addition, we construct a challenging dataset that contains more complicated queries and larger scene graphs than existing datasets. Experiments on four benchmarks demonstrate the effectiveness of our approach, which surpasses the previous state-of-the-art model by large margins.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances",
        "paper_url": "https://aclanthology.org/2022.coling-1.503/",
        "paper_authors": [
            "Yike Wu",
            "Yu Zhao",
            "Shiwan Zhao",
            "Ying Zhang",
            "Xiaojie Yuan",
            "Guoqing Zhao",
            "Ning Jiang"
        ],
        "paper_abstract": "Despite the great progress of Visual Question Answering (VQA), current VQA models heavily rely on the superficial correlation between the question type and its corresponding frequent answers (i.e., language priors) to make predictions, without really understanding the input. In this work, we define the training instances with the same question type but different answers as superficially similar instances, and attribute the language priors to the confusion of VQA model on such instances. To solve this problem, we propose a novel training framework that explicitly encourages the VQA model to distinguish between the superficially similar instances. Specifically, for each training instance, we first construct a set that contains its superficially similar counterparts. Then we exploit the proposed distinguishing module to increase the distance between the instance and its counterparts in the answer space. In this way, the VQA model is forced to further focus on the other parts of the input beyond the question type, which helps to overcome the language priors. Experimental results show that our method achieves the state-of-the-art performance on VQA-CP v2. Codes are available at Distinguishing-VQA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Efficient Multilingual Multi-modal Pre-training through Triple Contrastive Loss",
        "paper_url": "https://aclanthology.org/2022.coling-1.504/",
        "paper_authors": [
            "Youhan Lee",
            "KyungTae Lim",
            "Woonhyuk Baek",
            "Byungseok Roh",
            "Saehoon Kim"
        ],
        "paper_abstract": "Learning visual and textual representations in the shared space from web-scale image-text pairs improves the performance of diverse vision-and-language tasks, as well as modality-specific tasks. Many attempts in this framework have been made to connect English-only texts and images, and only a few works have been proposed to extend this framework in multilingual settings with the help of many translation pairs. In this multilingual approach, a typical setup is to use pairs of (image and English-text) and translation pairs. The major limitation of this approach is that the learning signal of aligning visual representation with under-resourced language representation is not strong, achieving a sub-optimal performance of vision-and-language tasks. In this work, we propose a simple yet effective enhancement scheme for previous multilingual multi-modal representation methods by using a limited number of pairs of images and non-English texts. In specific, our scheme fine-tunes a pre-trained multilingual model by minimizing a triplet contrastive loss on triplets of image and two different language texts with the same meaning, improving the connection between images and non-English texts. Experiments confirm that our enhancement strategy achieves performance gains in image-text retrieval, zero-shot image classification, and sentence embedding tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation",
        "paper_url": "https://aclanthology.org/2022.coling-1.505/",
        "paper_authors": [
            "Yue Zhang",
            "Parisa Kordjamshidi"
        ],
        "paper_abstract": "Understanding spatial and visual information is essential for a navigation agent who follows natural language instructions. The current Transformer-based VLN agents entangle the orientation and vision information, which limits the gain from the learning of each information source. In this paper, we design a neural agent with explicit Orientation and Vision modules. Those modules learn to ground spatial information and landmark mentions in the instructions to the visual environment more effectively. To strengthen the spatial reasoning and visual perception of the agent, we design specific pre-training tasks to feed and better utilize the corresponding modules in our final navigation model. We evaluate our approach on both Room2room (R2R) and Room4room (R4R) datasets and achieve the state of the art results on both benchmarks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.506/",
        "paper_authors": [
            "Anthony Colas",
            "Mehrdad Alvandipour",
            "Daisy Zhe Wang"
        ],
        "paper_abstract": "Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in a graph.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Content Type Profiling of Data-to-Text Generation Datasets",
        "paper_url": "https://aclanthology.org/2022.coling-1.507/",
        "paper_authors": [
            "Ashish Upadhyay",
            "Stewart Massie"
        ],
        "paper_abstract": "Data-to-Text Generation (D2T) problems can be considered as a stream of time-stamped events with a text summary being produced for each. The problem becomes more challenging when event summaries contain complex insights derived from multiple records either within an event, or across several events from the event stream. It is important to understand the different types of content present in the summary to help us better define the system requirements so that we can build better systems. In this paper, we propose a novel typology of content types, that we use to classify the contents of event summaries. Using the typology, a profile of a dataset is generated as the distribution of the aggregated content types which captures the specific characteristics of the dataset and gives a measure of the complexity present in the problem. Extensive experimentation on different D2T datasets is performed and these demonstrate that neural systems struggle in generating contents of complex types.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CoLo: A Contrastive Learning Based Re-ranking Framework for One-Stage Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.508/",
        "paper_authors": [
            "Chenxin An",
            "Ming Zhong",
            "Zhiyong Wu",
            "Qin Zhu",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "paper_abstract": "Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a Contrastive Learning based re-ranking framework for one-stage summarization called CoLo. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that CoLo boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 GPU training hours and obtaining 3x 8x speed-up ratio during inference while maintaining comparable results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.509/",
        "paper_authors": [
            "Cyril Chhun",
            "Pierre Colombo",
            "Fabian M. Suchanek",
            "Chlo\u00e9 Clavel"
        ],
        "paper_abstract": "Research on Automatic Story Generation (ASG) relies heavily on human and automatic evaluation. However, there is no consensus on which human evaluation criteria to use, and no analysis of how well automatic criteria correlate with them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a set of 6 orthogonal and comprehensive human criteria, carefully motivated by the social sciences literature. We also present HANNA, an annotated dataset of 1,056 stories produced by 10 different ASG systems. HANNA allows us to quantitatively evaluate the correlations of 72 automatic metrics with human criteria. Our analysis highlights the weaknesses of current metrics for ASG and allows us to formulate practical recommendations for ASG evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Selective Token Generation for Few-shot Natural Language Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.510/",
        "paper_authors": [
            "Daejin Jo",
            "Taehwan Kwon",
            "Eun-Sol Kim",
            "Sungwoong Kim"
        ],
        "paper_abstract": "Natural language modeling with limited training data is a challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization ability. Among them, additive learning that incorporates a task-specific adapter on top of the fixed large-scale PLM has been popularly used in the few-shot setting. However, this added adapter is still easy to disregard the knowledge of the PLM especially for few-shot natural language generation (NLG) since an entire sequence is usually generated by only the newly trained adapter. Therefore, in this work, we develop a novel additive learning algorithm based on reinforcement learning (RL) that selectively outputs language tokens between the task-general PLM and the task-specific adapter during both training and inference. This output token selection over the two generators allows the adapter to take into account solely the task-relevant parts in sequence generation, and therefore makes it more robust to overfitting as well as more stable in RL training. In addition, to obtain the complementary adapter from the PLM for each few-shot task, we exploit a separate selecting module that is also simultaneously trained using RL. Experimental results on various few-shot NLG tasks including question answering, data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A-TIP: Attribute-aware Text Infilling via Pre-trained Language Model",
        "paper_url": "https://aclanthology.org/2022.coling-1.511/",
        "paper_authors": [
            "Dongyuan Li",
            "Jingyi You",
            "Kotaro Funakoshi",
            "Manabu Okumura"
        ],
        "paper_abstract": "Text infilling aims to restore incomplete texts by filling in blanks, which has attracted more attention recently because of its wide application in ancient text restoration and text rewriting. However, attribute- aware text infilling is yet to be explored, and existing methods seldom focus on the infilling length of each blank or the number/location of blanks. In this paper, we propose an Attribute-aware Text Infilling method via a Pre-trained language model (A-TIP), which contains a text infilling component and a plug- and-play discriminator. Specifically, we first design a unified text infilling component with modified attention mechanisms and intra- and inter-blank positional encoding to better perceive the number of blanks and the infilling length for each blank. Then, we propose a plug-and-play discriminator to guide generation towards the direction of improving attribute relevance without decreasing text fluency. Finally, automatic and human evaluations on three open-source datasets indicate that A-TIP achieves state-of- the-art performance compared with all baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi Graph Neural Network for Extractive Long Document Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.512/",
        "paper_authors": [
            "Xuan-Dung Doan",
            "Le-Minh Nguyen",
            "Khac-Hoai Nam Bui"
        ],
        "paper_abstract": "Heterogeneous Graph Neural Networks (HeterGNN) have been recently introduced as an emergent approach for extracting document summarization (EDS) by exploiting the cross-relations between words and sentences. However, applying HeterGNN for long documents is still an open research issue. One of the main majors is the lacking of inter-sentence connections. In this regard, this paper exploits how to apply HeterGNN for long documents by building a graph on sentence-level nodes (homogeneous graph) and combine with HeterGNN for capturing the semantic information in terms of both inter and intra-sentence connections. Experiments on two benchmark datasets of long documents such as PubMed and ArXiv show that our method is able to achieve state-of-the-art results in this research field.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Zero-Shot Multilingual Text Generation via Iterative Distillation",
        "paper_url": "https://aclanthology.org/2022.coling-1.513/",
        "paper_authors": [
            "Ernie Chang",
            "Alex Marin",
            "Vera Demberg"
        ],
        "paper_abstract": "The demand for multilingual dialogue systems often requires a costly labeling process, where human translators derive utterances in low resource languages from resource rich language annotation. To this end, we explore leveraging the inductive biases for target languages learned by numerous pretrained teacher models by transferring them to student models via sequence-level knowledge distillation. By assuming no target language text, the both the teacher and student models need to learn from the target distribution in a few/zero-shot manner. On the MultiATIS++ benchmark, we explore the effectiveness of our proposed technique to derive the multilingual text for 6 languages, using only the monolingual English data and the pretrained models. We show that training on the synthetic multilingual generation outputs yields close performance to training on human annotations in both slot F1 and intent accuracy; the synthetic text also scores high in naturalness and correctness based on human evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Using Structured Content Plans for Fine-grained Syntactic Control in Pretrained Language Model Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.514/",
        "paper_authors": [
            "Fei-Tzin Lee",
            "Miguel Ballesteros",
            "Feng Nan",
            "Kathleen McKeown"
        ],
        "paper_abstract": "Large pretrained language models offer powerful generation capabilities, but cannot be reliably controlled at a sub-sentential level. We propose to make such fine-grained control possible in pretrained LMs by generating text directly from a semantic representation, Abstract Meaning Representation (AMR), which is augmented at the node level with syntactic control tags. We experiment with English-language generation of three modes of syntax relevant to the framing of a sentence - verb voice, verb tense, and realization of human entities - and demonstrate that they can be reliably controlled, even in settings that diverge drastically from the training distribution. These syntactic aspects contribute to how information is framed in text, something that is important for applications such as summarization which aim to highlight salient information.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PrefScore: Pairwise Preference Learning for Reference-free Summarization Quality Assessment",
        "paper_url": "https://aclanthology.org/2022.coling-1.515/",
        "paper_authors": [
            "Ge Luo",
            "Hebi Li",
            "Youbiao He",
            "Forrest Sheng Bao"
        ],
        "paper_abstract": "Evaluating machine-generated summaries without a human-written reference summary has been a need for a long time. Inspired by preference labeling in existing work of summarization evaluation, we propose to judge summary quality by learning the preference rank of summaries using the Bradley-Terry power ranking model from inferior summaries generated by corrupting base summaries. Extensive experiments on several datasets show that our weakly supervised scheme can produce scores highly correlated with human ratings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Attribute Controlled Text Generation with Contrastive-Generator and External-Discriminator",
        "paper_url": "https://aclanthology.org/2022.coling-1.516/",
        "paper_authors": [
            "Guisheng Liu",
            "Yi Li",
            "Yanqing Guo",
            "Xiangyang Luo",
            "Bo Wang"
        ],
        "paper_abstract": "Though existing researches have achieved impressive results in controlled text generation, they focus mainly on single-attribute control. However, in applications like automatic comments, the topic and sentiment need to be controlled simultaneously. In this work, we propose a new framework for multi-attribute controlled text generation. To achieve this, we design a contrastive-generator that can effectively generate texts with more attributes. In order to increase the convergence of the text on the desired attributes, we adopt an external-discriminator to distinguish whether the generated text holds the desired attributes. Moreover, we propose top-n weighted decoding to further improve the relevance of texts to attributes. Automated evaluations and human evaluations show that our framework achieves remarkable controllability in multi-attribute generation while keeping the text fluent and diverse. It also yields promising performance on zero-shot generation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Coordination Generation via Synchronized Text-Infilling",
        "paper_url": "https://aclanthology.org/2022.coling-1.517/",
        "paper_authors": [
            "Hiroki Teranishi",
            "Yuji Matsumoto"
        ],
        "paper_abstract": "Generating synthetic data for supervised learning from large-scale pre-trained language models has enhanced performances across several NLP tasks, especially in low-resource scenarios. In particular, many studies of data augmentation employ masked language models to replace words with other words in a sentence. However, most of them are evaluated on sentence classification tasks and cannot immediately be applied to tasks related to the sentence structure. In this paper, we propose a simple yet effective approach to generating sentences with a coordinate structure in which the boundaries of its conjuncts are explicitly specified. For a given span in a sentence, our method embeds a mask with a coordinating conjunction in two ways (\u201dX and [mask]\u201d, \u201d[mask] and X\u201d) and forces masked language models to fill the two blanks with an identical text. To achieve this, we introduce decoding methods for BERT and T5 models with the constraint that predictions for different masks are synchronized. Furthermore, we develop a training framework that effectively selects synthetic examples for the supervised coordination disambiguation task. We demonstrate that our method produces promising coordination instances that provide gains for the task in low-resource settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "KHANQ: A Dataset for Generating Deep Questions in Education",
        "paper_url": "https://aclanthology.org/2022.coling-1.518/",
        "paper_authors": [
            "Huanli Gong",
            "Liangming Pan",
            "Hengchang Hu"
        ],
        "paper_abstract": "Designing in-depth educational questions is a time-consuming and cognitively demanding task. Therefore, it is intriguing to study how to build Question Generation (QG) models to automate the question creation process. However, existing QG datasets are not suitable for educational question generation because the questions are not real questions asked by humans during learning and can be solved by simply searching for information. To bridge this gap, we present KHANQ, a challenging dataset for educational question generation, containing 1,034 high-quality learner-generated questions seeking an in-depth understanding of the taught online courses in Khan Academy. Each data sample is carefully paraphrased and annotated as a triple of 1) Context: an independent paragraph on which the question is based; 2) Prompt: a text prompt for the question (e.g., the learner\u2019s background knowledge); 3) Question: a deep question based on Context and coherent with Prompt. By conducting a human evaluation on the aspects of appropriateness, coverage, coherence, and complexity, we show that state-of-the-art QG models which perform well on shallow question generation datasets have difficulty in generating useful educational questions. This makes KHANQ a challenging testbed for educational question generation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Figurative Language Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.519/",
        "paper_authors": [
            "Huiyuan Lai",
            "Malvina Nissim"
        ],
        "paper_abstract": "Figurative language generation is the task of reformulating a given text in the desired figure of speech while still being faithful to the original context. We take the first step towards multi-figurative language modelling by providing a benchmark for the automatic generation of five common figurative forms in English. We train mFLAG employing a scheme for multi-figurative language pre-training on top of BART, and a mechanism for injecting the target figurative information into the encoder; this enables the generation of text with the target figurative form from another figurative form without parallel figurative-figurative sentence pairs. Our approach outperforms all strong baselines. We also offer some qualitative analysis and reflections on the relationship between the different figures of speech.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enhancing Task-Specific Distillation in Small Data Regimes through Language Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.520/",
        "paper_authors": [
            "Husam Quteineh",
            "Spyridon Samothrakis",
            "Richard Sutcliffe"
        ],
        "paper_abstract": "Large-scale pretrained language models have led to significant improvements in Natural Language Processing. Unfortunately, they come at the cost of high computational and storage requirements that complicate their deployment on low-resource devices. This issue can be addressed by distilling knowledge from larger models to smaller ones through pseudo-labels on task-specific datasets. However, this can be difficult for tasks with very limited data. To overcome this challenge, we present a novel approach where knowledge can be distilled from a teacher model to a student model through the generation of synthetic data. For this to be done, we first fine-tune the teacher and student models, as well as a Natural Language Generation (NLG) model, on the target task dataset. We then let both student and teacher work together to condition the NLG model to generate examples that can enhance the performance of the student. We tested our approach on two data generation methods: a) Targeted generation using the Monte Carlo Tree Search (MCTS) algorithm, and b) A Non-Targeted Text Generation (NTTG) method. We evaluate the effectiveness of our approaches against a baseline that uses the BERT model for data augmentation through random word replacement. By testing this approach on the SST-2, MRPC, YELP-2, DBpedia, and TREC-6 datasets, we consistently witnessed considerable improvements over the word-replacement baseline.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Boosting Code Summarization by Embedding Code Structures",
        "paper_url": "https://aclanthology.org/2022.coling-1.521/",
        "paper_authors": [
            "Jikyoeng Son",
            "Joonghyuk Hahn",
            "HyeonTae Seo",
            "Yo-Sub Han"
        ],
        "paper_abstract": "Recent research on code summarization relies on the structural information from the abstract syntax tree (AST) of source codes. It is, however, questionable whether it is the most effective to use AST for expressing the structural information. We find that a program dependency graph (PDG) can represent the structure of a code more effectively. We propose PDG Boosting Module (PBM) that encodes PDG into graph embedding and the framework to implement the proposed PBM with the existing models. PBM achieves improvements of 6.67% (BLEU) and 7.47% (ROUGE) on average. We then analyze the experimental results, and examine how PBM helps the training of baseline models and its performance robustness. For the validation of robustness, we measure the performance of an out-of-domain benchmark dataset, and confirm its robustness. In addition, we apply a new evaluation measure, SBERT score, to evaluate the semantic performance. The models implemented with PBM improve the performance of SBERT score. This implies that they generate summaries that are semantically more similar to the reference summary.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Comparative Graph-based Summarization of Scientific Papers Guided by Comparative Citations",
        "paper_url": "https://aclanthology.org/2022.coling-1.522/",
        "paper_authors": [
            "Jingqiang Chen",
            "Chaoxiang Cai",
            "Xiaorui Jiang",
            "Kejia Chen"
        ],
        "paper_abstract": "With the rapid growth of scientific papers, understanding the changes and trends in a research area is rather time-consuming. The first challenge is to find related and comparable articles for the research. Comparative citations compare co-cited papers in a citation sentence and can serve as good guidance for researchers to track a research area. We thus go through comparative citations to find comparable objects and build a comparative scientific summarization corpus (CSSC). And then, we propose the comparative graph-based summarization (CGSUM) method to create comparative summaries using citations as guidance. The comparative graph is constructed using sentences as nodes and three different relationships of sentences as edges. The relationship that sentences occur in the same paper is used to calculate the salience of sentences, the relationship that sentences occur in two different papers is used to calculate the difference between sentences, and the relationship that sentences are related to citations is used to calculate the commonality of sentences. Experiments show that CGSUM outperforms comparative baselines on CSSC and performs well on DUC2006 and DUC2007.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "JPG - Jointly Learn to Align: Automated Disease Prediction and Radiology Report Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.523/",
        "paper_authors": [
            "Jingyi You",
            "Dongyuan Li",
            "Manabu Okumura",
            "Kenji Suzuki"
        ],
        "paper_abstract": "Automated radiology report generation aims to generate paragraphs that describe fine-grained visual differences among cases, especially those between the normal and the diseased. Existing methods seldom consider the cross-modal alignment between textual and visual features and tend to ignore disease tags as an auxiliary for report generation. To bridge the gap between textual and visual information, in this study, we propose a \u201cJointly learning framework for automated disease Prediction and radiology report Generation (JPG)\u201d to improve the quality of reports through the interaction between the main task (report generation) and two auxiliary tasks (feature alignment and disease prediction). The feature alignment and disease prediction help the model learn text-correlated visual features and record diseases as keywords so that it can output high-quality reports. Besides, the improved reports in turn provide additional harder samples for feature alignment and disease prediction to learn more precise visual and textual representations and improve prediction accuracy. All components are jointly trained in a manner that helps improve them iteratively and progressively. Experimental results demonstrate the effectiveness of JPG on the most commonly used IU X-RAY dataset, showing its superior performance over multiple state-of-the-art image captioning and medical report generation methods with regard to BLEU, METEOR, and ROUGE metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automatic Nominalization of Clauses through Textual Entailment",
        "paper_url": "https://aclanthology.org/2022.coling-1.524/",
        "paper_authors": [
            "John S. Y. Lee",
            "Ho Hung Lim",
            "Carol Webster",
            "Anton Melser"
        ],
        "paper_abstract": "Nominalization re-writes a clause as a noun phrase. It requires the transformation of the head verb of the clause into a deverbal noun, and the verb\u2019s modifiers into nominal modifiers. Past research has focused on the selection of deverbal nouns, but has paid less attention to predicting the word positions and word forms for the nominal modifiers. We propose the use of a textual entailment model for clause nominalization. We obtained the best performance by fine-tuning a textual entailment model on this task, outperforming a number of unsupervised approaches using language model scores from a state-of-the-art neural language model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Benchmarking Compositionality with Formal Languages",
        "paper_url": "https://aclanthology.org/2022.coling-1.525/",
        "paper_authors": [
            "Josef Valvoda",
            "Naomi Saphra",
            "Jonathan Rawski",
            "Adina Williams",
            "Ryan Cotterell"
        ],
        "paper_abstract": "Recombining known primitive concepts into larger novel combinations is a quintessentially human cognitive capability. Whether large neural models in NLP acquire this ability while learning from data is an open question. In this paper, we look at this problem from the perspective of formal languages. We use deterministic finite-state transducers to make an unbounded number of datasets with controllable properties governing compositionality. By randomly sampling over many transducers, we explore which of their properties (number of states, alphabet size, number of transitions etc.) contribute to learnability of a compositional relation by a neural network. In general, we find that the models either learn the relations completely or not at all. The key is transition coverage, setting a soft learnability limit at 400 examples per transition.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Source-summary Entity Aggregation in Abstractive Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.526/",
        "paper_authors": [
            "Jos\u00e9 \u00c1ngel Gonz\u00e1lez",
            "Annie Louis",
            "Jackie Chi Kit Cheung"
        ],
        "paper_abstract": "In a text, entities mentioned earlier can be referred to in later discourse by a more general description. For example, Celine Dion and Justin Bieber can be referred to by Canadian singers or celebrities. In this work, we study this phenomenon in the context of summarization, where entities from a source text are generalized in the summary. We call such instances source-summary entity aggregations. We categorize these aggregations into two types and analyze them in the Cnn/Dailymail corpus, showing that they are reasonably frequent. We then examine how well three state-of-the-art summarization systems can generate such aggregations within summaries. We also develop techniques to encourage them to generate more aggregations. Our results show that there is significant room for improvement in producing semantically correct aggregations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation",
        "paper_url": "https://aclanthology.org/2022.coling-1.527/",
        "paper_authors": [
            "Julius Steen",
            "Katja Markert"
        ],
        "paper_abstract": "Automatically evaluating the coherence of summaries is of great significance both to enable cost-efficient summarizer evaluation and as a tool for improving coherence by selecting high-scoring candidate summaries. While many different approaches have been suggested to model summary coherence, they are often evaluated using disparate datasets and metrics. This makes it difficult to understand their relative performance and identify ways forward towards better summary coherence modelling. In this work, we conduct a large-scale investigation of various methods for summary coherence modelling on an even playing field. Additionally, we introduce two novel analysis measures, _intra-system correlation_ and _bias matrices_, that help identify biases in coherence measures and provide robustness against system-level confounders. While none of the currently available automatic coherence measures are able to assign reliable coherence scores to system summaries across all evaluation metrics, large-scale language models fine-tuned on self-supervised tasks show promising results, as long as fine-tuning takes into account that they need to generalize across different summary lengths.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Summarizing Dialogues with Negative Cues",
        "paper_url": "https://aclanthology.org/2022.coling-1.528/",
        "paper_authors": [
            "Junpeng Liu",
            "Yanyan Zou",
            "Yuxuan Xi",
            "Shengjie Li",
            "Mian Ma",
            "Zhuoye Ding"
        ],
        "paper_abstract": "Abstractive dialogue summarization aims to convert a long dialogue content into its short form where the salient information is preserved while the redundant pieces are ignored. Different from the well-structured text, such as news and scientific articles, dialogues often consist of utterances coming from two or more interlocutors, where the conversations are often informal, verbose, and repetitive, sprinkled with false-starts, backchanneling, reconfirmations, hesitations, speaker interruptions and the salient information is often scattered across the whole chat. The above properties of conversations make it difficult to directly concentrate on scattered outstanding utterances and thus present new challenges of summarizing dialogues. In this work, rather than directly forcing a summarization system to merely pay more attention to the salient pieces, we propose to explicitly have the model perceive the redundant parts of an input dialogue history during the training phase. To be specific, we design two strategies to construct examples without salient pieces as negative cues. Then, the sequence-to-sequence likelihood loss is cooperated with the unlikelihood objective to drive the model to focus less on the unimportant information and also pay more attention to the salient pieces. Extensive experiments on the benchmark dataset demonstrate that our simple method significantly outperforms the baselines with regard to both semantic matching and factual consistent based metrics. The human evaluation also proves the performance gains.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification",
        "paper_url": "https://aclanthology.org/2022.coling-1.529/",
        "paper_authors": [
            "Kai North",
            "Marcos Zampieri",
            "Tharindu Ranasinghe"
        ],
        "paper_abstract": "Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their potential substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS-ES protocol for Spanish opening exciting new avenues for cross-lingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated three models for substitute generation on this dataset, namely mBERT, XLM-R, and BERTimbau. The latter achieved the highest performance across all evaluation metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "APPDIA: A Discourse-aware Transformer-based Style Transfer Model for Offensive Social Media Conversations",
        "paper_url": "https://aclanthology.org/2022.coling-1.530/",
        "paper_authors": [
            "Katherine Atwell",
            "Sabit Hassan",
            "Malihe Alikhani"
        ],
        "paper_abstract": "Using style-transfer models to reduce offensiveness of social media comments can help foster a more inclusive environment. However, there are no sizable datasets that contain offensive texts and their inoffensive counterparts, and fine-tuning pretrained models with limited labeled data can lead to the loss of original meaning in the style-transferred text. To address this issue, we provide two major contributions. First, we release the first publicly-available, parallel corpus of offensive Reddit comments and their style-transferred counterparts annotated by expert sociolinguists. Then, we introduce the first discourse-aware style-transfer models that can effectively reduce offensiveness in Reddit text while preserving the meaning of the original text. These models are the first to examine inferential links between the comment and the text it is replying to when transferring the style of offensive Reddit text. We propose two different methods of integrating discourse relations with pretrained transformer models and evaluate them on our dataset of offensive comments from Reddit and their inoffensive counterparts. Improvements over the baseline with respect to both automatic metrics and human evaluation indicate that our discourse-aware models are better at preserving meaning in style-transferred text when compared to the state-of-the-art discourse-agnostic models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "View Dialogue in 2D: A Two-stream Model in Time-speaker Perspective for Dialogue Summarization and beyond",
        "paper_url": "https://aclanthology.org/2022.coling-1.531/",
        "paper_authors": [
            "Keli Xie",
            "Dongchen He",
            "Jiaxin Zhuang",
            "Siyuan Lu",
            "Zhongfeng Wang"
        ],
        "paper_abstract": "Existing works on dialogue summarization often follow the common practice in document summarization and view the dialogue, which comprises utterances of different speakers, as a single utterance stream ordered by time. However, this single-stream approach without specific attention to the speaker-centered points has limitations in fully understanding the dialogue. To better capture the dialogue information, we propose a 2D view of dialogue based on a time-speaker perspective, where the time and speaker streams of dialogue can be obtained as strengthened input. Based on this 2D view, we present an effective two-stream model called ATM to combine the two streams. Extensive experiments on various summarization datasets demonstrate that ATM significantly surpasses other models regarding diverse metrics and beats the state-of-the-art models on the QMSum dataset in ROUGE scores. Besides, ATM achieves great improvements in summary faithfulness and human evaluation. Moreover, results on machine reading comprehension datasets show the generalization ability of the proposed methods and shed light on other dialogue-based tasks. Our code will be publicly available online.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Denoising Large-Scale Image Captioning from Alt-text Data Using Content Selection Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.532/",
        "paper_authors": [
            "Khyathi Raghavi Chandu",
            "Piyush Sharma",
            "Soravit Changpinyo",
            "Ashish V. Thapliyal",
            "Radu Soricut"
        ],
        "paper_abstract": "Training large-scale image captioning (IC) models demands access to a rich and diverse set of training examples that are expensive to curate both in terms of time and man-power. Instead, alt-text based captions gathered from the web is a far cheaper alternative to scale with the downside of being noisy. Recent modeling approaches to IC often fall short in terms of performance in leveraging these noisy datasets in favor of clean annotations. We address this problem with a simple yet effective technique of breaking down the task into two smaller, more controllable tasks \u2013 skeleton prediction and skeleton-based caption generation. Specifically, we show that sub-selecting content words as skeletons helps in generating improved and denoised captions when leveraging rich yet noisy alt-text\u2013based uncurated datasets. We also show that the predicted English skeletons can further cross-lingually be leveraged to generate non-English captions, and present experimental results covering caption generation in French, Italian, German, Spanish and Hindi. We also show that skeleton-based prediction allows for better control of certain caption properties, such as length, content, and gender expression, providing a handle to perform human-in-the-loop interpretable semi-automatic corrections.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Meta-CQG: A Meta-Learning Framework for Complex Question Generation over Knowledge Bases",
        "paper_url": "https://aclanthology.org/2022.coling-1.533/",
        "paper_authors": [
            "Kun Zhang",
            "Yunqi Qiu",
            "Yuanzhuo Wang",
            "Long Bai",
            "Wei Li",
            "Xuhui Jiang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "paper_abstract": "Complex question generation over knowledge bases (KB) aims to generate natural language questions involving multiple KB relations or functional constraints. Existing methods train one encoder-decoder-based model to fit all questions. However, such a one-size-fits-all strategy may not perform well since complex questions exhibit an uneven distribution in many dimensions, such as question types, involved KB relations, and query structures, resulting in insufficient learning for long-tailed samples under different dimensions. To address this problem, we propose a meta-learning framework for complex question generation. The meta-trained generator can acquire universal and transferable meta-knowledge and quickly adapt to long-tailed samples through a few most related training samples. To retrieve similar samples for each input query, we design a self-supervised graph retriever to learn distributed representations for samples, and contrastive learning is leveraged to improve the learned representations. We conduct experiments on both WebQuestionsSP and ComplexWebQuestion, and results on long-tailed samples of different dimensions have been significantly improved, which demonstrates the effectiveness of the proposed framework.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Graph-to-Text Generation with Dynamic Structure Pruning",
        "paper_url": "https://aclanthology.org/2022.coling-1.534/",
        "paper_authors": [
            "Liang Li",
            "Ruiying Geng",
            "Bowen Li",
            "Can Ma",
            "Yinliang Yue",
            "Binhua Li",
            "Yongbin Li"
        ],
        "paper_abstract": "Most graph-to-text works are built on the encoder-decoder framework with cross-attention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Perspective Document Revision",
        "paper_url": "https://aclanthology.org/2022.coling-1.535/",
        "paper_authors": [
            "Mana Ihori",
            "Hiroshi Sato",
            "Tomohiro Tanaka",
            "Ryo Masumura"
        ],
        "paper_abstract": "This paper presents a novel multi-perspective document revision task. In conventional studies on document revision, tasks such as grammatical error correction, sentence reordering, and discourse relation classification have been performed individually; however, these tasks simultaneously should be revised to improve the readability and clarity of a whole document. Thus, our study defines multi-perspective document revision as a task that simultaneously revises multiple perspectives. To model the task, we design a novel Japanese multi-perspective document revision dataset that simultaneously handles seven perspectives to improve the readability and clarity of a document. Although a large amount of data that simultaneously handles multiple perspectives is needed to model multi-perspective document revision elaborately, it is difficult to prepare such a large amount of this data. Therefore, our study offers a multi-perspective document revision modeling method that can use a limited amount of matched data (i.e., data for the multi-perspective document revision task) and external partially-matched data (e.g., data for the grammatical error correction task). Experiments using our created dataset demonstrate the effectiveness of using multiple partially-matched datasets to model the multi-perspective document revision task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Survey of Automatic Text Summarization Using Graph Neural Networks",
        "paper_url": "https://aclanthology.org/2022.coling-1.536/",
        "paper_authors": [
            "Marco Ferdinand Salchner",
            "Adam Jatowt"
        ],
        "paper_abstract": "Although automatic text summarization (ATS) has been researched for several decades, the application of graph neural networks (GNNs) to this task started relatively recently. In this survey we provide an overview on the rapidly evolving approach of using GNNs for the task of automatic text summarization. In particular we provide detailed information on the functionality of GNNs in the context of ATS, and a comprehensive overview of models utilizing this approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Phrase-Level Localization of Inconsistency Errors in Summarization by Weak Supervision",
        "paper_url": "https://aclanthology.org/2022.coling-1.537/",
        "paper_authors": [
            "Masato Takatsuka",
            "Tetsunori Kobayashi",
            "Yoshihiko Hayashi"
        ],
        "paper_abstract": "Although the fluency of automatically generated abstractive summaries has improved significantly with advanced methods, the inconsistency that remains in summarization is recognized as an issue to be addressed. In this study, we propose a methodology for localizing inconsistency errors in summarization. A synthetic dataset that contains a variety of factual errors likely to be produced by a common summarizer is created by applying sentence fusion, compression, and paraphrasing operations. In creating the dataset, we automatically label erroneous phrases and the dependency relations between them as \u201cinconsistent,\u201d which can contribute to detecting errors more adequately than existing models that rely only on dependency arc-level labels. Subsequently, this synthetic dataset is employed as weak supervision to train a model called SumPhrase, which jointly localizes errors in a summary and their corresponding sentences in the source document. The empirical results demonstrate that our SumPhrase model can detect factual errors in summarization more effectively than existing weakly supervised methods owing to the phrase-level labeling. Moreover, the joint identification of error-corresponding original sentences is proven to be effective in improving error detection accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PoliSe: Reinforcing Politeness Using User Sentiment for Customer Care Response Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.538/",
        "paper_authors": [
            "Mauajama Firdaus",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "The interaction between a consumer and the customer service representative greatly contributes to the overall customer experience. Therefore, to ensure customers\u2019 comfort and retention, it is important that customer service agents and chatbots connect with users on social, cordial, and empathetic planes. In the current work, we automatically identify the sentiment of the user and transform the neutral responses into polite responses conforming to the sentiment and the conversational history. Our technique is basically a reinforced multi-task network- the primary task being \u2018polite response generation\u2019 and the secondary task being \u2018sentiment analysis\u2019- that uses a Transformer based encoder-decoder. We use sentiment annotated conversations from Twitter as the training data. The detailed evaluation shows that our proposed approach attains superior performance compared to the baseline models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Focus-Driven Contrastive Learning for Medical Question Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.539/",
        "paper_authors": [
            "Ming Zhang",
            "Shuai Dou",
            "Ziyang Wang",
            "Yunfang Wu"
        ],
        "paper_abstract": "Automatic medical question summarization can significantly help the system to understand consumer health questions and retrieve correct answers. The Seq2Seq model based on maximum likelihood estimation (MLE) has been applied in this task, which faces two general problems: the model can not capture well question focus and and the traditional MLE strategy lacks the ability to understand sentence-level semantics. To alleviate these problems, we propose a novel question focus-driven contrastive learning framework (QFCL). Specially, we propose an easy and effective approach to generate hard negative samples based on the question focus, and exploit contrastive learning at both encoder and decoder to obtain better sentence level representations. On three medical benchmark datasets, our proposed model achieves new state-of-the-art results, and obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline BART model on three datasets respectively. Further human judgement and detailed analysis prove that our QFCL model learns better sentence representations with the ability to distinguish different sentence meanings, and generates high-quality summaries by capturing question focus.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ArgLegalSumm: Improving Abstractive Summarization of Legal Documents with Argument Mining",
        "paper_url": "https://aclanthology.org/2022.coling-1.540/",
        "paper_authors": [
            "Mohamed Elaraby",
            "Diane Litman"
        ],
        "paper_abstract": "A challenging task when generating summaries of legal documents is the ability to address their argumentative nature. We introduce a simple technique to capture the argumentative structure of legal documents by integrating argument role labeling into the summarization process. Experiments with pretrained language models show that our proposed approach improves performance over strong baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantic Overlap Summarization among Multiple Alternative Narratives: An Exploratory Study",
        "paper_url": "https://aclanthology.org/2022.coling-1.541/",
        "paper_authors": [
            "Naman Bansal",
            "Mousumi Akter",
            "Shubhra Kanti Karmaker Santu"
        ],
        "paper_abstract": "In this paper, we introduce an important yet relatively unexplored NLP task called Semantic Overlap Summarization (SOS), which entails generating a single summary from multiple alternative narratives which can convey the common information provided by those narratives. As no benchmark dataset is readily available for this task, we created one by collecting 2,925 alternative narrative pairs from the web and then, went through the tedious process of manually creating 411 different reference summaries by engaging human annotators. As a way to evaluate this novel task, we first conducted a systematic study by borrowing the popular ROUGE metric from text-summarization literature and discovered that ROUGE is not suitable for our task. Subsequently, we conducted further human annotations to create 200 document-level and 1,518 sentence-level ground-truth overlap labels. Our experiments show that the sentence-wise annotation technique with three overlap labels, i.e., Absent (A), Partially-Present (PP), and Present (P), yields a higher correlation with human judgment and higher inter-rater agreement compared to the ROUGE metric.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Analyzing the Dialect Diversity in Multi-document Summaries",
        "paper_url": "https://aclanthology.org/2022.coling-1.542/",
        "paper_authors": [
            "Olubusayo Olabisi",
            "Aaron Hudson",
            "Antonie Jetter",
            "Ameeta Agrawal"
        ],
        "paper_abstract": "Social media posts provide a compelling, yet challenging source of data of diverse perspectives from many socially salient groups. Automatic text summarization algorithms make this data accessible at scale by compressing large collections of documents into short summaries that preserve salient information from the source text. In this work, we take a complementary approach to analyzing and improving the quality of summaries generated from social media data in terms of their ability to represent salient as well as diverse perspectives. We introduce a novel dataset, DivSumm, of dialect diverse tweets and human-written extractive and abstractive summaries. Then, we study the extent of dialect diversity reflected in human-written reference summaries as well as system-generated summaries. The results of our extensive experiments suggest that humans annotate fairly well-balanced dialect diverse summaries, and that cluster-based pre-processing approaches seem beneficial in improving the overall quality of the system-generated summaries without loss in diversity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
        "paper_url": "https://aclanthology.org/2022.coling-1.543/",
        "paper_authors": [
            "Pancheng Wang",
            "Shasha Li",
            "Kunyuan Pang",
            "Liangliang He",
            "Dong Li",
            "Jintao Tang",
            "Ting Wang"
        ],
        "paper_abstract": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generation of Patient After-Visit Summaries to Support Physicians",
        "paper_url": "https://aclanthology.org/2022.coling-1.544/",
        "paper_authors": [
            "Pengshan Cai",
            "Fei Liu",
            "Adarsha Bajracharya",
            "Joe Sills",
            "Alok Kapoor",
            "Weisong Liu",
            "Dan Berlowitz",
            "David Levy",
            "Richeek Pradhan",
            "Hong Yu"
        ],
        "paper_abstract": "An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients\u2019 disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.545/",
        "paper_authors": [
            "Tuan-Anh Phan",
            "Ngoc-Dung Ngoc Nguyen",
            "Khac-Hoai Nam Bui"
        ],
        "paper_abstract": "Graph Neural Network (GNN)-based models have proven effective in various Natural Language Processing (NLP) tasks in recent years. Specifically, in the case of the Extractive Document Summarization (EDS) task, modeling documents under graph structure is able to analyze the complex relations between semantic units (e.g., word-to-word, word-to-sentence, sentence-to-sentence) and enrich sentence representations via valuable information from their neighbors. However, long-form document summarization using graph-based methods is still an open research issue. The main challenge is to represent long documents in a graph structure in an effective way. In this regard, this paper proposes a new heterogeneous graph neural network (HeterGNN) model to improve the performance of long document summarization (HeterGraphLongSum). Specifically, the main idea is to add the passage nodes into the heterogeneous graph structure of word and sentence nodes for enriching the final representation of sentences. In this regard, HeterGraphLongSum is designed with three types of semantic units such as word, sentence, and passage. Experiments on two benchmark datasets for long documents such as Pubmed and Arxiv indicate promising results of the proposed model for the extractive long document summarization problem. Especially, HeterGraphLongSum is able to achieve state-of-the-art performance without relying on any pre-trained language models (e.g., BERT). The source code is available for further exploitation on the Github.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.546/",
        "paper_authors": [
            "Qianqian Xie",
            "Jimin Huang",
            "Tulika Saha",
            "Sophia Ananiadou"
        ],
        "paper_abstract": "Recently, neural topic models (NTMs) have been incorporated into pre-trained language models (PLMs), to capture the global semantic information for text summarization. However, in these methods, there remain limitations in the way they capture and integrate the global semantic information. In this paper, we propose a novel model, the graph contrastive topic enhanced language model (GRETEL), that incorporates the graph contrastive topic model with the pre-trained language model, to fully leverage both the global and local contextual semantics for long document extractive summarization. To better capture and incorporate the global semantic information into PLMs, the graph contrastive topic model integrates the hierarchical transformer encoder and the graph contrastive learning to fuse the semantic information from the global document context and the gold summary. To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics. Experimental results on both general domain and biomedical datasets demonstrate that our proposed method outperforms SOTA methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.547/",
        "paper_authors": [
            "Sedrick Scott Keh",
            "Kevin Lu",
            "Varun Gangal",
            "Steven Y. Feng",
            "Harsh Jhamtani",
            "Malihe Alikhani",
            "Eduard Hovy"
        ],
        "paper_abstract": "A personification is a figure of speech that endows inanimate entities with properties and actions typically seen as requiring animacy. In this paper, we explore the task of personification generation. To this end, we propose PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification data for Learning Enhanced generation. We curate a corpus of personifications called PersonifCorp, together with automatically generated de-personified literalizations of these personifications. We demonstrate the usefulness of this parallel corpus by training a seq2seq model to personify a given literal input. Both automatic and human evaluations show that fine-tuning with PersonifCorp leads to significant gains in personification-related qualities such as animacy and interestingness. A detailed qualitative analysis also highlights key strengths and imperfections of PINEAPPLE over baselines, demonstrating a strong ability to generate diverse and creative personifications that enhance the overall appeal of a sentence.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.548/",
        "paper_authors": [
            "Seungone Kim",
            "Se June Joo",
            "Hyungjoo Chae",
            "Chaehyeong Kim",
            "Seung-won Hwang",
            "Jinyoung Yeo"
        ],
        "paper_abstract": "In this paper, we propose to leverage the unique characteristics of dialogues sharing commonsense knowledge across participants, to resolve the difficulties in summarizing them. We present SICK, a framework that uses commonsense inferences as additional context. Compared to previous work that solely relies on the input dialogue, SICK uses an external knowledge model to generate a rich set of commonsense inferences and selects the most probable one with a similarity-based selection method. Built upon SICK, SICK++ utilizes commonsense as supervision, where the task of generating commonsense inferences is added upon summarizing the dialogue in a multi-task learning setting. Experimental results show that with injected commonsense knowledge, our framework generates more informative and consistent summaries than existing methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Type-dependent Prompt CycleQAG : Cycle Consistency for Multi-hop Question Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.549/",
        "paper_authors": [
            "Seungyeon Lee",
            "Minho Lee"
        ],
        "paper_abstract": "Multi-hop question generation (QG) is the process of generating answer related questions, which requires aggregating multiple pieces of information and reasoning from different parts of the texts. This is opposed to single-hop QG which generates questions from sentences containing an answer in a given paragraph. Single-hop QG requires no reasoning or complexity, while multi-hop QG often requires logical reasoning to derive an answer related question, making it a dual task. Not enough research has been made on the multi-hop QG due to its complexity. Also, a question should be created using the question type and words related to the correct answer as a prompt so that multi-hop questions can get more information. In this view, we propose a new type-dependent prompt cycleQAG (cyclic question-answer-generation), with a cycle consistency loss in which QG and Question Answering (QA) are learnt in a cyclic manner. The novelty is that the cycle consistency loss uses the negative cross entropy to generate syntactically diverse questions that enable selecting different word representations. Empirical evaluation on the multi-hop dataset with automatic and human evaluation metrics outperforms the baseline model by about 10.38% based on ROUGE score.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor",
        "paper_url": "https://aclanthology.org/2022.coling-1.550/",
        "paper_authors": [
            "Shangqing Tu",
            "Jifan Yu",
            "Fangwei Zhu",
            "Juanzi Li",
            "Lei Hou",
            "Jian-Yun Nie"
        ],
        "paper_abstract": "Multi-Document Summarization (MDS) commonly employs the 2-stage extract-then-abstract paradigm, which first extracts a relatively short meta-document, then feeds it into the deep neural networks to generate an abstract. Previous work usually takes the ROUGE score as the label for training a scoring model to evaluate source documents. However, the trained scoring model is prone to under-fitting for low-resource settings, as it relies on the training data. To extract documents effectively, we construct prompting templates that invoke the underlying knowledge in Pre-trained Language Model (PLM) to calculate the document and keyword\u2019s perplexity, which can assess the document\u2019s semantic salience. Our unsupervised approach can be applied as a plug-in to boost other metrics for evaluating a document\u2019s salience, thus improving the subsequent abstract generation. We get positive results on 2 MDS datasets, 2 data settings, and 2 abstractive backbone models, showing our method\u2019s effectiveness. Our code is available at https://github.com/THU-KEG/UPER",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DISK: Domain-constrained Instance Sketch for Math Word Problem Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.551/",
        "paper_authors": [
            "Tianyang Cao",
            "Shuang Zeng",
            "Xiaodan Xu",
            "Mairgup Mansur",
            "Baobao Chang"
        ],
        "paper_abstract": "A math word problem (MWP) is a coherent narrative which reflects the underlying logic of math equations. Successful MWP generation can automate the writing of mathematics questions. Previous methods mainly generate MWP text based on inflexible pre-defined templates. In this paper, we propose a neural model for generating MWP text from math equations. Firstly, we incorporate a matching model conditioned on the domain knowledge to retrieve a MWP instance which is most consistent with the ground-truth, where the domain is a latent variable extracted with a domain summarizer. Secondly, by constructing a Quantity Cell Graph (QCG) from the retrieved MWP instance and reasoning over it, we improve the model\u2019s comprehension of real-world scenarios and derive a domain-constrained instance sketch to guide the generation. Besides, the QCG also interacts with the equation encoder to enhance the alignment between math tokens (e.g., quantities and variables) and MWP text. Experiments and empirical analysis on educational MWP set show that our model achieves impressive performance in both automatic evaluation metrics and human evaluation metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Context-Tuning: Learning Contextualized Prompts for Natural Language Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.552/",
        "paper_authors": [
            "Tianyi Tang",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "paper_abstract": "Recently, pretrained language models (PLMs) have had exceptional success in language generation. To leverage the rich knowledge encoded by PLMs, a simple yet powerful paradigm is to use prompts in the form of either discrete tokens or continuous embeddings. In existing studies, these prompting methods are typically independent of the inputs, lacking sufficient consideration of input semantics. To address this issue, we propose a novel continuous prompting approach, called context-tuning, to fine-tuning PLMs for natural language generation. Firstly, the prompts are derived based on the input text to elicit useful knowledge from PLMs for generation. We refer to such prompts as contextualized prompts. Secondly, we use continuous inverse prompting to improve the process of natural language generation by modeling an inverse generation process from output to input, making the generated text more relevant to the inputs. Furthermore, we utilize a lightweight context-tuning method that fine-tunes only 0.12% of the parameters while maintaining good performance. Our code is publicly available at https://github.com/RUCAIBox/Context-Tuning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.553/",
        "paper_authors": [
            "Xiaochen Liu",
            "Yang Gao",
            "Yu Bai",
            "Jiawei Li",
            "Yinan Hu",
            "Heyan Huang",
            "Boxing Chen"
        ],
        "paper_abstract": "Few-shot abstractive summarization has become a challenging task in natural language generation. To support it, we developed a novel soft prompts architecture coupled with a prompt pre-training plus prompt fine-tuning paradigm, which is effective and tunes only extremely light parameters. To meet the structure of the generation models, the soft prompts comprise continuous input embeddings across an encoder and a decoder. Importantly, a new inner-prompt placed in the text is introduced to capture document-level information. The aim is to devote attention to understanding the document that better prompts the model to generate document-related content. In the training process, the prompt pre-training with self-supervised pseudo-data firstly teaches the model basic summarizing capability. Then, with few-shot examples, only the designed lightweight soft prompts are fine-tuned. Experimental results on the CNN/DailyMail and XSum datasets show that our method, with only 0.1% of the parameters, outperforms full-model tuning where all model parameters are tuned. It also surpasses Prompt Tuning by a large margin and delivers competitive results against Prefix-Tuning with 3% of the parameters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Continuous Decomposition of Granularity for Neural Paraphrase Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.554/",
        "paper_authors": [
            "Xiaodong Gu",
            "Zhaowei Zhang",
            "Sang-Woo Lee",
            "Kang Min Yoo",
            "Jung-Woo Ha"
        ],
        "paper_abstract": "While Transformers have had significant success in paragraph generation, they treat sentences as linear sequences of tokens and often neglect their hierarchical information. Prior work has shown that decomposing the levels of granularity (e.g., word, phrase, or sentence) for input tokens has produced substantial improvements, suggesting the possibility of enhancing Transformers via more fine-grained modeling of granularity. In this work, we present continuous decomposition of granularity for neural paraphrase generation (C-DNPG): an advanced extension of multi-head self-attention with: 1) a granularity head that automatically infers the hierarchical structure of a sentence by neurally estimating the granularity level of each input token; and 2) two novel attention masks, namely, granularity resonance and granularity scope, to efficiently encode granularity into attention. Experiments on two benchmarks, including Quora question pairs and Twitter URLs have shown that C-DNPG outperforms baseline models by a significant margin. Qualitative analysis reveals that C-DNPG indeed captures fine-grained levels of granularity with effectiveness.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Paraphrase Generation as Unsupervised Machine Translation",
        "paper_url": "https://aclanthology.org/2022.coling-1.555/",
        "paper_authors": [
            "Xiaofei Sun",
            "Yufei Tian",
            "Yuxian Meng",
            "Nanyun Peng",
            "Fei Wu",
            "Jiwei Li",
            "Chun Fan"
        ],
        "paper_abstract": "In this paper, we propose a new paradigm for paraphrase generation by treating the task as unsupervised machine translation (UMT) based on the assumption that there must be pairs of sentences expressing the same meaning in a large-scale unlabeled monolingual corpus. The proposed paradigm first splits a large unlabeled corpus into multiple clusters, and trains multiple UMT models using pairs of these clusters. Then based on the paraphrase pairs produced by these UMT models, a unified surrogate model can be trained to serve as the final model to generate paraphrases, which can be directly used for test in the unsupervised setup, or be finetuned on labeled datasets in the supervised setup. The proposed method offers merits over machine-translation-based paraphrase generation methods, as it avoids reliance on bilingual sentence pairs. It also allows human intervene with the model so that more diverse paraphrases can be generated using different filtering criteria. Extensive experiments on existing paraphrase dataset for both the supervised and unsupervised setups demonstrate the effectiveness the proposed paradigm.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries",
        "paper_url": "https://aclanthology.org/2022.coling-1.556/",
        "paper_authors": [
            "Xiaofei Sun",
            "Zijun Sun",
            "Yuxian Meng",
            "Jiwei Li",
            "Chun Fan"
        ],
        "paper_abstract": "The difficulty of generating coherent long texts lies in the fact that existing models overwhelmingly focus on the tasks of local word prediction, and cannot make high level plans on what to generate or capture the high-level discourse dependencies between chunks of texts. Inspired by how humans write, where a list of bullet points or a catalog is first outlined, and then each bullet point is expanded to form the whole article, we propose SOE, a pipelined system that involves of summarizing, outlining and elaborating for long text generation: the model first outlines the summaries for different segments of long texts, and then elaborates on each bullet point to generate the corresponding segment. To avoid the labor-intensive process of summary soliciting, we propose the reconstruction strategy, which extracts segment summaries in an unsupervised manner by selecting its most informative part to reconstruct the segment. The proposed generation system comes with the following merits: (1) the summary provides high-level guidance for text generation and avoids the local minimum of individual word predictions; (2) the high-level discourse dependencies are captured in the conditional dependencies between summaries and are preserved during the summary expansion process and (3) additionally, we are able to consider significantly more contexts by representing contexts as concise summaries. Extensive experiments demonstrate that SOE produces long texts with significantly better quality, along with faster convergence speed.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CoCGAN: Contrastive Learning for Adversarial Category Text Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.557/",
        "paper_authors": [
            "Xin Sheng",
            "Linli Xu",
            "Yinlong Xu",
            "Changcun Bao",
            "Huang Chen",
            "Bo Ren"
        ],
        "paper_abstract": "The task of generating texts of different categories has attracted more and more attention in the area of natural language generation recently. Meanwhile, generative adversarial net (GAN) has demonstrated its effectiveness on text generation, and is further applied to category text generation in later works. Different from existing methods, which mainly consider the pairwise relations between the text embedding and the corresponding fixed one-hot class label (data-to-class relations), this paper proposes a novel Contrastive Category Generative Adversarial Net (CoCGAN) to incorporate contrastive learning into adversarial category text generation, considering more flexible data-to-class relations as well as relations between the multiple text embeddings in the same batch (data-to-data relations). The discriminator of CoCGAN discriminates the authenticity of given samples and optimizes a contrastive learning objective to capture both more flexible data-to-class relations and data-to-data relations among training samples. Accordingly, the generator tries to produce more realistic samples which can confuse the discriminator. Experimental results on both synthetic and real category text generation datasets demonstrate that CoCGAN can achieve significant improvements over the baseline category text generation models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Efficient Coarse-to-Fine Facet-Aware Unsupervised Summarization Framework Based on Semantic Blocks",
        "paper_url": "https://aclanthology.org/2022.coling-1.558/",
        "paper_authors": [
            "Xinnian Liang",
            "Jing Li",
            "Shuangzhi Wu",
            "Jiali Zeng",
            "Yufan Jiang",
            "Mu Li",
            "Zhoujun Li"
        ],
        "paper_abstract": "Unsupervised summarization methods have achieved remarkable results by incorporating representations from pre-trained language models. However, existing methods fail to consider efficiency and effectiveness at the same time when the input document is extremely long. To tackle this problem, in this paper, we proposed an efficient Coarse-to-Fine Facet-Aware Ranking (C2F-FAR) framework for unsupervised long document summarization, which is based on the semantic block. The semantic block refers to continuous sentences in the document that describe the same facet. Specifically, we address this problem by converting the one-step ranking method into the hierarchical multi-granularity two-stage ranking. In the coarse-level stage, we proposed a new segment algorithm to split the document into facet-aware semantic blocks and then filter insignificant blocks. In the fine-level stage, we select salient sentences in each block and then extract the final summary from selected sentences. We evaluate our framework on four long document summarization datasets: Gov-Report, BillSum, arXiv, and PubMed. Our C2F-FAR can achieve new state-of-the-art unsupervised summarization results on Gov-Report and BillSum. In addition, our method speeds up 4-28 times more than previous methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CHAE: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions",
        "paper_url": "https://aclanthology.org/2022.coling-1.559/",
        "paper_authors": [
            "Xinpeng Wang",
            "Han Jiang",
            "Zhihua Wei",
            "Shanlin Zhou"
        ],
        "paper_abstract": "Story generation has emerged as an interesting yet challenging NLP task in recent years. Some existing studies aim at generating fluent and coherent stories from keywords and outlines; while others attempt to control the global features of the story, such as emotion, style and topic. However, these works focus on coarse-grained control on the story, neglecting control on the details of the story, which is also crucial for the task. To fill the gap, this paper proposes a model for fine-grained control on the story, which allows the generation of customized stories with characters, corresponding actions and emotions arbitrarily assigned. Extensive experimental results on both automatic and human manual evaluations show the superiority of our method. It has strong controllability to generate stories according to the fine-grained personalized guidance, unveiling the effectiveness of our methodology. Our code is available at https://github.com/victorup/CHAE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Chinese Couplet Generation with Syntactic Information",
        "paper_url": "https://aclanthology.org/2022.coling-1.560/",
        "paper_authors": [
            "Yan Song"
        ],
        "paper_abstract": "Chinese couplet generation aims to generate a pair of clauses (usually generating a subsequent clause given an antecedent one) with certain rules (e.g., morphological and syntactical symmetry) adhered and has long been a challenging task with cultural background. To generate high-quality couplet (antecedent) clauses, it normally requires a model to learn the correspondences between antecedent and subsequent clauses under aforementioned rules and constraint of few characters with their concise usage. To tackle this task, previous studies normally directly adopt deep neural networks without explicitly taking into account fine-grained analysis of the clauses, in this paper, we propose to enhance Chinese couplet generation by leveraging syntactic information, i.e., part-of-speech (POS) tags and word dependencies. In doing so, we identify word boundaries in the antecedent clause and then use a special attention module to encode the syntactic information over the words for better generating the subsequent clause. Experimental results on a dataset for Chinese couplet generation illustrate the validity and effectiveness of our approach, which outperforms strong baselines with respect to automatic and manual evaluation metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Noise-injected Consistency Training and Entropy-constrained Pseudo Labeling for Semi-supervised Extractive Summarization",
        "paper_url": "https://aclanthology.org/2022.coling-1.561/",
        "paper_authors": [
            "Yiming Wang",
            "Qianren Mao",
            "Junnan Liu",
            "Weifeng Jiang",
            "Hongdong Zhu",
            "Jianxin Li"
        ],
        "paper_abstract": "Labeling large amounts of extractive summarization data is often prohibitive expensive due to time, financial, and expertise constraints, which poses great challenges to incorporating summarization system in practical applications. This limitation can be overcome by semi-supervised approaches: consistency-training and pseudo-labeling to make full use of unlabeled data. Researches on the two, however, are conducted independently, and very few works try to connect them. In this paper, we first use the noise-injected consistency training paradigm to regularize model predictions. Subsequently, we propose a novel entropy-constrained pseudo labeling strategy to obtain high-confidence labels from unlabeled predictions, which can obtain high-confidence labels from unlabeled predictions by comparing the entropy of supervised and unsupervised predictions. By combining consistency training and pseudo-labeling, this framework enforce a low-density separation between classes, which decently improves the performance of supervised learning over an insufficient labeled extractive summarization dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Question Generation Based on Grammar Knowledge and Fine-grained Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.562/",
        "paper_authors": [
            "Yuan Sun",
            "Sisi Liu",
            "Zhengcuo Dan",
            "Xiaobing Zhao"
        ],
        "paper_abstract": "Question generation is the task of automatically generating questions based on given context and answers, and there are problems that the types of questions and answers do not match. In minority languages such as Tibetan, since the grammar rules are complex and the training data is small, the related research on question generation is still in its infancy. To solve the above problems, this paper constructs a question type classifier and a question generator. We perform fine-grained division of question types and integrate grammatical knowledge into question type classifiers to improve the accuracy of question types. Then, the types predicted by the question type classifier are fed into the question generator. Our model improves the accuracy of interrogative words in generated questions, and the BLEU-4 on SQuAD reaches 17.52, the BLEU-4 on HotpotQA reaches 19.31, the BLEU-4 on TibetanQA reaches 25.58.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CM-Gen: A Neural Framework for Chinese Metaphor Generation with Explicit Context Modelling",
        "paper_url": "https://aclanthology.org/2022.coling-1.563/",
        "paper_authors": [
            "Yucheng Li",
            "Chenghua Lin",
            "Frank Guerin"
        ],
        "paper_abstract": "Nominal metaphors are frequently used in human language and have been shown to be effective in persuading, expressing emotion, and stimulating interest. This paper tackles the problem of Chinese Nominal Metaphor (NM) generation. We introduce a novel multitask framework, which jointly optimizes three tasks: NM identification, NM component identification, and NM generation. The metaphor identification module is able to perform a self-training procedure, which discovers novel metaphors from a large-scale unlabeled corpus for NM generation. The NM component identification module emphasizes components during training and conditions the generation on these NM components for more coherent results. To train the NM identification and component identification modules, we construct an annotated corpus consisting of 6.3k sentences that contain diverse metaphorical patterns. Automatic metrics show that our method can produce diverse metaphors with good readability, where 92% of them are novel metaphorical comparisons. Human evaluation shows our model significantly outperforms baselines on consistency and creativity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Psychology-guided Controllable Story Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.564/",
        "paper_authors": [
            "Yuqiang Xie",
            "Yue Hu",
            "Yunpeng Li",
            "Guanqun Bi",
            "Luxi Xing",
            "Wei Peng"
        ],
        "paper_abstract": "Controllable story generation is a challenging task in the field of NLP, which has attracted increasing research interest in recent years. However, most existing works generate a whole story conditioned on the appointed keywords or emotions, ignoring the psychological changes of the protagonist. Inspired by psychology theories, we introduce global psychological state chains, which include the needs and emotions of the protagonists, to help a story generation system create more controllable and well-planned stories. In this paper, we propose a Psychology-guided Controllable Story Generation System (PICS) to generate stories that adhere to the given leading context and desired psychological state chains for the protagonist. Specifically, psychological state trackers are employed to memorize the protagonist\u2019s local psychological states to capture their inner temporal relationships. In addition, psychological state planners are adopted to gain the protagonist\u2019s global psychological states for story planning. Eventually, a psychology controller is designed to integrate the local and global psychological states into the story context representation for composing psychology-guided stories. Automatic and manual evaluations demonstrate that PICS outperforms baselines, and each part of PICS shows effectiveness for writing stories with more consistent psychological changes.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Few-shot Table-to-text Generation with Prefix-Controlled Generator",
        "paper_url": "https://aclanthology.org/2022.coling-1.565/",
        "paper_authors": [
            "Yutao Luo",
            "Menghua Lu",
            "Gongshen Liu",
            "Shilin Wang"
        ],
        "paper_abstract": "Neural table-to-text generation approaches are data-hungry, limiting their adaption for low-resource real-world applications. Previous works mostly resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a table. However, they often contain hallucinated contents due to the uncontrolled nature of PLMs. Moreover, the topological differences between tables and sequences are rarely studied. Last but not least, fine-tuning on PLMs with a handful of instances may lead to over-fitting and catastrophic forgetting. To alleviate these problems, we propose a prompt-based approach, Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation. We prepend a task-specific prefix for a PLM to make the table structure better fit the pre-trained input. In addition, we generate an input-specific prefix to control the factual contents and word order of the generated text. Both automatic and human evaluations on different domains (humans, books and songs) of the Wikibio dataset prove the effectiveness of our approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Text Simplification of College Admissions Instructions: A Professionally Simplified and Verified Corpus",
        "paper_url": "https://aclanthology.org/2022.coling-1.566/",
        "paper_authors": [
            "Zachary W. Taylor",
            "Maximus H. Chu",
            "Junyi Jessy Li"
        ],
        "paper_abstract": "Access to higher education is critical for minority populations and emergent bilingual students. However, the language used by higher education institutions to communicate with prospective students is often too complex; concretely, many institutions in the US publish admissions application instructions far above the average reading level of a typical high school graduate, often near the 13th or 14th grade level. This leads to an unnecessary barrier between students and access to higher education. This work aims to tackle this challenge via text simplification. We present PSAT (Professionally Simplified Admissions Texts), a dataset with 112 admissions instructions randomly selected from higher education institutions across the US. These texts are then professionally simplified, and verified and accepted by subject-matter experts who are full-time employees in admissions offices at various institutions. Additionally, PSAT comes with manual alignments of 1,883 original-simplified sentence pairs. The result is a first-of-its-kind corpus for the evaluation and fine-tuning of text simplification systems in a high-stakes genre distinct from existing simplification resources.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART",
        "paper_url": "https://aclanthology.org/2022.coling-1.567/",
        "paper_authors": [
            "Zebin Ou",
            "Meishan Zhang",
            "Yue Zhang"
        ],
        "paper_abstract": "Word ordering is a constrained language generation task taking unordered words as input. Existing work uses linear models and neural networks for the task, yet pre-trained language models have not been studied in word ordering, let alone why they help. We use BART as an instance and show its effectiveness in the task. To explain why BART helps word ordering, we extend analysis with probing and empirically identify that syntactic dependency knowledge in BART is a reliable explanation. We also report performance gains with BART in the related partial tree linearization task, which readily extends our analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Visual Information Guided Zero-Shot Paraphrase Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.568/",
        "paper_authors": [
            "Zhe Lin",
            "Xiaojun Wan"
        ],
        "paper_abstract": "Zero-shot paraphrase generation has drawn much attention as the large-scale high-quality paraphrase corpus is limited. Back-translation, also known as the pivot-based method, is typical to this end. Several works leverage different information as \u201dpivot\u201d such as language, semantic representation and so on. In this paper, we explore using visual information such as image as the \u201dpivot\u201d of back-translation. Different with the pipeline back-translation method, we propose visual information guided zero-shot paraphrase generation (ViPG) based only on paired image-caption data. It jointly trains an image captioning model and a paraphrasing model and leverage the image captioning model to guide the training of the paraphrasing model. Both automatic evaluation and human evaluation show our model can generate paraphrase with good relevancy, fluency and diversity, and image is a promising kind of pivot for zero-shot paraphrase generation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.569/",
        "paper_authors": [
            "Zhichao Geng",
            "Ming Zhong",
            "Zhangyue Yin",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Pre-trained models have brought remarkable success on the text summarization task. For dialogue summarization, the subdomain of text summarization, utterances are concatenated to flat text before being processed. As a result, existing summarization systems based on pre-trained models are unable to recognize the unique format of the speaker-utterance pair well in the dialogue. To investigate this issue, we conduct probing tests and manual analysis, and find that the powerful pre-trained model can not identify different speakers well in the conversation, which leads to various factual errors. Moreover, we propose three speaker-aware supervised contrastive learning (SCL) tasks: Token-level SCL, Turn-level SCL, and Global-level SCL. Comprehensive experiments demonstrate that our methods achieve significant performance improvement on two mainstream dialogue summarization datasets. According to detailed human evaluations, pre-trained models equipped with SCL tasks effectively generate summaries with better factual consistency.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Diversifying Neural Text Generation with Part-of-Speech Guided Softmax and Sampling",
        "paper_url": "https://aclanthology.org/2022.coling-1.570/",
        "paper_authors": [
            "Zhixian Yang",
            "Pengxuan Xu",
            "Xiaojun Wan"
        ],
        "paper_abstract": "Neural text generation models are likely to suffer from the low-diversity problem. Various decoding strategies and training-based methods have been proposed to promote diversity only by exploiting contextual features, but rarely do they consider incorporating syntactic structure clues. In this work, we propose using linguistic annotation, i.e., part-of-speech (POS), to guide the text generation. In detail, we introduce POS Guided Softmax to explicitly model two posterior probabilities: (i) next-POS, and (ii) next-token from the vocabulary of the target POS. A POS Guided Sampling strategy is further proposed to address the low-diversity problem by enriching the diversity of POS. Extensive experiments and human evaluations show that, compared with existing state-of-the-art methods, our POS Guided Softmax and Sampling (POSG) can generate more diverse text while maintaining comparable quality.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enhancing Pre-trained Models with Text Structure Knowledge for Question Generation",
        "paper_url": "https://aclanthology.org/2022.coling-1.571/",
        "paper_authors": [
            "Zichen Wu",
            "Xin Jia",
            "Fanyi Qu",
            "Yunfang Wu"
        ],
        "paper_abstract": "Today the pre-trained language models achieve great success for question generation (QG) task and significantly outperform traditional sequence-to-sequence approaches. However, the pre-trained models treat the input passage as a flat sequence and are thus not aware of the text structure of input passage. For QG task, we model text structure as answer position and syntactic dependency, and propose answer localness modeling and syntactic mask attention to address these limitations. Specially, we present localness modeling with a Gaussian bias to enable the model to focus on answer-surrounded context, and propose a mask attention mechanism to make the syntactic structure of input passage accessible in question generation process. Experiments on SQuAD dataset show that our proposed two modules improve performance over the strong pre-trained model ProphetNet, and combing them together achieves very competitive results with the state-of-the-art pre-trained model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LFKQG: A Controlled Generation Framework with Local Fine-tuning for Question Generation over Knowledge Bases",
        "paper_url": "https://aclanthology.org/2022.coling-1.572/",
        "paper_authors": [
            "Zichu Fei",
            "Xin Zhou",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Question generation over knowledge bases (KBQG) aims at generating natural questions about a subgraph, which can be answered by a given answer entity. Existing KBQG models still face two main challenges: (1) Most models often focus on the most relevant part of the answer entity, while neglecting the rest of the subgraph. (2) There are a large number of out-of-vocabulary (OOV) predicates in real-world scenarios, which are hard to adapt for most KBQG models. To address these challenges, we propose LFKQG, a controlled generation framework for Question Generation over Knowledge Bases. (1) LFKQG employs a simple controlled generation method to generate the questions containing the critical entities in the subgraph, ensuring the question is relevant to the whole subgraph. (2) We propose an optimization strategy called local fine-tuning, which can make good use of the rich information hidden in the pre-trained model to improve the ability of the model to adapt the OOV predicates. Extensive experiments show that our method outperforms existing methods significantly on three widely-used benchmark datasets SimpleQuestion, PathQuestions, and WebQuestions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Demystifying Neural Fake News via Linguistic Feature-Based Interpretation",
        "paper_url": "https://aclanthology.org/2022.coling-1.573/",
        "paper_authors": [
            "Ankit Aich",
            "Souvik Bhattacharya",
            "Natalie Parde"
        ],
        "paper_abstract": "The spread of fake news can have devastating ramifications, and recent advancements to neural fake news generators have made it challenging to understand how misinformation generated by these models may best be confronted. We conduct a feature-based study to gain an interpretative understanding of the linguistic attributes that neural fake news generators may most successfully exploit. When comparing models trained on subsets of our features and confronting the models with increasingly advanced neural fake news, we find that stylistic features may be the most robust. We discuss our findings, subsequent analyses, and broader implications in the pages within.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Measuring Geographic Performance Disparities of Offensive Language Classifiers",
        "paper_url": "https://aclanthology.org/2022.coling-1.574/",
        "paper_authors": [
            "Brandon Lwowski",
            "Paul Rad",
            "Anthony Rios"
        ],
        "paper_abstract": "Text classifiers are applied at scale in the form of one-size-fits-all solutions. Nevertheless, many studies show that classifiers are biased regarding different languages and dialects. When measuring and discovering these biases, some gaps present themselves and should be addressed. First, \u201cDoes language, dialect, and topical content vary across geographical regions?\u201d and secondly \u201cIf there are differences across the regions, do they impact model performance?\u201d. We introduce a novel dataset called GeoOLID with more than 14 thousand examples across 15 geographically and demographically diverse cities to address these questions. We perform a comprehensive analysis of geographical-related content and their impact on performance disparities of offensive language detection models. Overall, we find that current models do not generalize across locations. Likewise, we show that while offensive language models produce false positives on African American English, model performance is not correlated with each city\u2019s minority population proportions. Warning: This paper contains offensive language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Offensive Content Detection via Synthetic Code-Switched Text",
        "paper_url": "https://aclanthology.org/2022.coling-1.575/",
        "paper_authors": [
            "Cesa Salaam",
            "Franck Dernoncourt",
            "Trung Bui",
            "Danda Rawat",
            "Seunghyun Yoon"
        ],
        "paper_abstract": "The prevalent use of offensive content in social media has become an important reason for concern for online platforms (customer service chat-boxes, social media platforms, etc). Classifying offensive and hate-speech content in online settings is an essential task in many applications that needs to be addressed accordingly. However, online text from online platforms can contain code-switching, a combination of more than one language. The non-availability of labeled code-switched data for low-resourced code-switching combinations adds difficulty to this problem. To overcome this, we release a real-world dataset containing around 10k samples for testing for three language combinations en-fr, en-es, and en-de, and a synthetic code-switched textual dataset containing ~30k samples for training In this paper, we describe the process for gathering the human-generated data and our algorithm for creating synthetic code-switched offensive content data. We also introduce the results of a keyword classification baseline and a multi-lingual transformer-based classification model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Survey on Multimodal Disinformation Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.576/",
        "paper_authors": [
            "Firoj Alam",
            "Stefano Cresci",
            "Tanmoy Chakraborty",
            "Fabrizio Silvestri",
            "Dimiter Dimitrov",
            "Giovanni Da San Martino",
            "Shaden Shaar",
            "Hamed Firooz",
            "Preslav Nakov"
        ],
        "paper_abstract": "Recent years have witnessed the proliferation of offensive content online such as fake news, propaganda, misinformation, and disinformation. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract more attention, and spread further than text. As a result, researchers started leveraging different modalities and combinations thereof to tackle online multimodal offensive content. In this study, we offer a survey on the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, speech, video, social media network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation \u2013 (i) factuality, and (ii) harmfulness \u2013, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate Speech Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.577/",
        "paper_authors": [
            "Jiyun Kim",
            "Byounghan Lee",
            "Kyung-Ah Sohn"
        ],
        "paper_abstract": "In a hate speech detection model, we should consider two critical aspects in addition to detection performance\u2013bias and explainability. Hate speech cannot be identified based solely on the presence of specific words; the model should be able to reason like humans and be explainable. To improve the performance concerning the two aspects, we propose Masked Rationale Prediction (MRP) as an intermediate task. MRP is a task to predict the masked human rationales\u2013snippets of a sentence that are grounds for human judgment\u2013by referring to surrounding tokens combined with their unmasked rationales. As the model learns its reasoning ability based on rationales by MRP, it performs hate speech detection robustly in terms of bias and explainability. The proposed method generally achieves state-of-the-art performance in various metrics, demonstrating its effectiveness for hate speech detection. Warning: This paper contains samples that may be upsetting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Domain Classification-based Source-specific Term Penalization for Domain Adaptation in Hate-speech Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.578/",
        "paper_authors": [
            "Tulika Bose",
            "Nikolaos Aletras",
            "Irina Illina",
            "Dominique Fohr"
        ],
        "paper_abstract": "State-of-the-art approaches for hate-speech detection usually exhibit poor performance in out-of-domain settings. This occurs, typically, due to classifiers overemphasizing source-specific information that negatively impacts its domain invariance. Prior work has attempted to penalize terms related to hate-speech from manually curated lists using feature attribution methods, which quantify the importance assigned to input terms by the classifier when making a prediction. We, instead, propose a domain adaptation approach that automatically extracts and penalizes source-specific terms using a domain classifier, which learns to differentiate between domains, and feature-attribution scores for hate-speech classes, yielding consistent improvements in cross-domain evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generalizable Implicit Hate Speech Detection Using Contrastive Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.579/",
        "paper_authors": [
            "Youngwook Kim",
            "Shinwoo Park",
            "Yo-Sub Han"
        ],
        "paper_abstract": "Hate speech detection has gained increasing attention with the growing prevalence of hateful contents. When a text contains an obvious hate word or expression, it is fairly easy to detect it. However, it is challenging to identify implicit hate speech in nuance or context when there are insufficient lexical cues. Recently, there are several attempts to detect implicit hate speech leveraging pre-trained language models such as BERT and HateBERT. Fine-tuning on an implicit hate speech dataset shows satisfactory performance when evaluated on the test set of the dataset used for training. However, we empirically confirm that the performance drops at least 12.5%p in F1 score when tested on the dataset that is different from the one used for training. We tackle this cross-dataset underperforming problem using contrastive learning. Based on our observation of common underlying implications in various forms of hate posts, we propose a novel contrastive learning method, ImpCon, that pulls an implication and its corresponding posts close in representation space. We evaluate the effectiveness of ImpCon by running cross-dataset evaluation on three implicit hate speech benchmarks. The experimental results on cross-dataset show that ImpCon improves at most 9.10% on BERT, and 8.71% on HateBERT.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Social Bot-Aware Graph Neural Network for Early Rumor Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.580/",
        "paper_authors": [
            "Zhen Huang",
            "Zhilong Lv",
            "Xiaoyun Han",
            "Binyang Li",
            "Menglong Lu",
            "Dongsheng Li"
        ],
        "paper_abstract": "Early rumor detection is a key challenging task to prevent rumors from spreading widely. Sociological research shows that social bots\u2019 behavior in the early stage has become the main reason for rumors\u2019 wide spread. However, current models do not explicitly distinguish genuine users from social bots, and their failure in identifying rumors timely. Therefore, this paper aims at early rumor detection by accounting for social bots\u2019 behavior, and presents a Social Bot-Aware Graph Neural Network, named SBAG. SBAG firstly pre-trains a multi-layer perception network to capture social bot features, and then constructs multiple graph neural networks by embedding the features to model the early propagation of posts, which is further used to detect rumors. Extensive experiments on three benchmark datasets show that SBAG achieves significant improvements against the baselines and also identifies rumors within 3 hours while maintaining more than 90% accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Contrastive Cross-Channel Data Augmentation Framework for Aspect-Based Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.581/",
        "paper_authors": [
            "Bing Wang",
            "Liang Ding",
            "Qihuang Zhong",
            "Ximing Li",
            "Dacheng Tao"
        ],
        "paper_abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task, which focuses on detecting the sentiment polarity towards the aspect in a sentence. However, it is always sensitive to the multi-aspect challenge, where features of multiple aspects in a sentence will affect each other. To mitigate this issue, we design a novel training framework, called Contrastive Cross-Channel Data Augmentation (C3 DA), which leverages an in-domain generator to construct more multi-aspect samples and then boosts the robustness of ABSA models via contrastive learning on these generated data. In practice, given a generative pretrained language model and some limited ABSA labeled data, we first employ some parameter-efficient approaches to perform the in-domain fine-tuning. Then, the obtained in-domain generator is used to generate the synthetic sentences from two channels, i.e., Aspect Augmentation Channel and Polarity Augmentation Channel, which generate the sentence condition on a given aspect and polarity respectively. Specifically, our C3 DA performs the sentence generation in a cross-channel manner to obtain more sentences, and proposes an Entropy-Minimization Filter to filter low-quality generated samples. Extensive experiments show that our C3 DA can outperform those baselines without any augmentations by about 1% on accuracy and Macro- F1. Code and data are released in https://github.com/wangbing1416/C3DA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Sentiment Interpretable Logic Tensor Network for Aspect-Term Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.582/",
        "paper_authors": [
            "Bowen Zhang",
            "Xu Huang",
            "Zhichao Huang",
            "Hu Huang",
            "Baoquan Zhang",
            "Xianghua Fu",
            "Liwen Jing"
        ],
        "paper_abstract": "Aspect-term sentiment analysis (ATSA) is an important task that aims to infer the sentiment towards the given aspect-terms. It is often required in the industry that ATSA should be performed with interpretability, computational efficiency and high accuracy. However, such an ATSA method has not yet been developed. This study aims to develop an ATSA method that fulfills all these requirements. To achieve the goal, we propose a novel Sentiment Interpretable Logic Tensor Network (SILTN). SILTN is interpretable because it is a neurosymbolic formalism and a computational model that supports learning and reasoning about data with a differentiable first-order logic language (FOL). To realize SILTN with high inferring accuracy, we propose a novel learning strategy called the two-stage syntax knowledge distillation (TSynKD). Using widely used datasets, we experimentally demonstrate that the proposed TSynKD is effective for improving the accuracy of SILTN, and the SILTN has both high interpretability and computational efficiency.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting Minority Arguments for Mutual Understanding: A Moderation Tool for the Online Climate Change Debate",
        "paper_url": "https://aclanthology.org/2022.coling-1.583/",
        "paper_authors": [
            "Cedric Waterschoot",
            "Ernst van den Hemel",
            "Antal van den Bosch"
        ],
        "paper_abstract": "Moderating user comments and promoting healthy understanding is a challenging task, especially in the context of polarized topics such as climate change. We propose a moderation tool to assist moderators in promoting mutual understanding in regard to this topic. The approach is twofold. First, we train classifiers to label incoming posts for the arguments they entail, with a specific focus on minority arguments. We apply active learning to further supplement the training data with rare arguments. Second, we dive deeper into singular arguments and extract the lexical patterns that distinguish each argument from the others. Our findings indicate that climate change arguments form clearly separable clusters in the embedding space. These classes are characterized by their own unique lexical patterns that provide a quick insight in an argument\u2019s key concepts. Additionally, supplementing our training data was necessary for our classifiers to be able to adequately recognize rare arguments. We argue that this detailed rundown of each argument provides insight into where others are coming from. These computational approaches can be part of the toolkit for content moderators and researchers struggling with polarized topics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.584/",
        "paper_authors": [
            "Changzhi Zhou",
            "Dandan Song",
            "Jing Xu",
            "Zhijing Wu"
        ],
        "paper_abstract": "Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause analysis, which extracts potential emotion-cause pairs from an emotional document. Most recent studies use end-to-end methods to tackle the ECPE task. However, these methods either suffer from a label sparsity problem or fail to model complicated relations between emotions and causes. Furthermore, they all do not consider explicit semantic information of clauses. To this end, we transform the ECPE task into a document-level machine reading comprehension (MRC) task and propose a Multi-turn MRC framework with Rethink mechanism (MM-R). Our framework can model complicated relations between emotions and causes while avoiding generating the pairing matrix (the leading cause of the label sparsity problem). Besides, the multi-turn structure can fuse explicit semantic information flow between emotions and causes. Extensive experiments on the benchmark emotion cause corpus demonstrate the effectiveness of our proposed framework, which outperforms existing state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Structural Bias for Aspect Sentiment Triplet Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.585/",
        "paper_authors": [
            "Chen Zhang",
            "Lei Ren",
            "Fang Ma",
            "Jingang Wang",
            "Wei Wu",
            "Dawei Song"
        ],
        "paper_abstract": "Structural bias has recently been exploited for aspect sentiment triplet extraction (ASTE) and led to improved performance. On the other hand, it is recognized that explicitly incorporating structural bias would have a negative impact on efficiency, whereas pretrained language models (PLMs) can already capture implicit structures. Thus, a natural question arises: Is structural bias still a necessity in the context of PLMs? To answer the question, we propose to address the efficiency issues by using an adapter to integrate structural bias in the PLM and using a cheap-to-compute relative position structure in place of the syntactic dependency structure. Benchmarking evaluation is conducted on the SemEval datasets. The results show that our proposed structural adapter is beneficial to PLMs and achieves state-of-the-art performance over a range of strong baselines, yet with a light parameter demand and low latency. Meanwhile, we give rise to the concern that the current evaluation default with data of small scale is under-confident. Consequently, we release a large-scale dataset for ASTE. The results on the new dataset hint that the structural adapter is confidently effective and efficient to a large scale. Overall, we draw the conclusion that structural bias shall still be a necessity even with PLMs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Data Augmentation for Aspect Based Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.586/",
        "paper_authors": [
            "David Z. Chen",
            "Adam Faulkner",
            "Sahil Badyal"
        ],
        "paper_abstract": "Recent approaches to Aspect-based Sentiment Analysis (ABSA) take a co-extraction approach to this span-level classification task, performing the subtasks of aspect term extraction (ATE) and aspect sentiment classification (ASC) simultaneously. In this work, we build on recent progress in applying pre-training to this co-extraction task with the introduction of an adaptation of Unsupervised Data Augmentation in semi-supervised learning. As originally implemented, UDA cannot accommodate span-level classification since it relies on advanced data augmentation techniques, such as back-translation, that alter the sequence lengths of the original data and cause index mismatches. We introduce an adaptation of UDA using Masked Language Model (MLM) unmasking that accommodates this index-match constraint and test the approach on standard ABSA benchmark datasets. We show that simple augmentations applied to modest-sized datasets along with consistency training lead to competitive performance with the current ABSA state-of-the-art in the restaurant and laptop domains using only 75% of the training data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Sentiment and Emotion Aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting",
        "paper_url": "https://aclanthology.org/2022.coling-1.587/",
        "paper_authors": [
            "Dushyant Singh Chauhan",
            "Gopendra Vikram Singh",
            "Aseem Arora",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "In this paper, we hypothesize that humor is closely related to sentiment and emotions. Also, due to the tremendous growth in multilingual content, there is a great demand for building models and systems that support multilingual information access. To end this, we first extend the recently released Multimodal Multiparty Hindi Humor (M2H2) dataset by adding parallel English utterances corresponding to Hindi utterances and then annotating each utterance with sentiment and emotion classes. We name it Sentiment, Humor, and Emotion aware Multilingual Multimodal Multiparty Dataset (SHEMuD). Therefore, we propose a multitask framework wherein the primary task is humor detection, and the auxiliary tasks are sentiment and emotion identification. We design a multitasking framework wherein we first propose a Context Transformer to capture the deep contextual relationships with the input utterances. We then propose a Sentiment and Emotion aware Embedding (SE-Embedding) to get the overall representation of a particular emotion and sentiment w.r.t. the specific humor situation. Experimental results on the SHEMuD show the efficacy of our approach and shows that multitask learning offers an improvement over the single-task framework for both monolingual (4.86 points in Hindi and 5.9 points in English in F1-score) and multilingual (5.17 points in F1-score) setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TSAM: A Two-Stream Attention Model for Causal Emotion Entailment",
        "paper_url": "https://aclanthology.org/2022.coling-1.588/",
        "paper_authors": [
            "Duzhen Zhang",
            "Zhen Yang",
            "Fandong Meng",
            "Xiuyi Chen",
            "Jie Zhou"
        ],
        "paper_abstract": "Causal Emotion Entailment (CEE) aims to discover the potential causes behind an emotion in a conversational utterance. Previous works formalize CEE as independent utterance pair classification problems, with emotion and speaker information neglected. From a new perspective, this paper considers CEE in a joint framework. We classify multiple utterances synchronously to capture the correlations between utterances in a global view and propose a Two-Stream Attention Model (TSAM) to effectively model the speaker\u2019s emotional influences in the conversational history. Specifically, the TSAM comprises three modules: Emotion Attention Network (EAN), Speaker Attention Network (SAN), and interaction module. The EAN and SAN incorporate emotion and speaker information in parallel, and the subsequent interaction module effectively interchanges relevant information between the EAN and SAN via a mutual BiAffine transformation. Extensive experimental results demonstrate that our model achieves new State-Of-The-Art (SOTA) performance and outperforms baselines remarkably.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Entity-Level Sentiment Analysis (ELSA): An Exploratory Task Survey",
        "paper_url": "https://aclanthology.org/2022.coling-1.589/",
        "paper_authors": [
            "Egil R\u00f8nningstad",
            "Erik Velldal",
            "Lilja \u00d8vrelid"
        ],
        "paper_abstract": "This paper explores the task of identifying the overall sentiment expressed towards volitional entities (persons and organizations) in a document - what we refer to as Entity-Level Sentiment Analysis (ELSA). While identifying sentiment conveyed towards an entity is well researched for shorter texts like tweets, we find little to no research on this specific task for longer texts with multiple mentions and opinions towards the same entity. This lack of research would be understandable if ELSA can be derived from existing tasks and models. To assess this, we annotate a set of professional reviews for their overall sentiment towards each volitional entity in the text. We sample from data already annotated for document-level, sentence-level, and target-level sentiment in a multi-domain review corpus, and our results indicate that there is no single proxy task that provides this overall sentiment we seek for the entities at a satisfactory level of performance. We present a suite of experiments aiming to assess the contribution towards ELSA provided by document-, sentence-, and target-level sentiment analysis, and provide a discussion of their shortcomings. We show that sentiment in our dataset is expressed not only with an entity mention as target, but also towards targets with a sentiment-relevant relation to a volitional entity. In our data, these relations extend beyond anaphoric coreference resolution, and our findings call for further research of the topic. Finally, we also present a survey of previous relevant work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning from Adjective-Noun Pairs: A Knowledge-enhanced Framework for Target-Oriented Multimodal Sentiment Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.590/",
        "paper_authors": [
            "Fei Zhao",
            "Zhen Wu",
            "Siyu Long",
            "Xinyu Dai",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "paper_abstract": "Target-oriented multimodal sentiment classification (TMSC) is a new subtask of aspect-based sentiment analysis, which aims to determine the sentiment polarity of the opinion target mentioned in a (sentence, image) pair. Recently, dominant works employ the attention mechanism to capture the corresponding visual representations of the opinion target, and then aggregate them as evidence to make sentiment predictions. However, they still suffer from two problems: (1) The granularity of the opinion target in two modalities is inconsistent, which causes visual attention sometimes fail to capture the corresponding visual representations of the target; (2) Even though it is captured, there are still significant differences between the visual representations expressing the same mood, which brings great difficulty to sentiment prediction. To this end, we propose a novel Knowledge-enhanced Framework (KEF) in this paper, which can successfully exploit adjective-noun pairs extracted from the image to improve the visual attention capability and sentiment prediction capability of the TMSC task. Extensive experimental results show that our framework consistently outperforms state-of-the-art works on two public datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Exploiting Sticker for Multimodal Sentiment Analysis in Social Media: A New Dataset and Baseline",
        "paper_url": "https://aclanthology.org/2022.coling-1.591/",
        "paper_authors": [
            "Feng Ge",
            "Weizhao Li",
            "Haopeng Ren",
            "Yi Cai"
        ],
        "paper_abstract": "Sentiment analysis in social media is challenging since posts are short of context. As a popular way to express emotion on social media, stickers related to these posts can supplement missing sentiments and help identify sentiments precisely. However, research about stickers has not been investigated further. To this end, we present a Chinese sticker-based multimodal dataset for the sentiment analysis task (CSMSA). Compared with previous real-world photo-based multimodal datasets, the CSMSA dataset focuses on stickers, conveying more vivid and moving emotions. The sticker-based multimodal sentiment analysis task is challenging in three aspects: inherent multimodality of stickers, significant inter-series variations between stickers, and complex multimodal sentiment fusion. We propose SAMSAM to address the above three challenges. Our model introduces a flexible masked self-attention mechanism to allow the dynamic interaction between post texts and stickers. The experimental results indicate that our model performs best compared with other models. More researches need to be devoted to this field. The dataset is publicly available at https://github.com/Logos23333/CSMSA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora",
        "paper_url": "https://aclanthology.org/2022.coling-1.592/",
        "paper_authors": [
            "Flor Miriam Plaza-del-Arco",
            "Mar\u00eda-Teresa Mart\u00edn-Valdivia",
            "Roman Klinger"
        ],
        "paper_abstract": "Within textual emotion classification, the set of relevant labels depends on the domain and application scenario and might not be known at the time of model development. This conflicts with the classical paradigm of supervised learning in which the labels need to be predefined. A solution to obtain a model with a flexible set of labels is to use the paradigm of zero-shot learning as a natural language inference task, which in addition adds the advantage of not needing any labeled training data. This raises the question how to prompt a natural language inference model for zero-shot learning emotion classification. Options for prompt formulations include the emotion name anger alone or the statement \u201cThis text expresses anger\u201d. With this paper, we analyze how sensitive a natural language inference-based zero-shot-learning classifier is to such changes to the prompt under consideration of the corpus: How carefully does the prompt need to be selected? We perform experiments on an established set of emotion datasets presenting different language registers according to different sources (tweets, events, blogs) with three natural language inference models and show that indeed the choice of a particular prompt formulation needs to fit to the corpus. We show that this challenge can be tackled with combinations of multiple prompts. Such ensemble is more robust across corpora than individual prompts and shows nearly the same performance as the individual best prompt for a particular corpus.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CommunityLM: Probing Partisan Worldviews from Language Models",
        "paper_url": "https://aclanthology.org/2022.coling-1.593/",
        "paper_authors": [
            "Hang Jiang",
            "Doug Beeferman",
            "Brandon Roy",
            "Deb Roy"
        ],
        "paper_abstract": "As political attitudes have diverged ideologically in the United States, political speech has diverged lingusitically. The ever-widening polarization between the US political parties is accelerated by an erosion of mutual understanding between them. We aim to make these communities more comprehensible to each other with a framework that probes community-specific responses to the same survey questions using community language models CommunityLM. In our framework we identify committed partisan members for each community on Twitter and fine-tune LMs on the tweets authored by them. We then assess the worldviews of the two groups using prompt-based probing of their corresponding LMs, with prompts that elicit opinions about public figures and groups surveyed by the American National Election Studies (ANES) 2020 Exploratory Testing Survey. We compare the responses generated by the LMs to the ANES survey results, and find a level of alignment that greatly exceeds several baseline methods. Our work aims to show that we can use community LMs to query the worldview of any group of people given a sufficiently large sample of their social media discussions or media diet.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Composition-based Heterogeneous Graph Multi-channel Attention Network for Multi-aspect Multi-sentiment Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.594/",
        "paper_authors": [
            "Hao Niu",
            "Yun Xiong",
            "Jian Gao",
            "Zhongchen Miao",
            "Xiaosu Wang",
            "Hongrun Ren",
            "Yao Zhang",
            "Yangyong Zhu"
        ],
        "paper_abstract": "Aspect-based sentiment analysis (ABSA) has drawn more and more attention because of its extensive applications. However, towards the sentence carried with more than one aspect, most existing works generate an aspect-specific sentence representation for each aspect term to predict sentiment polarity, which neglects the sentiment relationship among aspect terms. Besides, most current ABSA methods focus on sentences containing only one aspect term or multiple aspect terms with the same sentiment polarity, which makes ABSA degenerate into sentence-level sentiment analysis. In this paper, to deal with this problem, we construct a heterogeneous graph to model inter-aspect relationships and aspect-context relationships simultaneously and propose a novel Composition-based Heterogeneous Graph Multi-channel Attention Network (CHGMAN) to encode the constructed heterogeneous graph. Meanwhile, we conduct extensive experiments on three datasets: MAMSATSA, Rest14, and Laptop14, experimental results show the effectiveness of our method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and Argumentation Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.595/",
        "paper_authors": [
            "Jens Lemmens",
            "Jens Van Nooten",
            "Tim Kreutz",
            "Walter Daelemans"
        ],
        "paper_abstract": "We present CoNTACT: a Dutch language model adapted to the domain of COVID-19 tweets. The model was developed by continuing the pre-training phase of RobBERT (Delobelle et al., 2020) by using 2.8M Dutch COVID-19 related tweets posted in 2021. In order to test the performance of the model and compare it to RobBERT, the two models were tested on two tasks: (1) binary vaccine hesitancy detection and (2) detection of arguments for vaccine hesitancy. For both tasks, not only Twitter but also Facebook data was used to show cross-genre performance. In our experiments, CoNTACT showed statistically significant gains over RobBERT in all experiments for task 1. For task 2, we observed substantial improvements in virtually all classes in all experiments. An error analysis indicated that the domain adaptation yielded better representations of domain-specific terminology, causing CoNTACT to make more accurate classification decisions. For task 2, we observed substantial improvements in virtually all classes in all experiments. An error analysis indicated that the domain adaptation yielded better representations of domain-specific terminology, causing CoNTACT to make more accurate classification decisions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SSR: Utilizing Simplified Stance Reasoning Process for Robust Stance Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.596/",
        "paper_authors": [
            "Jianhua Yuan",
            "Yanyan Zhao",
            "Yanyue Lu",
            "Bing Qin"
        ],
        "paper_abstract": "Dataset bias in stance detection tasks allows models to achieve superior performance without using targets. Most existing debiasing methods are task-agnostic, which fail to utilize task knowledge to better discriminate between genuine and bias features. Motivated by how humans tackle stance detection tasks, we propose to incorporate the stance reasoning process as task knowledge to assist in learning genuine features and reducing reliance on bias features. The full stance reasoning process usually involves identifying the span of the mentioned target and corresponding opinion expressions, such fine-grained annotations are hard and expensive to obtain. To alleviate this, we simplify the stance reasoning process to relax the granularity of annotations from token-level to sentence-level, where labels for sub-tasks can be easily inferred from existing resources. We further implement those sub-tasks by maximizing mutual information between the texts and the opinioned targets. To evaluate whether stance detection models truly understand the task from various aspects, we collect and construct a series of new test sets. Our proposed model achieves better performance than previous task-agnostic debiasing methods on most of those new test sets while maintaining comparable performances to existing stance detection models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Transferring Confluent Knowledge to Argument Mining",
        "paper_url": "https://aclanthology.org/2022.coling-1.597/",
        "paper_authors": [
            "Jo\u00e3o Ant\u00f3nio Rodrigues",
            "Ant\u00f3nio Branco"
        ],
        "paper_abstract": "Relevant to all application domains where it is important to get at the reasons underlying sentiments and decisions, argument mining seeks to obtain structured arguments from unstructured text and has been addressed by approaches typically involving some feature and/or neural architecture engineering. By adopting a transfer learning methodology, and by means of a systematic study with a wide range of knowledge sources promisingly suitable to leverage argument mining, the aim of this paper is to empirically assess the potential of transferring such knowledge learned with confluent tasks. By adopting a lean approach that dispenses with heavier feature and model engineering, this study permitted both to gain novel empirically based insights into the argument mining task and to establish new state of the art levels of performance for its three main sub-tasks, viz. identification of argument components, classification of the components, and determination of the relation among them.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and Its Intensity",
        "paper_url": "https://aclanthology.org/2022.coling-1.598/",
        "paper_authors": [
            "Khalid Alnajjar",
            "Mika H\u00e4m\u00e4l\u00e4inen",
            "J\u00f6rg Tiedemann",
            "Jorma Laaksonen",
            "Mikko Kurimo"
        ],
        "paper_abstract": "Prerecorded laughter accompanying dialog in comedy TV shows encourages the audience to laugh by clearly marking humorous moments in the show. We present an approach for automatically detecting humor in the Friends TV show using multimodal data. Our model is capable of recognizing whether an utterance is humorous or not and assess the intensity of it. We use the prerecorded laughter in the show as annotation as it marks humor and the length of the audience\u2019s laughter tells us how funny a given joke is. We evaluate the model on episodes the model has not been exposed to during the training phase. Our results show that the model is capable of correctly detecting whether an utterance is humorous 78% of the time and how long the audience\u2019s laughter reaction should last with a mean absolute error of 600 milliseconds.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modeling Aspect Correlation for Aspect-based Sentiment Analysis via Recurrent Inverse Learning Guidance",
        "paper_url": "https://aclanthology.org/2022.coling-1.599/",
        "paper_authors": [
            "Longfeng Li",
            "Haifeng Sun",
            "Qi Qi",
            "Jingyu Wang",
            "Jing Wang",
            "Jianxin Liao"
        ],
        "paper_abstract": "Aspect-based sentiment analysis (ABSA) aims to distinguish sentiment polarity of every specific aspect in a given sentence. Previous researches have realized the importance of interactive learning with context and aspects. However, these methods are ill-studied to learn complex sentence with multiple aspects due to overlapped polarity feature. And they do not consider the correlation between aspects to distinguish overlapped feature. In order to solve this problem, we propose a new method called Recurrent Inverse Learning Guided Network (RILGNet). Our RILGNet has two points to improve the modeling of aspect correlation and the selecting of aspect feature. First, we use Recurrent Mechanism to improve the joint representation of aspects, which enhances the aspect correlation modeling iteratively. Second, we propose Inverse Learning Guidance to improve the selection of aspect feature by considering aspect correlation, which provides more useful information to determine polarity. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of RILGNet, and we further prove that RILGNet is state-of-the-art method in multiaspect scenarios.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Analyzing Persuasion Strategies of Debaters on Social Media",
        "paper_url": "https://aclanthology.org/2022.coling-1.600/",
        "paper_authors": [
            "Matti Wiegmann",
            "Khalid Al Khatib",
            "Vishal Khanna",
            "Benno Stein"
        ],
        "paper_abstract": "Existing studies on the analysis of persuasion in online discussions focus on investigating the effectiveness of comments in discussions and ignore the analysis of the effectiveness of debaters over multiple discussions. In this paper, we propose to quantify debaters effectiveness in the online discussion platform: \u201cChangeMyView\u201d in order to explore diverse insights into their persuasion strategies. In particular, targeting debaters with different levels of effectiveness (e.g., good vs. bad), various behavioral characteristics (e..g, engagement) and text stylistic features (e.g., used frames) of debaters are carefully examined, leading to several outcomes that can be the backbone of writing assistants and persuasive text generation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "KC-ISA: An Implicit Sentiment Analysis Model Combining Knowledge Enhancement and Context Features",
        "paper_url": "https://aclanthology.org/2022.coling-1.601/",
        "paper_authors": [
            "Minghao Xu",
            "Daling Wang",
            "Shi Feng",
            "Zhenfei Yang",
            "Yifei Zhang"
        ],
        "paper_abstract": "Sentiment analysis has always been an important research direction in natural language processing. The research can be divided into explicit sentiment analysis and implicit sentiment analysis according to whether there are sentiment words in language expression. There have been many research results in explicit sentiment analysis. However, implicit sentiment analysis is rarely studied. Compared with explicit sentiment expression, implicit sentiment expression usually omits a lot of knowledge and common sense, and context also has an important impact on implicit sentiment expression. In this paper, we use a knowledge graph to supplement implicit sentiment expression and propose a novel Implicit Sentiment Analysis model combining Knowledge enhancement and Context features (dubbed KC-ISA). The KC-ISA model can effectively integrate external knowledge and contextual features by the coattention mechanism. Finally, we conduct experiments on the SMP2019 implicit sentiment analysis dataset. Moreover, to verify the generality of the model, we also conduct experiments on two common sentiment analysis datasets. The results on three datasets show that our proposed KC-ISA model can achieve better results on text sentiment analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Domain Generalization for Text Classification with Memory-Based Supervised Contrastive Learning",
        "paper_url": "https://aclanthology.org/2022.coling-1.602/",
        "paper_authors": [
            "Qingyu Tan",
            "Ruidan He",
            "Lidong Bing",
            "Hwee Tou Ng"
        ],
        "paper_abstract": "While there is much research on cross-domain text classification, most existing approaches focus on one-to-one or many-to-one domain adaptation. In this paper, we tackle the more challenging task of domain generalization, in which domain-invariant representations are learned from multiple source domains, without access to any data from the target domains, and classification decisions are then made on test documents in unseen target domains. We propose a novel framework based on supervised contrastive learning with a memory-saving queue. In this way, we explicitly encourage examples of the same class to be closer and examples of different classes to be further apart in the embedding space. We have conducted extensive experiments on two Amazon review sentiment datasets, and one rumour detection dataset. Experimental results show that our domain generalization method consistently outperforms state-of-the-art domain adaptation methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Zero-Shot Claim Detection Framework Using Question Answering",
        "paper_url": "https://aclanthology.org/2022.coling-1.603/",
        "paper_authors": [
            "Revanth Gangi Reddy",
            "Sai Chetan Chinthakindi",
            "Yi R. Fung",
            "Kevin Small",
            "Heng Ji"
        ],
        "paper_abstract": "In recent years, there has been an increasing interest in claim detection as an important building block for misinformation detection. This involves detecting more fine-grained attributes relating to the claim, such as the claimer, claim topic, claim object pertaining to the topic, etc. Yet, a notable bottleneck of existing claim detection approaches is their portability to emerging events and low-resource training data settings. In this regard, we propose a fine-grained claim detection framework that leverages zero-shot Question Answering (QA) using directed questions to solve a diverse set of sub-tasks such as topic filtering, claim object detection, and claimer detection. We show that our approach significantly outperforms various zero-shot, few-shot and task-specific baselines on the NewsClaims benchmark (Reddy et al., 2021).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Asymmetric Mutual Learning for Multi-source Unsupervised Sentiment Adaptation with Dynamic Feature Network",
        "paper_url": "https://aclanthology.org/2022.coling-1.604/",
        "paper_authors": [
            "Rui Li",
            "Cheng Liu",
            "Dazhi Jiang"
        ],
        "paper_abstract": "Recently, fine-tuning the pre-trained language model (PrLM) on labeled sentiment datasets demonstrates impressive performance. However, collecting labeled sentiment dataset is time-consuming, and fine-tuning the whole PrLM brings about much computation cost. To this end, we focus on multi-source unsupervised sentiment adaptation problem with the pre-trained features, which is more practical and challenging. We first design a dynamic feature network to fully exploit the extracted pre-trained features for efficient domain adaptation. Meanwhile, with the difference of the traditional source-target domain alignment methods, we propose a novel asymmetric mutual learning strategy, which can robustly estimate the pseudo-labels of the target domain with the knowledge from all the other source models. Experiments on multiple sentiment benchmarks show that our method outperforms the recent state-of-the-art approaches, and we also conduct extensive ablation studies to verify the effectiveness of each the proposed module.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Target Really Matters: Target-aware Contrastive Learning and Consistency Regularization for Few-shot Stance Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.605/",
        "paper_authors": [
            "Rui Liu",
            "Zheng Lin",
            "Huishan Ji",
            "Jiangnan Li",
            "Peng Fu",
            "Weiping Wang"
        ],
        "paper_abstract": "Stance detection aims to identify the attitude from an opinion towards a certain target. Despite the significant progress on this task, it is extremely time-consuming and budget-unfriendly to collect sufficient high-quality labeled data for every new target under fully-supervised learning, whereas unlabeled data can be collected easier. Therefore, this paper is devoted to few-shot stance detection and investigating how to achieve satisfactory results in semi-supervised settings. As a target-oriented task, the core idea of semi-supervised few-shot stance detection is to make better use of target-relevant information from labeled and unlabeled data. Therefore, we develop a novel target-aware semi-supervised framework. Specifically, we propose a target-aware contrastive learning objective to learn more distinguishable representations for different targets. Such an objective can be easily applied with or without unlabeled data. Furthermore, to thoroughly exploit the unlabeled data and facilitate the model to learn target-relevant stance features in the opinion content, we explore a simple but effective target-aware consistency regularization combined with a self-training strategy. The experimental results demonstrate that our approach can achieve state-of-the-art performance on multiple benchmark datasets in the few-shot setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.606/",
        "paper_authors": [
            "Shunjie Chen",
            "Xiaochuan Shi",
            "Jingye Li",
            "Shengqiong Wu",
            "Hao Fei",
            "Fei Li",
            "Donghong Ji"
        ],
        "paper_abstract": "Emotion cause pair extraction (ECPE), as one of the derived subtasks of emotion cause analysis (ECA), shares rich inter-related features with emotion extraction (EE) and cause extraction (CE). Therefore EE and CE are frequently utilized as auxiliary tasks for better feature learning, modeled via multi-task learning (MTL) framework by prior works to achieve state-of-the-art (SoTA) ECPE results. However, existing MTL-based methods either fail to simultaneously model the specific features and the interactive feature in between, or suffer from the inconsistency of label prediction. In this work, we consider addressing the above challenges for improving ECPE by performing two alignment mechanisms with a novel A\u02c62Net model. We first propose a feature-task alignment to explicitly model the specific emotion-&cause-specific features and the shared interactive feature. Besides, an inter-task alignment is implemented, in which the label distance between the ECPE and the combinations of EE&CE are learned to be narrowed for better label consistency. Evaluations of benchmarks show that our methods outperform current best-performing systems on all ECA subtasks. Further analysis proves the importance of our proposed alignment mechanisms for the task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Causal Intervention Improves Implicit Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.607/",
        "paper_authors": [
            "Siyin Wang",
            "Jie Zhou",
            "Changzhi Sun",
            "Junjie Ye",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Despite having achieved great success for sentiment analysis, existing neural models struggle with implicit sentiment analysis. It is because they may latch onto spurious correlations (\u201cshortcuts\u201d, e.g., focusing only on explicit sentiment words), resulting in undermining the effectiveness and robustness of the learned model. In this work, we propose a CausaL intervention model for implicit sEntiment ANalysis using instrumental variable (CLEAN). We first review sentiment analysis from a causal perspective and analyze the confounders existing in this task. Then, we introduce instrumental variable to eliminate the confounding causal effects, thus extracting the pure causal effect between sentence and sentiment. We compare the proposed CLEAN with several strong baselines on both the general implicit sentiment analysis and aspect-based implicit sentiment analysis tasks. The results indicate the great advantages of our model and the efficacy of implicit sentiment reasoning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "COMMA-DEER: COmmon-sense Aware Multimodal Multitask Approach for Detection of Emotion and Emotional Reasoning in Conversations",
        "paper_url": "https://aclanthology.org/2022.coling-1.608/",
        "paper_authors": [
            "Soumitra Ghosh",
            "Gopendra Vikram Singh",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "Mental health is a critical component of the United Nations\u2019 Sustainable Development Goals (SDGs), particularly Goal 3, which aims to provide \u201cgood health and well-being\u201d. The present mental health treatment gap is exacerbated by stigma, lack of human resources, and lack of research capability for implementation and policy reform. We present and discuss a novel task of detecting emotional reasoning (ER) and accompanying emotions in conversations. In particular, we create a first-of-its-kind multimodal mental health conversational corpus that is manually annotated at the utterance level with emotional reasoning and related emotion. We develop a multimodal multitask framework with a novel multimodal feature fusion technique and a contextuality learning module to handle the two tasks. Leveraging multimodal sources of information, commonsense reasoning, and through a multitask framework, our proposed model produces strong results. We achieve performance gains of 6% accuracy and 4.62% F1 on the emotion detection task and 3.56% accuracy and 3.31% F1 on the ER detection task, when compared to the existing state-of-the-art model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "EmoMent: An Emotion Annotated Mental Health Corpus from Two South Asian Countries",
        "paper_url": "https://aclanthology.org/2022.coling-1.609/",
        "paper_authors": [
            "Thushari Atapattu",
            "Mahen Herath",
            "Charitha Elvitigala",
            "Piyanjali de Zoysa",
            "Kasun Gunawardana",
            "Menasha Thilakaratne",
            "Kasun de Zoysa",
            "Katrina Falkner"
        ],
        "paper_abstract": "People often utilise online media (e.g., Facebook, Reddit) as a platform to express their psychological distress and seek support. State-of-the-art NLP techniques demonstrate strong potential to automatically detect mental health issues from text. Research suggests that mental health issues are reflected in emotions (e.g., sadness) indicated in a person\u2019s choice of language. Therefore, we developed a novel emotion-annotated mental health corpus (EmoMent),consisting of 2802 Facebook posts (14845 sentences) extracted from two South Asian countries - Sri Lanka and India. Three clinical psychology postgraduates were involved in annotating these posts into eight categories, including \u2018mental illness\u2019 (e.g., depression) and emotions (e.g., \u2018sadness\u2019, \u2018anger\u2019). EmoMent corpus achieved \u2018very good\u2019 inter-annotator agreement of 98.3% (i.e. % with two or more agreement) and Fleiss\u2019 Kappa of 0.82. Our RoBERTa based models achieved an F1 score of 0.76 and a macro-averaged F1 score of 0.77 for the first task (i.e. predicting a mental health condition from a post) and the second task (i.e. extent of association of relevant posts with the categories defined in our taxonomy), respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LEGO-ABSA: A Prompt-based Task Assemblable Unified Generative Framework for Multi-task Aspect-based Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.610/",
        "paper_authors": [
            "Tianhao Gao",
            "Jun Fang",
            "Hanyu Liu",
            "Zhiyuan Liu",
            "Chao Liu",
            "Pengzhang Liu",
            "Yongjun Bao",
            "Weipeng Yan"
        ],
        "paper_abstract": "Aspect-based sentiment analysis (ABSA) has received increasing attention recently. ABSA can be divided into multiple tasks according to the different extracted elements. Existing generative methods usually treat the output as a whole string rather than the combination of different elements and only focus on a single task at once. This paper proposes a unified generative multi-task framework that can solve multiple ABSA tasks by controlling the type of task prompts consisting of multiple element prompts. Further, the proposed approach can train on simple tasks and transfer to difficult tasks by assembling task prompts, like assembling Lego bricks. We conduct experiments on six ABSA tasks across multiple benchmarks. Our proposed multi-task approach achieves new state-of-the-art results in almost all tasks and competitive results in task transfer scenarios.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.611/",
        "paper_authors": [
            "Wei Chen",
            "Jinglong Du",
            "Zhao Zhang",
            "Fuzhen Zhuang",
            "Zhongshi He"
        ],
        "paper_abstract": "Recently, some span-based methods have achieved encouraging performances for joint aspect-sentiment analysis, which first extract aspects (aspect extraction) by detecting aspect boundaries and then classify the span-level sentiments (sentiment classification). However, most existing approaches either sequentially extract task-specific features, leading to insufficient feature interactions, or they encode aspect features and sentiment features in a parallel manner, implying that feature representation in each task is largely independent of each other except for input sharing. Both of them ignore the internal correlations between the aspect extraction and sentiment classification. To solve this problem, we novelly propose a hierarchical interactive network (HI-ASA) to model two-way interactions between two tasks appropriately, where the hierarchical interactions involve two steps: shallow-level interaction and deep-level interaction. First, we utilize cross-stitch mechanism to combine the different task-specific features selectively as the input to ensure proper two-way interactions. Second, the mutual information technique is applied to mutually constrain learning between two tasks in the output layer, thus the aspect input and the sentiment input are capable of encoding features of the other task via backpropagation. Extensive experiments on three real-world datasets demonstrate HI-ASA\u2019s superiority over baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MuCDN: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations",
        "paper_url": "https://aclanthology.org/2022.coling-1.612/",
        "paper_authors": [
            "Weixiang Zhao",
            "Yanyan Zhao",
            "Bing Qin"
        ],
        "paper_abstract": "As an emerging research topic in natural language processing community, emotion recognition in multi-party conversations has attained increasing interest. Previous approaches that focus either on dyadic or multi-party scenarios exert much effort to cope with the challenge of emotional dynamics and achieve appealing results. However, since emotional interactions among speakers are often more complicated within the entangled multi-party conversations, these works are limited in capturing effective emotional clues in conversational context. In this work, we propose Mutual Conversational Detachment Network (MuCDN) to clearly and effectively understand the conversational context by separating conversations into detached threads. Specifically, two detachment ways are devised to perform context and speaker-specific modeling within detached threads and they are bridged through a mutual module. Experimental results on two datasets show that our model achieves better performance over the baseline models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "UECA-Prompt: Universal Prompt for Emotion Cause Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.613/",
        "paper_authors": [
            "Xiaopeng Zheng",
            "Zhiyue Liu",
            "Zizhen Zhang",
            "Zhaoyang Wang",
            "Jiahai Wang"
        ],
        "paper_abstract": "Emotion cause analysis (ECA) aims to extract emotion clauses and find the corresponding cause of the emotion. Existing methods adopt fine-tuning paradigm to solve certain types of ECA tasks. These task-specific methods have a deficiency of universality. And the relations among multiple objectives in one task are not explicitly modeled. Moreover, the relative position information introduced in most existing methods may make the model suffer from dataset bias. To address the first two problems, this paper proposes a universal prompt tuning method to solve different ECA tasks in the unified framework. As for the third problem, this paper designs a directional constraint module and a sequential learning module to ease the bias. Considering the commonalities among different tasks, this paper proposes a cross-task training method to further explore the capability of the model. The experimental results show that our method achieves competitive performance on the ECA datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "One-Teacher and Multiple-Student Knowledge Distillation on Sentiment Classification",
        "paper_url": "https://aclanthology.org/2022.coling-1.614/",
        "paper_authors": [
            "Xiaoqin Chang",
            "Sophia Yat Mei Lee",
            "Suyang Zhu",
            "Shoushan Li",
            "Guodong Zhou"
        ],
        "paper_abstract": "Knowledge distillation is an effective method to transfer knowledge from a large pre-trained teacher model to a compacted student model. However, in previous studies, the distilled student models are still large and remain impractical in highly speed-sensitive systems (e.g., an IR system). In this study, we aim to distill a deep pre-trained model into an extremely compacted shallow model like CNN. Specifically, we propose a novel one-teacher and multiple-student knowledge distillation approach to distill a deep pre-trained teacher model into multiple shallow student models with ensemble learning. Moreover, we leverage large-scale unlabeled data to improve the performance of students. Empirical studies on three sentiment classification tasks demonstrate that our approach achieves better results with much fewer parameters (0.9%-18%) and extremely high speedup ratios (100X-1000X).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Making Parameter-efficient Tuning More Efficient: A Unified Framework for Classification Tasks",
        "paper_url": "https://aclanthology.org/2022.coling-1.615/",
        "paper_authors": [
            "Xin Zhou",
            "Ruotian Ma",
            "Yicheng Zou",
            "Xuanting Chen",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang",
            "Rui Xie",
            "Wei Wu"
        ],
        "paper_abstract": "Large pre-trained language models (PLMs) have demonstrated superior performance in industrial applications. Recent studies have explored parameter-efficient PLM tuning, which only updates a small amount of task-specific parameters while achieving both high efficiency and comparable performance against standard fine-tuning. However, all these methods ignore the inefficiency problem caused by the task-specific output layers, which is inflexible for us to re-use PLMs and introduces non-negligible parameters. In this work, we focus on the text classification task and propose plugin-tuning, a framework that further improves the efficiency of existing parameter-efficient methods with a unified classifier. Specifically, we re-formulate both token and sentence classification tasks into a unified language modeling task, and map label spaces of different tasks into the same vocabulary space. In this way, we can directly re-use the language modeling heads of PLMs, avoiding introducing extra parameters for different tasks. We conduct experiments on six classification benchmarks. The experimental results show that plugin-tuning can achieve comparable performance against fine-tuned PLMs, while further saving around 50% parameters on top of other parameter-efficient methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Multi-Task Dual-Tree Network for Aspect Sentiment Triplet Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.616/",
        "paper_authors": [
            "Yichun Zhao",
            "Kui Meng",
            "Gongshen Liu",
            "Jintao Du",
            "Huijia Zhu"
        ],
        "paper_abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims at extracting triplets from a given sentence, where each triplet includes an aspect, its sentiment polarity, and a corresponding opinion explaining the polarity. Existing methods are poor at detecting complicated relations between aspects and opinions as well as classifying multiple sentiment polarities in a sentence. Detecting unclear boundaries of multi-word aspects and opinions is also a challenge. In this paper, we propose a Multi-Task Dual-Tree Network (MTDTN) to address these issues. We employ a constituency tree and a modified dependency tree in two sub-tasks of Aspect Opinion Co-Extraction (AOCE) and ASTE, respectively. To enhance the information interaction between the two sub-tasks, we further design a Transition-Based Inference Strategy (TBIS) that transfers the boundary information from tags of AOCE to ASTE through a transition matrix. Extensive experiments are conducted on four popular datasets, and the results show the effectiveness of our model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Exploiting Unlabeled Data for Target-Oriented Opinion Words Extraction",
        "paper_url": "https://aclanthology.org/2022.coling-1.617/",
        "paper_authors": [
            "Yidong Wang",
            "Hao Wu",
            "Ao Liu",
            "Wenxin Hou",
            "Zhen Wu",
            "Jindong Wang",
            "Takahiro Shinozaki",
            "Manabu Okumura",
            "Yue Zhang"
        ],
        "paper_abstract": "Target-oriented Opinion Words Extraction (TOWE) is a fine-grained sentiment analysis task that aims to extract the corresponding opinion words of a given opinion target from the sentence. Recently, deep learning approaches have made remarkable progress on this task. Nevertheless, the TOWE task still suffers from the scarcity of training data due to the expensive data annotation process. Limited labeled data increase the risk of distribution shift between test data and training data. In this paper, we propose exploiting massive unlabeled data to reduce the risk by increasing the exposure of the model to varying distribution shifts. Specifically, we propose a novel Multi-Grained Consistency Regularization (MGCR) method to make use of unlabeled data and design two filters specifically for TOWE to filter noisy data at different granularity. Extensive experimental results on four TOWE benchmark datasets indicate the superiority of MGCR compared with current state-of-the-art methods. The in-depth analysis also demonstrates the effectiveness of the different-granularity filters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learnable Dependency-based Double Graph Structure for Aspect-based Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.618/",
        "paper_authors": [
            "Yinglong Ma",
            "Yunhe Pang"
        ],
        "paper_abstract": "Dependency tree-based methods might be susceptible to the dependency tree due to that they inevitably introduce noisy information and neglect the rich relation information between words. In this paper, we propose a learnable dependency-based double graph (LD2G) model for aspect-based sentiment classification. We use multi-task learning for domain adaptive pretraining, which combines Biaffine Attention and Mask Language Model by incorporating features such as structure, relations and linguistic features in the sentiment text. Then we utilize the dependency enhanced double graph-based MPNN to deeply fuse structure features and relation features that are affected with each other for ASC. Experiment on four benchmark datasets shows that our model is superior to the state-of-the-art approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Structure-Aware Argument Encoder for Literature Discourse Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.619/",
        "paper_authors": [
            "Yinzi Li",
            "Wei Chen",
            "Zhongyu Wei",
            "Yujun Huang",
            "Chujun Wang",
            "Siyuan Wang",
            "Qi Zhang",
            "Xuanjing Huang",
            "Libo Wu"
        ],
        "paper_abstract": "Existing research for argument representation learning mainly treats tokens in the sentence equally and ignores the implied structure information of argumentative context. In this paper, we propose to separate tokens into two groups, namely framing tokens and topic ones, to capture structural information of arguments. In addition, we consider high-level structure by incorporating paragraph-level position information. A novel structure-aware argument encoder is proposed for literature discourse analysis. Experimental results on both a self-constructed corpus and a public corpus show the effectiveness of our model. Resources are available at https://github.com/lemuria-wchen/SAE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mere Contrastive Learning for Cross-Domain Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.620/",
        "paper_authors": [
            "Yun Luo",
            "Fang Guo",
            "Zihan Liu",
            "Yue Zhang"
        ],
        "paper_abstract": "Cross-domain sentiment analysis aims to predict the sentiment of texts in the target domain using the model trained on the source domain to cope with the scarcity of labeled data. Previous studies are mostly cross-entropy-based methods for the task, which suffer from instability and poor generalization. In this paper, we explore contrastive learning on the cross-domain sentiment analysis task. We propose a modified contrastive objective with in-batch negative samples so that the sentence representations from the same class can be pushed close while those from the different classes become further apart in the latent space. Experiments on two widely used datasets show that our model can achieve state-of-the-art performance in both cross-domain and multi-domain sentiment analysis tasks. Meanwhile, visualizations demonstrate the effectiveness of transferring knowledge learned in the source domain to the target domain and the adversarial test verifies the robustness of our model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Exploiting Sentiment and Common Sense for Zero-shot Stance Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.621/",
        "paper_authors": [
            "Yun Luo",
            "Zihan Liu",
            "Yuefeng Shi",
            "Stan Z. Li",
            "Yue Zhang"
        ],
        "paper_abstract": "The stance detection task aims to classify the stance toward given documents and topics. Since the topics can be implicit in documents and unseen in training data for zero-shot settings, we propose to boost the transferability of the stance detection model by using sentiment and commonsense knowledge, which are seldom considered in previous studies. Our model includes a graph autoencoder module to obtain commonsense knowledge and a stance detection module with sentiment and commonsense. Experimental results show that our model outperforms the state-of-the-art methods on the zero-shot and few-shot benchmark dataset\u2013VAST. Meanwhile, ablation studies prove the significance of each module in our model. Analysis of the relations between sentiment, common sense, and stance indicates the effectiveness of sentiment and common sense.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.622/",
        "paper_authors": [
            "Zijie Lin",
            "Bin Liang",
            "Yunfei Long",
            "Yixue Dang",
            "Min Yang",
            "Min Zhang",
            "Ruifeng Xu"
        ],
        "paper_abstract": "The existing research efforts in Multimodal Sentiment Analysis (MSA) have focused on developing the expressive ability of neural networks to fuse information from different modalities. However, these approaches lack a mechanism to understand the complex relations within and across different modalities, since some sentiments may be scattered in different modalities. To this end, in this paper, we propose a novel hierarchical graph contrastive learning (HGraph-CL) framework for MSA, aiming to explore the intricate relations of intra- and inter-modal representations for sentiment extraction. Specifically, regarding the intra-modal level, we build a unimodal graph for each modality representation to account for the modality-specific sentiment implications. Based on it, a graph contrastive learning strategy is adopted to explore the potential relations based on unimodal graph augmentations. Furthermore, we construct a multimodal graph of each instance based on the unimodal graphs to grasp the sentiment relations between different modalities. Then, in light of the multimodal augmentation graphs, a graph contrastive learning strategy over the inter-modal level is proposed to ulteriorly seek the possible graph structures for precisely learning sentiment relations. This essentially allows the framework to understand the appropriate graph structures for learning intricate relations among different modalities. Experimental results on two benchmark datasets show that the proposed framework outperforms the state-of-the-art baselines in MSA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "AMOA: Global Acoustic Feature Enhanced Modal-Order-Aware Network for Multimodal Sentiment Analysis",
        "paper_url": "https://aclanthology.org/2022.coling-1.623/",
        "paper_authors": [
            "Ziming Li",
            "Yan Zhou",
            "Weibo Zhang",
            "Yaxin Liu",
            "Chuanpeng Yang",
            "Zheng Lian",
            "Songlin Hu"
        ],
        "paper_abstract": "In recent years, multimodal sentiment analysis (MSA) has attracted more and more interest, which aims to predict the sentiment polarity expressed in a video. Existing methods typically 1) treat three modal features (textual, acoustic, visual) equally, without distinguishing the importance of different modalities; and 2) split the video into frames, leading to missing the global acoustic information. In this paper, we propose a global Acoustic feature enhanced Modal-Order-Aware network (AMOA) to address these problems. Firstly, a modal-order-aware network is designed to obtain the multimodal fusion feature. This network integrates the three modalities in a certain order, which makes the modality at the core position matter more. Then, we introduce the global acoustic feature of the whole video into our model. Since the global acoustic feature and multimodal fusion feature originally reside in their own spaces, contrastive learning is further employed to align them before concatenation. Experiments on two public datasets show that our model outperforms the state-of-the-art models. In addition, we also generalize our model to the sentiment with more complex semantics, such as sarcasm detection. Our model also achieves state-of-the-art performance on a widely used sarcasm dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Keyphrase Prediction from Video Transcripts: New Dataset and Directions",
        "paper_url": "https://aclanthology.org/2022.coling-1.624/",
        "paper_authors": [
            "Amir Pouran Ben Veyseh",
            "Quan Hung Tran",
            "Seunghyun Yoon",
            "Varun Manjunatha",
            "Hanieh Deilamsalehy",
            "Rajiv Jain",
            "Trung Bui",
            "Walter W. Chang",
            "Franck Dernoncourt",
            "Thien Huu Nguyen"
        ],
        "paper_abstract": "Keyphrase Prediction (KP) is an established NLP task, aiming to yield representative phrases to summarize the main content of a given document. Despite major progress in recent years, existing works on KP have mainly focused on formal texts such as scientific papers or weblogs. The challenges of KP in informal-text domains are not yet fully studied. To this end, this work studies new challenges of KP in transcripts of videos, an understudied domain for KP that involves informal texts and non-cohesive presentation styles. A bottleneck for KP research in this domain involves the lack of high-quality and large-scale annotated data that hinders the development of advanced KP models. To address this issue, we introduce a large-scale manually-annotated KP dataset in the domain of live-stream video transcripts obtained by automatic speech recognition tools. Concretely, transcripts of 500+ hours of videos streamed on the behance.net platform are manually labeled with important keyphrases. Our analysis of the dataset reveals the challenging nature of KP in transcripts. Moreover, for the first time in KP, we demonstrate the idea of improving KP for long documents (i.e., transcripts) by feeding models with paragraph-level keyphrases, i.e., hierarchical extraction. To foster future research, we will publicly release the dataset and code.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Event Extraction in Video Transcripts",
        "paper_url": "https://aclanthology.org/2022.coling-1.625/",
        "paper_authors": [
            "Amir Pouran Ben Veyseh",
            "Viet Dac Lai",
            "Franck Dernoncourt",
            "Thien Huu Nguyen"
        ],
        "paper_abstract": "Event extraction (EE) is one of the fundamental tasks for information extraction whose goal is to identify mentions of events and their participants in text. Due to its importance, different methods and datasets have been introduced for EE. However, existing EE datasets are limited to formally written documents such as news articles or scientific papers. As such, the challenges of EE in informal and noisy texts are not adequately studied. In particular, video transcripts constitute an important domain that can benefit tremendously from EE systems (e.g., video retrieval), but has not been studied in EE literature due to the lack of necessary datasets. To address this limitation, we propose the first large-scale EE dataset obtained for transcripts of streamed videos on the video hosting platform Behance to promote future research in this area. In addition, we extensively evaluate existing state-of-the-art EE methods on our new dataset. We demonstrate that such systems cannot achieve adequate performance on the proposed dataset, revealing challenges and opportunities for further research effort.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Recycle Your Wav2Vec2 Codebook: A Speech Perceiver for Keyword Spotting",
        "paper_url": "https://aclanthology.org/2022.coling-1.626/",
        "paper_authors": [
            "Guillermo C\u00e1mbara",
            "Jordi Luque",
            "Mireia Farr\u00fas"
        ],
        "paper_abstract": "Speech information in a pretrained wav2vec2.0 model is usually leveraged through its encoder, which has at least 95M parameters, being not so suitable for small footprint Keyword Spotting. In this work, we show an efficient way of profiting from wav2vec2.0\u2019s linguistic knowledge, by recycling the phonetic information encoded in its latent codebook, which has been typically thrown away after pretraining. We do so by transferring the codebook as weights for the latent bottleneck of a Keyword Spotting Perceiver, thus initializing such model with phonetic embeddings already. The Perceiver design relies on cross-attention between these embeddings and input data to generate better representations. Our method delivers accuracy gains compared to random initialization, at no latency costs. Plus, we show that the phonetic embeddings can easily be downsampled with k-means clustering, speeding up inference in 3.5 times at only slight accuracy penalties.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Code-switched ASR with Linguistic Information",
        "paper_url": "https://aclanthology.org/2022.coling-1.627/",
        "paper_authors": [
            "Jie Chi",
            "Peter Bell"
        ],
        "paper_abstract": "This paper seeks to improve the performance of automatic speech recognition (ASR) systems operating on code-switched speech. Code-switching refers to the alternation of languages within a conversation, a phenomenon that is of increasing importance considering the rapid rise in the number of bilingual speakers in the world. It is particularly challenging for ASR owing to the relative scarcity of code-switching speech and text data, even when the individual languages are themselves well-resourced. This paper proposes to overcome this challenge by applying linguistic theories in order to generate more realistic code-switching text, necessary for language modelling in ASR. Working with English-Spanish code-switching, we find that Equivalence Constraint theory and part-of-speech labelling are particularly helpful for text generation, and bring 2% improvement to ASR performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Language-specific Effects on Automatic Speech Recognition Errors for World Englishes",
        "paper_url": "https://aclanthology.org/2022.coling-1.628/",
        "paper_authors": [
            "June Choe",
            "Yiran Chen",
            "May Pik Yu Chan",
            "Aini Li",
            "Xin Gao",
            "Nicole Holliday"
        ],
        "paper_abstract": "Despite recent advancements in automated speech recognition (ASR) technologies, reports of unequal performance across speakers of different demographic groups abound. At the same time, the focus on performance metrics such as the Word Error Rate (WER) in prior studies limit the specificity and scope of recommendations that can be offered for system engineering to overcome these challenges. The current study bridges this gap by investigating the performance of Otter\u2019s automatic captioning system on native and non-native English speakers of different language background through a linguistic analysis of segment-level errors. By examining language-specific error profiles for vowels and consonants motivated by linguistic theory, we find that certain categories of errors can be predicted from the phonological structure of a speaker\u2019s native language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Transformer-based Threshold-Free Framework for Multi-Intent NLU",
        "paper_url": "https://aclanthology.org/2022.coling-1.629/",
        "paper_authors": [
            "Lisung Chen",
            "Nuo Chen",
            "Yuexian Zou",
            "Yong Wang",
            "Xinzhong Sun"
        ],
        "paper_abstract": "Multi-intent natural language understanding (NLU) has recently gained attention. It detects multiple intents in an utterance, which is better suited to real-world scenarios. However, the state-of-the-art joint NLU models mainly detect multiple intents on threshold-based strategy, resulting in one main issue: the model is extremely sensitive to the threshold settings. In this paper, we propose a transformer-based Threshold-Free Multi-intent NLU model (TFMN) with multi-task learning (MTL). Specifically, we first leverage multiple layers of a transformer-based encoder to generate multi-grain representations. Then we exploit the information of the number of multiple intents in each utterance without additional manual annotations and propose an auxiliary detection task: Intent Number detection (IND). Furthermore, we propose a threshold-free intent multi-intent classifier that utilizes the output of IND task and detects the multiple intents without depending on the threshold. Extensive experiments demonstrate that our proposed model achieves superior results on two public multi-intent datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Multi-scale Expressive Speaking Style Modeling with Hierarchical Context Information for Audiobook Speech Synthesis",
        "paper_url": "https://aclanthology.org/2022.coling-1.630/",
        "paper_authors": [
            "Xueyuan Chen",
            "Shun Lei",
            "Zhiyong Wu",
            "Dong Xu",
            "Weifeng Zhao",
            "Helen Meng"
        ],
        "paper_abstract": "Naturalness and expressiveness are crucial for audiobook speech synthesis, but now are limited by the averaged global-scale speaking style representation. In this paper, we propose an unsupervised multi-scale context-sensitive text-to-speech model for audiobooks. A multi-scale hierarchical context encoder is specially designed to predict both global-scale context style embedding and local-scale context style embedding from a wider context of input text in a hierarchical manner. Likewise, a multi-scale reference encoder is introduced to extract reference style embeddings at both global and local scales from the reference speech, which is used to guide the prediction of speaking styles. On top of these, a bi-reference attention mechanism is used to align both local-scale reference style embedding sequence and local-scale context style embedding sequence with corresponding phoneme embedding sequence. Both objective and subjective experiment results on a real-world multi-speaker Mandarin novel audio dataset demonstrate the excellent performance of our proposed method over all baselines in terms of naturalness and expressiveness of the synthesized speech.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Incorporating Instructional Prompts into a Unified Generative Framework for Joint Multiple Intent Detection and Slot Filling",
        "paper_url": "https://aclanthology.org/2022.coling-1.631/",
        "paper_authors": [
            "Yangjun Wu",
            "Han Wang",
            "Dongxiang Zhang",
            "Gang Chen",
            "Hao Zhang"
        ],
        "paper_abstract": "The joint multiple Intent Detection (ID) and Slot Filling (SF) is a significant challenge in spoken language understanding. Because the slots in an utterance may relate to multi-intents, most existing approaches focus on utilizing task-specific components to capture the relations between intents and slots. The customized networks restrict models from modeling commonalities between tasks and generalization for broader applications. To address the above issue, we propose a Unified Generative framework (UGEN) based on a prompt-based paradigm, and formulate the task as a question-answering problem. Specifically, we design 5-type templates as instructional prompts, and each template includes a question that acts as the driver to teach UGEN to grasp the paradigm, options that list the candidate intents or slots to reduce the answer search space, and the context denotes original utterance. Through the instructional prompts, UGEN is guided to understand intents, slots, and their implicit correlations. On two popular multi-intent benchmark datasets, experimental results demonstrate that UGEN achieves new SOTA performances on full-data and surpasses the baselines by a large margin on 5-shot (28.1%) and 10-shot (23%) scenarios, which verify that UGEN is robust and effective.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptive Unsupervised Self-training for Disfluency Detection",
        "paper_url": "https://aclanthology.org/2022.coling-1.632/",
        "paper_authors": [
            "Zhongyuan Wang",
            "Yixuan Wang",
            "Shaolei Wang",
            "Wanxiang Che"
        ],
        "paper_abstract": "Supervised methods have achieved remarkable results in disfluency detection. However, in real-world scenarios, human-annotated data is difficult to obtain. Recent works try to handle disfluency detection with unsupervised self-training, which can exploit existing large-scale unlabeled data efficiently. However, their self-training-based methods suffer from the problems of selection bias and error accumulation. To tackle these problems, we propose an adaptive unsupervised self-training method for disfluency detection. Specifically, we re-weight the importance of each training example according to its grammatical feature and prediction confidence. Experiments on the Switchboard dataset show that our method improves 2.3 points over the current SOTA unsupervised method. Moreover, our method is competitive with the SOTA supervised method.",
        "paper_code": "#",
        "paper_cite": -1
    }
]