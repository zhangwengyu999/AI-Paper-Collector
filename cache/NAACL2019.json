[
    {
        "paper_name": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
        "paper_url": "https://aclanthology.org/N19-1000/",
        "paper_authors": [
            "Jill Burstein",
            "Christy Doran",
            "Thamar Solorio"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Entity Recognition at First Sight: Improving NER with Eye Movement Information",
        "paper_url": "https://aclanthology.org/N19-1001/",
        "paper_authors": [
            "Nora Hollenstein",
            "Ce Zhang"
        ],
        "paper_abstract": "Previous research shows that eye-tracking data contains information about the lexical and syntactic properties of text, which can be used to improve natural language processing models. In this work, we leverage eye movement features from three corpora with recorded gaze information to augment a state-of-the-art neural model for named entity recognition (NER) with gaze embeddings. These corpora were manually annotated with named entity labels. Moreover, we show how gaze features, generalized on word type level, eliminate the need for recorded eye-tracking data at test time. The gaze-augmented models for NER using token-level and type-level features outperform the baselines. We present the benefits of eye-tracking features by evaluating the NER models on both individual datasets as well as in cross-domain settings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The emergence of number and syntax units in LSTM language models",
        "paper_url": "https://aclanthology.org/N19-1002/",
        "paper_authors": [
            "Yair Lakretz",
            "German Kruszewski",
            "Theo Desbordes",
            "Dieuwke Hupkes",
            "Stanislas Dehaene",
            "Marco Baroni"
        ],
        "paper_abstract": "Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two \u201cnumber units\u201d. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Self-Training through Spaced Repetition",
        "paper_url": "https://aclanthology.org/N19-1003/",
        "paper_authors": [
            "Hadi Amiri"
        ],
        "paper_abstract": "Self-training is a semi-supervised learning approach for utilizing unlabeled data to create better learners. The efficacy of self-training algorithms depends on their data sampling techniques. The majority of current sampling techniques are based on predetermined policies which may not effectively explore the data space or improve model generalizability. In this work, we tackle the above challenges by introducing a new data sampling technique based on spaced repetition that dynamically samples informative and diverse unlabeled instances with respect to individual learner and instance characteristics. The proposed model is specifically effective in the context of neural models which can suffer from overfitting and high-variance gradients when trained with small amount of labeled data. Our model outperforms current semi-supervised learning approaches developed for neural networks on publicly-available datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural language models as psycholinguistic subjects: Representations of syntactic state",
        "paper_url": "https://aclanthology.org/N19-1004/",
        "paper_authors": [
            "Richard Futrell",
            "Ethan Wilcox",
            "Takashi Morita",
            "Peng Qian",
            "Miguel Ballesteros",
            "Roger Levy"
        ],
        "paper_abstract": "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding language-elicited EEG data by predicting it from a fine-tuned language model",
        "paper_url": "https://aclanthology.org/N19-1005/",
        "paper_authors": [
            "Dan Schwartz",
            "Tom Mitchell"
        ],
        "paper_abstract": "Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations \u2013 known collectively as event-related potentials (ERPs) \u2013 are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by finetuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pre-training on high-resource speech recognition improves low-resource speech-to-text translation",
        "paper_url": "https://aclanthology.org/N19-1006/",
        "paper_authors": [
            "Sameer Bansal",
            "Herman Kamper",
            "Karen Livescu",
            "Adam Lopez",
            "Sharon Goldwater"
        ],
        "paper_abstract": "We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve Spanish English ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available. Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement, despite the fact that the shared language in these tasks is the target language text, not the source language audio. Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders",
        "paper_url": "https://aclanthology.org/N19-1007/",
        "paper_authors": [
            "Cory Shain",
            "Micha Elsner"
        ],
        "paper_abstract": "In this paper, we deploy binary stochastic neural autoencoder networks as models of infant language learning in two typologically unrelated languages (Xitsonga and English). We show that the drive to model auditory percepts leads to latent clusters that partially align with theory-driven phonemic categories. We further evaluate the degree to which theory-driven phonological features are encoded in the latent bit patterns, finding that some (e.g. [+-approximant]), are well represented by the network in both languages, while others (e.g. [+-spread glottis]) are less so. Together, these findings suggest that many reliable cues to phonemic structure are immediately available to infants from bottom-up perceptual characteristics alone, but that these cues must eventually be supplemented by top-down lexical and phonotactic information to achieve adult-like phone discrimination. Our results also suggest differences in degree of perceptual availability between features, yielding testable predictions as to which features might depend more or less heavily on top-down cues during child language acquisition.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Giving Attention to the Unexpected: Using Prosody Innovations in Disfluency Detection",
        "paper_url": "https://aclanthology.org/N19-1008/",
        "paper_authors": [
            "Vicky Zayats",
            "Mari Ostendorf"
        ],
        "paper_abstract": "Disfluencies in spontaneous speech are known to be associated with prosodic disruptions. However, most algorithms for disfluency detection use only word transcripts. Integrating prosodic cues has proved difficult because of the many sources of variability affecting the acoustic correlates. This paper introduces a new approach to extracting acoustic-prosodic cues using text-based distributional prediction of acoustic cues to derive vector z-score features (innovations). We explore both early and late fusion techniques for integrating text and prosody, showing gains over a high-accuracy text-only model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Massively Multilingual Adversarial Speech Recognition",
        "paper_url": "https://aclanthology.org/N19-1009/",
        "paper_authors": [
            "Oliver Adams",
            "Matthew Wiesner",
            "Shinji Watanabe",
            "David Yarowsky"
        ],
        "paper_abstract": "We report on adaptation of multilingual end-to-end speech recognition models trained on as many as 100 languages. Our findings shed light on the relative importance of similarity between the target and pretraining languages along the dimensions of phonetics, phonology, language family, geographical location, and orthography. In this context, experiments demonstrate the effectiveness of two additional pretraining objectives in encouraging language-independent encoder representations: a context-independent phoneme objective paired with a language-adversarial classification objective.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Lost in Interpretation: Predicting Untranslated Terminology in Simultaneous Interpretation",
        "paper_url": "https://aclanthology.org/N19-1010/",
        "paper_authors": [
            "Nikolai Vogler",
            "Craig Stewart",
            "Graham Neubig"
        ],
        "paper_abstract": "Simultaneous interpretation, the translation of speech from one language to another in real-time, is an inherently difficult and strenuous task. One of the greatest challenges faced by interpreters is the accurate translation of difficult terminology like proper names, numbers, or other entities. Intelligent computer-assisted interpreting (CAI) tools that could analyze the spoken word and detect terms likely to be untranslated by an interpreter could reduce translation error and improve interpreter performance. In this paper, we propose a task of predicting which terminology simultaneous interpreters will leave untranslated, and examine methods that perform this task using supervised sequence taggers. We describe a number of task-specific features explicitly designed to indicate when an interpreter may struggle with translating a word. Experimental results on a newly-annotated version of the NAIST Simultaneous Translation Corpus (Shimizu et al., 2014) indicate the promise of our proposed method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "AudioCaps: Generating Captions for Audios in The Wild",
        "paper_url": "https://aclanthology.org/N19-1011/",
        "paper_authors": [
            "Chris Dongjoo Kim",
            "Byeongchang Kim",
            "Hyunmin Lee",
            "Gunhee Kim"
        ],
        "paper_abstract": "We explore the problem of Audio Captioning: generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research. We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset. Our thorough empirical studies not only show that our collected captions are indeed faithful to audio inputs but also discover what forms of audio representation and captioning models are effective for the audio captioning. From extensive experiments, we also propose two novel components that help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "\u201cPresident Vows to Cut <Taxes> Hair\u201d: Dataset and Analysis of Creative Text Editing for Humorous Headlines",
        "paper_url": "https://aclanthology.org/N19-1012/",
        "paper_authors": [
            "Nabil Hossain",
            "John Krumm",
            "Michael Gamon"
        ],
        "paper_abstract": "We introduce, release, and analyze a new dataset, called Humicroedit, for research in computational humor. Our publicly available data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. We carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline. The simple edits, usually just a single word replacement, mean we can apply straightforward analysis techniques to determine what makes our edited headlines humorous. We show how the data support classic theories of humor, such as incongruity, superiority, and setup/punchline. Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Answer-based Adversarial Training for Generating Clarification Questions",
        "paper_url": "https://aclanthology.org/N19-1013/",
        "paper_authors": [
            "Sudha Rao",
            "Hal Daum\u00e9 III"
        ],
        "paper_abstract": "We present an approach for generating clarification questions with the goal of eliciting new information that would make the given textual context more complete. We propose that modeling hypothetical answers (to clarification questions) as latent variables can guide our approach into generating more useful clarification questions. We develop a Generative Adversarial Network (GAN) where the generator is a sequence-to-sequence model and the discriminator is a utility function that models the value of updating the context with the answer to the clarification question. We evaluate on two datasets, using both automatic metrics and human judgments of usefulness, specificity and relevance, showing that our approach outperforms both a retrieval-based model and ablations that exclude the utility model and the adversarial training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data",
        "paper_url": "https://aclanthology.org/N19-1014/",
        "paper_authors": [
            "Wei Zhao",
            "Liang Wang",
            "Kewei Shen",
            "Ruoyu Jia",
            "Jingming Liu"
        ],
        "paper_abstract": "Neural machine translation systems have become state-of-the-art approaches for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence. Since the GEC suffers from not having enough labeled training data to achieve high accuracy. We pre-train the copy-augmented architecture with a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons between the fully pre-trained model and a partially pre-trained model. It is the first time copying words from the source context and fully pre-training a sequence to sequence model are experimented on the GEC task. Moreover, We add token-level and sentence-level multi-task learning for the GEC task. The evaluation results on the CoNLL-2014 test set show that our approach outperforms all recently published state-of-the-art results by a large margin.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Topic-Guided Variational Auto-Encoder for Text Generation",
        "paper_url": "https://aclanthology.org/N19-1015/",
        "paper_authors": [
            "Wenlin Wang",
            "Zhe Gan",
            "Hongteng Xu",
            "Ruiyi Zhang",
            "Guoyin Wang",
            "Dinghan Shen",
            "Changyou Chen",
            "Lawrence Carin"
        ],
        "paper_abstract": "We propose a topic-guided variational auto-encoder (TGVAE) model for text generation. Distinct from existing variational auto-encoder (VAE) based approaches, which assume a simple Gaussian prior for latent code, our model specifies the prior as a Gaussian mixture model (GMM) parametrized by a neural topic module. Each mixture component corresponds to a latent topic, which provides a guidance to generate sentences under the topic. The neural topic module and the VAE-based neural sequence module in our model are learned jointly. In particular, a sequence of invertible Householder transformations is applied to endow the approximate posterior of the latent code with high flexibility during the model inference. Experimental results show that our TGVAE outperforms its competitors on both unconditional and conditional text generation, which can also generate semantically-meaningful sentences with various topics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Implementation of a Chomsky-Sch\u00fctzenberger n-best parser for weighted multiple context-free grammars",
        "paper_url": "https://aclanthology.org/N19-1016/",
        "paper_authors": [
            "Thomas Ruprecht",
            "Tobias Denkinger"
        ],
        "paper_abstract": "Constituent parsing has been studied extensively in the last decades. Chomsky-Sch\u00fctzenberger parsing as an approach to constituent parsing has only been investigated theoretically, yet. It uses the decomposition of a language into a regular language, a homomorphism, and a bracket language to divide the parsing problem into simpler subproblems. We provide the first implementation of Chomsky-Sch\u00fctzenberger parsing. It employs multiple context-free grammars and incorporates many refinements to achieve feasibility. We compare its performance to state-of-the-art grammar-based parsers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Phylogenic Multi-Lingual Dependency Parsing",
        "paper_url": "https://aclanthology.org/N19-1017/",
        "paper_authors": [
            "Mathieu Dehouck",
            "Pascal Denis"
        ],
        "paper_abstract": "Languages evolve and diverge over time. Their evolutionary history is often depicted in the shape of a phylogenetic tree. Assuming parsing models are representations of their languages grammars, their evolution should follow a structure similar to that of the phylogenetic tree. In this paper, drawing inspiration from multi-task learning, we make use of the phylogenetic tree to guide the learning of multi-lingual dependency parsers leveraging languages structural similarities. Experiments on data from the Universal Dependency project show that phylogenetic training is beneficial to low resourced languages and to well furnished languages families. As a side product of phylogenetic training, our model is able to perform zero-shot parsing of previously unseen languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Discontinuous Constituency Parsing with a Stack-Free Transition System and a Dynamic Oracle",
        "paper_url": "https://aclanthology.org/N19-1018/",
        "paper_authors": [
            "Maximin Coavoux",
            "Shay B. Cohen"
        ],
        "paper_abstract": "We introduce a novel transition system for discontinuous constituency parsing. Instead of storing subtrees in a stack \u2013i.e. a data structure with linear-time sequential access\u2013 the proposed system uses a set of parsing items, with constant-time random access. This change makes it possible to construct any discontinuous constituency tree in exactly 4n\u20132 transitions for a sentence of length n. At each parsing step, the parser considers every item in the set to be combined with a focus item and to construct a new constituent in a bottom-up fashion. The parsing strategy is based on the assumption that most syntactic structures can be parsed incrementally and that the set \u2013the memory of the parser\u2013 remains reasonably small on average. Moreover, we introduce a provably correct dynamic oracle for the new transition system, and present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our parser obtains state-of-the-art results on three English and German discontinuous treebanks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "How Bad are PoS Tagger in Cross-Corpora Settings? Evaluating Annotation Divergence in the UD Project.",
        "paper_url": "https://aclanthology.org/N19-1019/",
        "paper_authors": [
            "Guillaume Wisniewski",
            "Fran\u00e7ois Yvon"
        ],
        "paper_abstract": "The performance of Part-of-Speech tagging varies significantly across the treebanks of the Universal Dependencies project. This work points out that these variations may result from divergences between the annotation of train and test sets. We show how the annotation variation principle, introduced by Dickinson and Meurers (2003) to automatically detect errors in gold standard, can be used to identify inconsistencies between annotations; we also evaluate their impact on prediction performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CCG Parsing Algorithm with Incremental Tree Rotation",
        "paper_url": "https://aclanthology.org/N19-1020/",
        "paper_authors": [
            "Milo\u0161 Stanojevi\u0107",
            "Mark Steedman"
        ],
        "paper_abstract": "The main obstacle to incremental sentence processing arises from right-branching constituent structures, which are present in the majority of English sentences, as well as optional constituents that adjoin on the right, such as right adjuncts and right conjuncts. In CCG, many right-branching derivations can be replaced by semantically equivalent left-branching incremental derivations. The problem of right-adjunction is more resistant to solution, and has been tackled in the past using revealing-based approaches that often rely either on the higher-order unification over lambda terms (Pareschi and Steedman,1987) or heuristics over dependency representations that do not cover the whole CCGbank (Ambati et al., 2015). We propose a new incremental parsing algorithm for CCG following the same revealing tradition of work but having a purely syntactic approach that does not depend on access to a distinct level of semantic representation. This algorithm can cover the whole CCGbank, with greater incrementality and accuracy than previous proposals.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing",
        "paper_url": "https://aclanthology.org/N19-1021/",
        "paper_authors": [
            "Hao Fu",
            "Chunyuan Li",
            "Xiaodong Liu",
            "Jianfeng Gao",
            "Asli Celikyilmaz",
            "Lawrence Carin"
        ],
        "paper_abstract": "Variational autoencoders (VAE) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. VAE objective consists of two terms, the KL regularization term and the reconstruction term, balanced by a weighting hyper-parameter \ud835\udefd. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for \ud835\udefd, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of optimization. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing \ud835\udefd multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including language modeling, dialog response generation and semi-supervised text classification.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Recurrent models and lower bounds for projective syntactic decoding",
        "paper_url": "https://aclanthology.org/N19-1022/",
        "paper_authors": [
            "Natalie Schluter"
        ],
        "paper_abstract": "The current state-of-the-art in neural graph-based parsing uses only approximate decoding at the training phase. In this paper aim to understand this result better. We show how recurrent models can carry out projective maximum spanning tree decoding. This result holds for both current state-of-the-art models for shift-reduce and graph-based parsers, projective or not. We also provide the first proof on the lower bounds of projective maximum spanning tree decoding.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating Composition Models for Verb Phrase Elliptical Sentence Embeddings",
        "paper_url": "https://aclanthology.org/N19-1023/",
        "paper_authors": [
            "Gijs Wijnholds",
            "Mehrnoosh Sadrzadeh"
        ],
        "paper_abstract": "Ellipsis is a natural language phenomenon where part of a sentence is missing and its information must be recovered from its surrounding context, as in \u201cCats chase dogs and so do foxes.\u201d. Formal semantics has different methods for resolving ellipsis and recovering the missing information, but the problem has not been considered for distributional semantics, where words have vector embeddings and combinations thereof provide embeddings for sentences. In elliptical sentences these combinations go beyond linear as copying of elided information is necessary. In this paper, we develop different models for embedding VP-elliptical sentences. We extend existing verb disambiguation and sentence similarity datasets to ones containing elliptical phrases and evaluate our models on these datasets for a variety of non-linear combinations and their linear counterparts. We compare results of these compositional models to state of the art holistic sentence encoders. Our results show that non-linear addition and a non-linear tensor-based composition outperform the naive non-compositional baselines and the linear models, and that sentence encoders perform well on sentence similarity, but not on verb disambiguation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Finite-State Transducers: Beyond Rational Relations",
        "paper_url": "https://aclanthology.org/N19-1024/",
        "paper_authors": [
            "Chu-Cheng Lin",
            "Hao Zhu",
            "Matthew R. Gormley",
            "Jason Eisner"
        ],
        "paper_abstract": "We introduce neural finite state transducers (NFSTs), a family of string transduction models defining joint and conditional probability distributions over pairs of strings. The probability of a string pair is obtained by marginalizing over all its accepting paths in a finite state transducer. In contrast to ordinary weighted FSTs, however, each path is scored using an arbitrary function such as a recurrent neural network, which breaks the usual conditional independence assumption (Markov property). NFSTs are more powerful than previous finite-state models with neural features (Rastogi et al., 2016.) We present training and inference algorithms for locally and globally normalized variants of NFSTs. In experiments on different transduction tasks, they compete favorably against seq2seq models while offering interpretable paths that correspond to hard monotonic alignments.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for Text Modeling",
        "paper_url": "https://aclanthology.org/N19-1025/",
        "paper_authors": [
            "Prince Zizhuang Wang",
            "William Yang Wang"
        ],
        "paper_abstract": "Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks. These models often face a difficult optimization problem, also known as KL vanishing, where the posterior easily collapses to the prior and model will ignore latent codes in generative tasks. To address this problem, we introduce an improved Variational Wasserstein Autoencoder (WAE) with Riemannian Normalizing Flow (RNF) for text modeling. The RNF transforms a latent variable into a space that respects the geometric characteristics of input space, which makes posterior impossible to collapse to the non-informative prior. The Wasserstein objective minimizes the distance between marginal distribution and the prior directly and therefore does not force the posterior to match the prior. Empirical experiments show that our model avoids KL vanishing over a range of datasets and has better performance in tasks such as language modeling, likelihood approximation, and text generation. Through a series of experiments and analysis over latent space, we show that our model learns latent distributions that respect latent space geometry and is able to generate sentences that are more diverse.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Study of Incorrect Paraphrases in Crowdsourced User Utterances",
        "paper_url": "https://aclanthology.org/N19-1026/",
        "paper_authors": [
            "Mohammad-Ali Yaghoub-Zadeh-Fard",
            "Boualem Benatallah",
            "Moshe Chai Barukh",
            "Shayan Zamanirad"
        ],
        "paper_abstract": "Developing bots demands highquality training samples, typically in the form of user utterances and their associated intents. Given the fuzzy nature of human language, such datasets ideally must cover all possible utterances of each single intent. Crowdsourcing has widely been used to collect such inclusive datasets by paraphrasing an initial utterance. However, the quality of this approach often suffers from various issues, particularly language errors produced by unqualified crowd workers. More so, since workers are tasked to write open-ended text, it is very challenging to automatically asses the quality of paraphrased utterances. In this paper, we investigate common crowdsourced paraphrasing issues, and propose an annotated dataset called Para-Quality, for detecting the quality issues. We also investigate existing tools and services to provide baselines for detecting each category of issues. In all, this work presents a data-driven view of incorrect paraphrases during the bot development process, and we pave the way towards automatic detection of unqualified paraphrases.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters",
        "paper_url": "https://aclanthology.org/N19-1027/",
        "paper_authors": [
            "Abdalghani Abujabal",
            "Rishiraj Saha Roy",
            "Mohamed Yahya",
            "Gerhard Weikum"
        ],
        "paper_abstract": "To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers. ComQA contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of crowdsourcing. We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on ComQA, demonstrating that our dataset can be a driver of future research on QA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase",
        "paper_url": "https://aclanthology.org/N19-1028/",
        "paper_authors": [
            "Kelvin Jiang",
            "Dekun Wu",
            "Hui Jiang"
        ],
        "paper_abstract": "In this paper, we present a new data set, named FreebaseQA, for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove any false positive in these matched triples. Using this method, we are able to efficiently generate over 54K matches from about 28K unique questions with minimal cost. Our analysis shows that this data set is suitable for model training in factoid QA tasks beyond simpler questions since FreebaseQA provides more linguistically sophisticated questions than other existing data sets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Simple Question Answering with Subgraph Ranking and Joint-Scoring",
        "paper_url": "https://aclanthology.org/N19-1029/",
        "paper_authors": [
            "Wenbo Zhao",
            "Tagyoung Chung",
            "Anuj Goyal",
            "Angeliki Metallinou"
        ],
        "paper_abstract": "Knowledge graph based simple question answering (KBSQA) is a major area of research within question answering. Although only dealing with simple questions, i.e., questions that can be answered through a single knowledge base (KB) fact, this task is neither simple nor close to being solved. Targeting on the two main steps, subgraph selection and fact selection, the literature has developed sophisticated approaches. However, the importance of subgraph ranking and leveraging the subject\u2013relation dependency of a KB fact have not been sufficiently explored. Motivated by this, we present a unified framework to describe and analyze existing approaches. Using this framework as a starting point we focus on two aspects: improving subgraph selection through a novel ranking method, and leveraging the subject\u2013relation dependency by proposing a joint scoring CNN model with a novel loss function that enforces the well-order of scores. Our methods achieve a new state of the art (85.44% in accuracy) on the SimpleQuestions dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Open-domain Question Answering",
        "paper_url": "https://aclanthology.org/N19-1030/",
        "paper_authors": [
            "Jianmo Ni",
            "Chenguang Zhu",
            "Weizhu Chen",
            "Julian McAuley"
        ],
        "paper_abstract": "Open-domain question answering remains a challenging task as it requires models that are capable of understanding questions and answers, collecting useful information, and reasoning over evidence. Previous work typically formulates this task as a reading comprehension or entailment problem given evidence retrieved from search engines. However, existing techniques struggle to retrieve indirectly related evidence when no directly related evidence is provided, especially for complex questions where it is hard to parse precisely what the question asks. In this paper we propose a retriever-reader model that learns to attend on essential terms during the question answering process. We build (1) an essential term selector which first identifies the most important words in a question, then reformulates the query and searches for related evidence; and (2) an enhanced reader that distinguishes between essential terms and distracting words to predict the answer. We evaluate our model on multiple open-domain QA datasets, notably achieving the level of the state-of-the-art on the AI2 Reasoning Challenge (ARC) dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "UHop: An Unrestricted-Hop Relation Extraction Framework for Knowledge-Based Question Answering",
        "paper_url": "https://aclanthology.org/N19-1031/",
        "paper_authors": [
            "Zi-Yuan Chen",
            "Chih-Hung Chang",
            "Yi-Pei Chen",
            "Jijnasa Nayak",
            "Lun-Wei Ku"
        ],
        "paper_abstract": "In relation extraction for knowledge-based question answering, searching from one entity to another entity via a single relation is called \u201cone hop\u201d. In related work, an exhaustive search from all one-hop relations, two-hop relations, and so on to the max-hop relations in the knowledge graph is necessary but expensive. Therefore, the number of hops is generally restricted to two or three. In this paper, we propose UHop, an unrestricted-hop framework which relaxes this restriction by use of a transition-based search framework to replace the relation-chain-based search one. We conduct experiments on conventional 1- and 2-hop questions as well as lengthy questions, including datasets such as WebQSP, PathQuestion, and Grid World. Results show that the proposed framework enables the ability to halt, works well with state-of-the-art models, achieves competitive performance without exhaustive searches, and opens the performance gap for long relation paths.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering",
        "paper_url": "https://aclanthology.org/N19-1032/",
        "paper_authors": [
            "Yu Cao",
            "Meng Fang",
            "Dacheng Tao"
        ],
        "paper_abstract": "Multi-hop reasoning question answering requires deep comprehension of relationships between various documents and queries. We propose a Bi-directional Attention Entity Graph Convolutional Network (BAG), leveraging relationships between nodes in an entity graph and attention information between a query and the entity graph, to solve this task. Graph convolutional networks are used to obtain a relation-aware representation of nodes for entity graphs built from documents with multi-level features. Bidirectional attention is then applied on graphs and queries to generate a query-aware nodes representation, which will be used for the final prediction. Experimental evaluation shows BAG achieves state-of-the-art accuracy performance on the QAngaroo WIKIHOP dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation",
        "paper_url": "https://aclanthology.org/N19-1033/",
        "paper_authors": [
            "Radu Tudor Ionescu",
            "Andrei Butnaru"
        ],
        "paper_abstract": "In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis",
        "paper_url": "https://aclanthology.org/N19-1034/",
        "paper_authors": [
            "Md Shad Akhtar",
            "Dushyant Chauhan",
            "Deepanway Ghosal",
            "Soujanya Poria",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "Related tasks often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The multi-modal inputs (i.e. text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance. We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment and emotion analysis. Evaluation results suggest that multi-task learning framework offers improvement over the single-task framework. The proposed approach reports new state-of-the-art performance for both sentiment analysis and emotion analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
        "paper_url": "https://aclanthology.org/N19-1035/",
        "paper_authors": [
            "Chi Sun",
            "Luyao Huang",
            "Xipeng Qiu"
        ],
        "paper_abstract": "Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets. The source codes are available at https://github.com/HSLCY/ABSA-BERT-pair.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Variational Approach to Weakly Supervised Document-Level Multi-Aspect Sentiment Classification",
        "paper_url": "https://aclanthology.org/N19-1036/",
        "paper_authors": [
            "Ziqian Zeng",
            "Wenxuan Zhou",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "paper_abstract": "In this paper, we propose a variational approach to weakly supervised document-level multi-aspect sentiment classification. Instead of using user-generated ratings or annotations provided by domain experts, we use target-opinion word pairs as \u201csupervision.\u201d These word pairs can be extracted by using dependency parsers and simple rules. Our objective is to predict an opinion word given a target word while our ultimate goal is to learn a sentiment polarity classifier to predict the sentiment polarity of each aspect given a document. By introducing a latent variable, i.e., the sentiment polarity, to the objective function, we can inject the sentiment polarity classifier to the objective via the variational lower bound. We can learn a sentiment polarity classifier by optimizing the lower bound. We show that our method can outperform weakly supervised baselines on TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "HiGRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition",
        "paper_url": "https://aclanthology.org/N19-1037/",
        "paper_authors": [
            "Wenxiang Jiao",
            "Haiqin Yang",
            "Irwin King",
            "Michael R. Lyu"
        ],
        "paper_abstract": "In this paper, we address three challenges in utterance-level emotion recognition in dialogue systems: (1) the same word can deliver different emotions in different contexts; (2) some emotions are rarely seen in general dialogues; (3) long-range contextual information is hard to be effectively captured. We therefore propose a hierarchical Gated Recurrent Unit (HiGRU) framework with a lower-level GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings. Moreover, we promote the framework to two variants, Hi-GRU with individual features fusion (HiGRU-f) and HiGRU with self-attention and features fusion (HiGRU-sf), so that the word/utterance-level individual inputs and the long-range contextual information can be sufficiently utilized. Experiments on three dialogue emotion datasets, IEMOCAP, Friends, and EmotionPush demonstrate that our proposed Hi-GRU models attain at least 8.7%, 7.5%, 6.0% improvement over the state-of-the-art methods on each dataset, respectively. Particularly, by utilizing only the textual feature in IEMOCAP, our HiGRU models gain at least 3.8% improvement over the state-of-the-art conversational memory network (CMN) with the trimodal features of text, video, and audio.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach",
        "paper_url": "https://aclanthology.org/N19-1038/",
        "paper_authors": [
            "Nicolas Pr\u00f6llochs",
            "Stefan Feuerriegel",
            "Dirk Neumann"
        ],
        "paper_abstract": "Negation scope detection is widely performed as a supervised learning task which relies upon negation labels at word level. This suffers from two key drawbacks: (1) such granular annotations are costly and (2) highly subjective, since, due to the absence of explicit linguistic resolution rules, human annotators often disagree in the perceived negation scopes. To the best of our knowledge, our work presents the first approach that eliminates the need for world-level negation labels, replacing it instead with document-level sentiment annotations. For this, we present a novel strategy for learning fully interpretable negation rules via weak supervision: we apply reinforcement learning to find a policy that reconstructs negation rules from sentiment predictions at document level. Our experiments demonstrate that our approach for weak supervision can effectively learn negation rules. Furthermore, an out-of-sample evaluation via sentiment analysis reveals consistent improvements (of up to 4.66%) over both a sentiment analysis with (i) no negation handling and (ii) the use of word-level annotations from humans. Moreover, the inferred negation rules are fully interpretable.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Simplified Neural Unsupervised Domain Adaptation",
        "paper_url": "https://aclanthology.org/N19-1039/",
        "paper_authors": [
            "Timothy Miller"
        ],
        "paper_abstract": "Unsupervised domain adaptation (UDA) is the task of training a statistical model on labeled data from a source domain to achieve better performance on data from a target domain, with access to only unlabeled data in the target domain. Existing state-of-the-art UDA approaches use neural networks to learn representations that are trained to predict the values of subset of important features called \u201cpivot features\u201d on combined data from the source and target domains. In this work, we show that it is possible to improve on existing neural domain adaptation algorithms by 1) jointly training the representation learner with the task learner; and 2) removing the need for heuristically-selected \u201cpivot features.\u201d Our results show competitive performance with a simpler model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Bilingual Sentiment-Specific Word Embeddings without Cross-lingual Supervision",
        "paper_url": "https://aclanthology.org/N19-1040/",
        "paper_authors": [
            "Yanlin Feng",
            "Xiaojun Wan"
        ],
        "paper_abstract": "Word embeddings learned in two languages can be mapped to a common space to produce Bilingual Word Embeddings (BWE). Unsupervised BWE methods learn such a mapping without any parallel data. However, these methods are mainly evaluated on tasks of word translation or word similarity. We show that these methods fail to capture the sentiment information and do not perform well enough on cross-lingual sentiment analysis. In this work, we propose UBiSE (Unsupervised Bilingual Sentiment Embeddings), which learns sentiment-specific word representations for two languages in a common space without any cross-lingual supervision. Our method only requires a sentiment corpus in the source language and pretrained monolingual word embeddings of both languages. We evaluate our method on three language pairs for cross-lingual sentiment analysis. Experimental results show that our method outperforms previous unsupervised BWE methods and even supervised BWE methods. Our method succeeds for a distant language pair English-Basque.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems",
        "paper_url": "https://aclanthology.org/N19-1041/",
        "paper_authors": [
            "Inigo Jauregi Unanue",
            "Ehsan Zare Borzeshi",
            "Nazanin Esmaili",
            "Massimo Piccardi"
        ],
        "paper_abstract": "Regularization of neural machine translation is still a significant problem, especially in low-resource settings. To mollify this problem, we propose regressing word embeddings (ReWE) as a new regularization technique in a system that is jointly trained to predict the next word in the translation (categorical value) and its word embedding (continuous value). Such a joint training allows the proposed system to learn the distributional properties represented by the word embeddings, empirically improving the generalization to unseen sentences. Experiments over three translation datasets have showed a consistent improvement over a strong baseline, ranging between 0.91 and 2.4 BLEU points, and also a marked improvement over a state-of-the-art system.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Lost in Machine Translation: A Method to Reduce Meaning Loss",
        "paper_url": "https://aclanthology.org/N19-1042/",
        "paper_authors": [
            "Reuben Cohn-Gordon",
            "Noah Goodman"
        ],
        "paper_abstract": "A desideratum of high-quality translation systems is that they preserve meaning, in the sense that two sentences with different meanings should not translate to one and the same sentence in another language. However, state-of-the-art systems often fail in this regard, particularly in cases where the source and target languages partition the \u201cmeaning space\u201d in different ways. For instance, \u201cI cut my finger.\u201d and \u201cI cut my finger off.\u201d describe different states of the world but are translated to French (by both Fairseq and Google Translate) as \u201cJe me suis coup\u00e9 le doigt.\u201d, which is ambiguous as to whether the finger is detached. More generally, translation systems are typically many-to-one (non-injective) functions from source to target language, which in many cases results in important distinctions in meaning being lost in translation. Building on Bayesian models of informative utterance production, we present a method to define a less ambiguous translation system in terms of an underlying pre-trained neural sequence-to-sequence model. This method increases injectivity, resulting in greater preservation of meaning as measured by improvement in cycle-consistency, without impeding translation quality (measured by BLEU score).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bi-Directional Differentiable Input Reconstruction for Low-Resource Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1043/",
        "paper_authors": [
            "Xing Niu",
            "Weijia Xu",
            "Marine Carpuat"
        ],
        "paper_abstract": "We aim to better exploit the limited amounts of parallel text available in low-resource settings by introducing a differentiable reconstruction loss for neural machine translation (NMT). This loss compares original inputs to reconstructed inputs, obtained by back-translating translation hypotheses into the input language. We leverage differentiable sampling and bi-directional NMT to train models end-to-end, without introducing additional parameters. This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Code-Switching for Enhancing NMT with Pre-Specified Translation",
        "paper_url": "https://aclanthology.org/N19-1044/",
        "paper_authors": [
            "Kai Song",
            "Yue Zhang",
            "Heng Yu",
            "Weihua Luo",
            "Kun Wang",
            "Min Zhang"
        ],
        "paper_abstract": "Leveraging user-provided translation to constrain NMT has practical significance. Existing methods can be classified into two main categories, namely the use of placeholder tags for lexicon words and the use of hard constraints during decoding. Both methods can hurt translation fidelity for various reasons. We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations. Our method does not change the MNT model or decoding algorithm, allowing the model to learn lexicon translations by copying source-side target words. Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Aligning Vector-spaces with Noisy Supervised Lexicon",
        "paper_url": "https://aclanthology.org/N19-1045/",
        "paper_authors": [
            "Noa Yehezkel Lubin",
            "Jacob Goldberger",
            "Yoav Goldberg"
        ],
        "paper_abstract": "The problem of learning to translate between two vector spaces given a set of aligned points arises in several application areas of NLP. Current solutions assume that the lexicon which defines the alignment pairs is noise-free. We consider the case where the set of aligned points is allowed to contain an amount of noise, in the form of incorrect lexicon pairs and show that this arises in practice by analyzing the edited dictionaries after the cleaning process. We demonstrate that such noise substantially degrades the accuracy of the learned translation when using current methods. We propose a model that accounts for noisy pairs. This is achieved by introducing a generative model with a compatible iterative EM algorithm. The algorithm jointly learns the noise level in the lexicon, finds the set of noisy pairs, and learns the mapping between the spaces. We demonstrate the effectiveness of our proposed algorithm on two alignment problems: bilingual word embedding translation, and mapping between diachronic embedding spaces for recovering the semantic shifts of words across time periods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding and Improving Hidden Representations for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1046/",
        "paper_authors": [
            "Guanlin Li",
            "Lemao Liu",
            "Xintong Li",
            "Conghui Zhu",
            "Tiejun Zhao",
            "Shuming Shi"
        ],
        "paper_abstract": "Multilayer architectures are currently the gold standard for large-scale neural machine translation. Existing works have explored some methods for understanding the hidden representations, however, they have not sought to improve the translation quality rationally according to their understanding. Towards understanding for performance improvement, we first artificially construct a sequence of nested relative tasks and measure the feature generalization ability of the learned hidden representation over these tasks. Based on our understanding, we then propose to regularize the layer-wise representations with all tree-induced tasks. To overcome the computational bottleneck resulting from the large number of regularization terms, we design efficient approximation methods by selecting a few coarse-to-fine tasks for regularization. Extensive experiments on two widely-used datasets demonstrate the proposed methods only lead to small extra overheads in training but no additional overheads in testing, and achieve consistent improvements (up to +1.3 BLEU) compared to the state-of-the-art translation model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Content Differences in Syntactic and Semantic Representation",
        "paper_url": "https://aclanthology.org/N19-1047/",
        "paper_authors": [
            "Daniel Hershcovich",
            "Omri Abend",
            "Ari Rappoport"
        ],
        "paper_abstract": "Syntactic analysis plays an important role in semantic parsing, but the nature of this role remains a topic of ongoing debate. The debate has been constrained by the scarcity of empirical comparative studies between syntactic and semantic schemes, which hinders the development of parsing methods informed by the details of target schemes and constructions. We target this gap, and take Universal Dependencies (UD) and UCCA as a test case. After abstracting away from differences of convention or formalism, we find that most content divergences can be ascribed to: (1) UCCA\u2019s distinction between a Scene and a non-Scene; (2) UCCA\u2019s distinction between primary relations, secondary ones and participants; (3) different treatment of multi-word expressions, and (4) different treatment of inter-clause linkage. We further discuss the long tail of cases where the two schemes take markedly different approaches. Finally, we show that the proposed comparison methodology can be used for fine-grained evaluation of UCCA parsing, highlighting both challenges and potential sources for improvement. The substantial differences between the schemes suggest that semantic parsers are likely to benefit downstream text understanding applications beyond their syntactic counterparts.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts",
        "paper_url": "https://aclanthology.org/N19-1048/",
        "paper_authors": [
            "Timo Schick",
            "Hinrich Sch\u00fctze"
        ],
        "paper_abstract": "Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word\u2019s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating Style Transfer for Text",
        "paper_url": "https://aclanthology.org/N19-1049/",
        "paper_authors": [
            "Remi Mir",
            "Bjarke Felbo",
            "Nick Obradovich",
            "Iyad Rahwan"
        ],
        "paper_abstract": "Research in the area of style transfer for text is currently bottlenecked by a lack of standard evaluation practices. This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset. We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work. We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment: direction-corrected Earth Mover\u2019s Distance, Word Mover\u2019s Distance on style-masked texts, and adversarial classification for the respective aspects. We also show that the three examined models exhibit tradeoffs between aspects of interest, demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots. We release software with our evaluation metrics to facilitate research.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Big BiRD: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition",
        "paper_url": "https://aclanthology.org/N19-1050/",
        "paper_authors": [
            "Shima Asaadi",
            "Saif Mohammad",
            "Svetlana Kiritchenko"
        ],
        "paper_abstract": "Bigrams (two-word sequences) hold a special place in semantic composition research since they are the smallest unit formed by composing words. A semantic relatedness dataset that includes bigrams will thus be useful in the development of automatic methods of semantic composition. However, existing relatedness datasets only include pairs of unigrams (single words). Further, existing datasets were created using rating scales and thus suffer from limitations such as in consistent annotations and scale region bias. In this paper, we describe how we created a large, fine-grained, bigram relatedness dataset (BiRD), using a comparative annotation technique called Best\u2013Worst Scaling. Each of BiRD\u2019s 3,345 English term pairs involves at least one bigram. We show that the relatedness scores obtained are highly reliable (split-half reliability r= 0.937). We analyze the data to obtain insights into bigram semantic relatedness. Finally, we present benchmark experiments on using the relatedness dataset as a testbed to evaluate simple unsupervised measures of semantic composition. BiRD is made freely available to foster further research on how meaning can be represented and how meaning can be composed.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Outlier Detection for Improved Data Quality and Diversity in Dialog Systems",
        "paper_url": "https://aclanthology.org/N19-1051/",
        "paper_authors": [
            "Stefan Larson",
            "Anish Mahendran",
            "Andrew Lee",
            "Jonathan K. Kummerfeld",
            "Parker Hill",
            "Michael A. Laurenzano",
            "Johann Hauswald",
            "Lingjia Tang",
            "Jason Mars"
        ],
        "paper_abstract": "In a corpus of data, outliers are either errors: mistakes in the data that are counterproductive, or are unique: informative samples that improve model robustness. Identifying outliers can lead to better datasets by (1) removing noise in datasets and (2) guiding collection of additional data to fill gaps. However, the problem of detecting both outlier types has received relatively little attention in NLP, particularly for dialog systems. We introduce a simple and effective technique for detecting both erroneous and unique samples in a corpus of short texts using neural sentence embeddings combined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iteratively mine unique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Asking the Right Question: Inferring Advice-Seeking Intentions from Personal Narratives",
        "paper_url": "https://aclanthology.org/N19-1052/",
        "paper_authors": [
            "Liye Fu",
            "Jonathan P. Chang",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "paper_abstract": "People often share personal narratives in order to seek advice from others. To properly infer the narrator\u2019s intention, one needs to apply a certain degree of common sense and social intuition. To test the capabilities of NLP systems to recover such intuition, we introduce the new task of inferring what is the advice-seeking goal behind a personal narrative. We formulate this as a cloze test, where the goal is to identify which of two advice-seeking questions was removed from a given narrative. The main challenge in constructing this task is finding pairs of semantically plausible advice-seeking questions for given narratives. To address this challenge, we devise a method that exploits commonalities in experiences people share online to automatically extract pairs of questions that are appropriate candidates for the cloze task. This results in a dataset of over 20,000 personal narratives, each matched with a pair of related advice-seeking questions: one actually intended by the narrator, and the other one not. The dataset covers a very broad array of human experiences, from dating, to career options, to stolen iPads. We use human annotation to determine the degree to which the task relies on common sense and social intuition in addition to a semantic understanding of the narrative. By introducing several baselines for this new task we demonstrate its feasibility and identify avenues for better modeling the intention of the narrator.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Seeing Things from a Different Angle:Discovering Diverse Perspectives about Claims",
        "paper_url": "https://aclanthology.org/N19-1053/",
        "paper_authors": [
            "Sihao Chen",
            "Daniel Khashabi",
            "Wenpeng Yin",
            "Chris Callison-Burch",
            "Dan Roth"
        ],
        "paper_abstract": "One key consequence of the information revolution is a significant increase and a contamination of our information supply. The practice of fact checking won\u2019t suffice to eliminate the biases in text data we observe, as the degree of factuality alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as \u201canimals should have lawful rights\u201d, and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a natural language understanding task, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct PERSPECTRUM, a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify our dataset. We use crowd-sourcing to filter out noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively. We provide a thorough analysis of the dataset to highlight key underlying language understanding challenges, and show that human baselines across multiple subtasks far outperform ma-chine baselines built upon state-of-the-art NLP techniques. This poses a challenge and opportunity for the NLP community to address.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "IMHO Fine-Tuning Improves Claim Detection",
        "paper_url": "https://aclanthology.org/N19-1054/",
        "paper_authors": [
            "Tuhin Chakrabarty",
            "Christopher Hidey",
            "Kathy McKeown"
        ],
        "paper_abstract": "Claims are the central component of an argument. Detecting claims across different domains or data sets can often be challenging due to their varying conceptualization. We propose to alleviate this problem by fine-tuning a language model using a Reddit corpus of 5.5 million opinionated claims. These claims are self-labeled by their authors using the internet acronyms IMO/IMHO (in my (humble) opinion). Empirical results show that using this approach improves the state of art performance across four benchmark argumentation data sets by an average of 4 absolute F1 points in claim detection. As these data sets include diverse domains such as social media and student essays this improvement demonstrates the robustness of fine-tuning on this novel corpus.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Joint Multiple Intent Detection and Slot Labeling for Goal-Oriented Dialog",
        "paper_url": "https://aclanthology.org/N19-1055/",
        "paper_authors": [
            "Rashmi Gangadharaiah",
            "Balakrishnan Narayanaswamy"
        ],
        "paper_abstract": "Neural network models have recently gained traction for sentence-level intent classification and token-based slot-label identification. In many real-world scenarios, users have multiple intents in the same utterance, and a token-level slot label can belong to more than one intent. We investigate an attention-based neural network model that performs multi-label classification for identifying multiple intents and produces labels for both intents and slot-labels at the token-level. We show state-of-the-art performance for both intent detection and slot-label identification by comparing against strong, recently proposed models. Our model provides a small but statistically significant improvement of 0.2% on the predominantly single-intent ATIS public data set, and 55% intent accuracy improvement on an internal multi-intent dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CITE: A Corpus of Image-Text Discourse Relations",
        "paper_url": "https://aclanthology.org/N19-1056/",
        "paper_authors": [
            "Malihe Alikhani",
            "Sreyasi Nag Chowdhury",
            "Gerard de Melo",
            "Matthew Stone"
        ],
        "paper_abstract": "This paper presents a novel crowd-sourced resource for multimodal discourse: our resource characterizes inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations. Like previous corpora annotating discourse structure between text arguments, such as the Penn Discourse Treebank, our new corpus aids in establishing a better understanding of natural communication and common-sense reasoning, while our findings have implications for a wide range of applications, such as understanding and generation of multimodal documents.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Dialogue State Tracking by Discerning the Relevant Context",
        "paper_url": "https://aclanthology.org/N19-1057/",
        "paper_authors": [
            "Sanuj Sharma",
            "Prafulla Kumar Choubey",
            "Ruihong Huang"
        ],
        "paper_abstract": "A typical conversation comprises of multiple turns between participants where they go back and forth between different topics. At each user turn, dialogue state tracking (DST) aims to estimate user\u2019s goal by processing the current utterance. However, in many turns, users implicitly refer to the previous goal, necessitating the use of relevant dialogue history. Nonetheless, distinguishing relevant history is challenging and a popular method of using dialogue recency for that is inefficient. We, therefore, propose a novel framework for DST that identifies relevant historical context by referring to the past utterances where a particular slot-value changes and uses that together with weighted system utterance to identify the relevant context. Specifically, we use the current user utterance and the most recent system utterance to determine the relevance of a system utterance. Empirical analyses show that our method improves joint goal accuracy by 2.75% and 2.36% on WoZ 2.0 and Multi-WoZ restaurant domain datasets respectively over the previous state-of-the-art GLAD model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog",
        "paper_url": "https://aclanthology.org/N19-1058/",
        "paper_authors": [
            "Satwik Kottur",
            "Jos\u00e9 M. F. Moura",
            "Devi Parikh",
            "Dhruv Batra",
            "Marcus Rohrbach"
        ],
        "paper_abstract": "Visual Dialog is a multimodal task of answering a sequence of questions grounded in an image (using the conversation history as context). It entails challenges in vision, language, reasoning, and grounding. However, studying these subtasks in isolation on large, real datasets is infeasible as it requires prohibitively-expensive complete annotation of the \u2018state\u2019 of all images and dialogs. We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round reasoning in visual dialog. Specifically, we construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset. This combination results in a dataset where all aspects of the visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs. We use CLEVR-Dialog to benchmark performance of standard visual dialog models; in particular, on visual coreference resolution (as a function of the coreference distance). This is the first analysis of its kind for visual dialog models that was not possible without this dataset. We hope the findings from CLEVR-Dialog will help inform the development of future models for visual dialog. Our code and dataset are publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Outside the Box: Discourse-level Features Improve Metaphor Identification",
        "paper_url": "https://aclanthology.org/N19-1059/",
        "paper_authors": [
            "Jesse Mu",
            "Helen Yannakoudakis",
            "Ekaterina Shutova"
        ],
        "paper_abstract": "Most current approaches to metaphor identification use restricted linguistic contexts, e.g. by considering only a verb\u2019s arguments or the sentence containing a phrase. Inspired by pragmatic accounts of metaphor, we argue that broader discourse features are crucial for better metaphor identification. We train simple gradient boosting classifiers on representations of an utterance and its surrounding discourse learned with a variety of document embedding methods, obtaining near state-of-the-art results on the 2018 VU Amsterdam metaphor identification task without the complex metaphor-specific features or deep neural architectures employed by other systems. A qualitative analysis further confirms the need for broader context in metaphor processing.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detection of Abusive Language: the Problem of Biased Datasets",
        "paper_url": "https://aclanthology.org/N19-1060/",
        "paper_authors": [
            "Michael Wiegand",
            "Josef Ruppenhofer",
            "Thomas Kleinbauer"
        ],
        "paper_abstract": "We discuss the impact of data bias on abusive language detection. We show that classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling. Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
        "paper_url": "https://aclanthology.org/N19-1061/",
        "paper_authors": [
            "Hila Gonen",
            "Yoav Goldberg"
        ],
        "paper_abstract": "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-1062/",
        "paper_authors": [
            "Thomas Manzini",
            "Lim Yao Chong",
            "Alan W Black",
            "Yulia Tsvetkov"
        ],
        "paper_abstract": "Online texts - across genres, registers, domains, and styles - are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting, such as binary gender. Next, we propose a novel methodology for the evaluation of multiclass debiasing. We demonstrate that our multiclass debiasing is robust and maintains the efficacy in standard NLP tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On Measuring Social Biases in Sentence Encoders",
        "paper_url": "https://aclanthology.org/N19-1063/",
        "paper_authors": [
            "Chandler May",
            "Alex Wang",
            "Shikha Bordia",
            "Samuel R. Bowman",
            "Rachel Rudinger"
        ],
        "paper_abstract": "The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Gender Bias in Contextualized Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-1064/",
        "paper_authors": [
            "Jieyu Zhao",
            "Tianlu Wang",
            "Mark Yatskar",
            "Ryan Cotterell",
            "Vicente Ordonez",
            "Kai-Wei Chang"
        ],
        "paper_abstract": "In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo\u2019s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Combining Sentiment Lexica with a Multi-View Variational Autoencoder",
        "paper_url": "https://aclanthology.org/N19-1065/",
        "paper_authors": [
            "Alexander Miserlis Hoyle",
            "Lawrence Wolf-Sonkin",
            "Hanna Wallach",
            "Ryan Cotterell",
            "Isabelle Augenstein"
        ],
        "paper_abstract": "When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enhancing Opinion Role Labeling with Semantic-Aware Word Representations from Semantic Role Labeling",
        "paper_url": "https://aclanthology.org/N19-1066/",
        "paper_authors": [
            "Meishan Zhang",
            "Peili Liang",
            "Guohong Fu"
        ],
        "paper_abstract": "Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Frowning Frodo, Wincing Leia, and a Seriously Great Friendship: Learning to Classify Emotional Relationships of Fictional Characters",
        "paper_url": "https://aclanthology.org/N19-1067/",
        "paper_authors": [
            "Evgeny Kim",
            "Roman Klinger"
        ],
        "paper_abstract": "The development of a fictional plot is centered around characters who closely interact with each other forming dynamic social networks. In literature analysis, such networks have mostly been analyzed without particular relation types or focusing on roles which the characters take with respect to each other. We argue that an important aspect for the analysis of stories and their development is the emotion between characters. In this paper, we combine these aspects into a unified framework to classify emotional relationships of fictional characters. We formalize it as a new task and describe the annotation of a corpus, based on fan-fiction short stories. The extraction pipeline which we propose consists of character identification (which we treat as given by an oracle here) and the relation classification. For the latter, we provide results using several approaches previously proposed for relation identification with neural methods. The best result of 0.45 F1 is achieved with a GRU with character position indicators on the task of predicting undirected emotion relations in the associated social network graph.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generalizing Unmasking for Short Texts",
        "paper_url": "https://aclanthology.org/N19-1068/",
        "paper_authors": [
            "Janek Bevendorff",
            "Benno Stein",
            "Matthias Hagen",
            "Martin Potthast"
        ],
        "paper_abstract": "Authorship verification is the problem of inferring whether two texts were written by the same author. For this task, unmasking is one of the most robust approaches as of today with the major shortcoming of only being applicable to book-length texts. In this paper, we present a generalized unmasking approach which allows for authorship verification of texts as short as four printed pages with very high precision at an adjustable recall tradeoff. Our generalized approach therefore reduces the required material by orders of magnitude, making unmasking applicable to authorship cases of more practical proportions. The new approach is on par with other state-of-the-art techniques that are optimized for texts of this length: it achieves accuracies of 75\u201380%, while also allowing for easy adjustment to forensic scenarios that require higher levels of confidence in the classification.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adversarial Training for Satire Detection: Controlling for Confounding Variables",
        "paper_url": "https://aclanthology.org/N19-1069/",
        "paper_authors": [
            "Robert McHardy",
            "Heike Adel",
            "Roman Klinger"
        ],
        "paper_abstract": "The automatic detection of satire vs. regular news is relevant for downstream applications (for instance, knowledge base population) and to improve the understanding of linguistic characteristics of satire. Recent approaches build upon corpora which have been labeled automatically based on article sources. We hypothesize that this encourages the models to learn characteristics for different publication sources (e.g., \u201cThe Onion\u201d vs. \u201cThe Guardian\u201d) rather than characteristics of satire, leading to poor generalization performance to unseen publication sources. We therefore propose a novel model for satire detection with an adversarial component to control for the confounding variable of publication source. On a large novel data set collected from German news (which we make available to the research community), we observe comparable satire classification performance and, as desired, a considerable drop in publication classification performance with adversarial training. Our analysis shows that the adversarial component is crucial for the model to learn to pay attention to linguistic properties of satire.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Keyphrase Generation: A Text Summarization Struggle",
        "paper_url": "https://aclanthology.org/N19-1070/",
        "paper_authors": [
            "Erion \u00c7ano",
            "Ond\u0159ej Bojar"
        ],
        "paper_abstract": "Authors\u2019 keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the simpler unsupervised methods, or the existing supervised ones.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SEQ\u02c63: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression",
        "paper_url": "https://aclanthology.org/N19-1071/",
        "paper_authors": [
            "Christos Baziotis",
            "Ion Androutsopoulos",
            "Ioannis Konstas",
            "Alexandros Potamianos"
        ],
        "paper_abstract": "Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ\u02c63), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the model to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from categorical distributions, allowing gradient-based optimization, unlike alternatives that rely on reinforcement learning. The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation",
        "paper_url": "https://aclanthology.org/N19-1072/",
        "paper_authors": [
            "Ori Shapira",
            "David Gabay",
            "Yang Gao",
            "Hadar Ronen",
            "Ramakanth Pasunuru",
            "Mohit Bansal",
            "Yael Amsterdamer",
            "Ido Dagan"
        ],
        "paper_abstract": "Conducting a manual evaluation is considered an essential part of summary evaluation methodology. Traditionally, the Pyramid protocol, which exhaustively compares system summaries to references, has been perceived as very reliable, providing objective scores. Yet, due to the high cost of the Pyramid method and the required expertise, researchers resorted to cheaper and less thorough manual evaluation methods, such as Responsiveness and pairwise comparison, attainable via crowdsourcing. We revisit the Pyramid approach, proposing a lightweight sampling-based version that is crowdsourcable. We analyze the performance of our method in comparison to original expert-based Pyramid evaluations, showing higher correlation relative to the common Responsiveness method. We release our crowdsourced Summary-Content-Units, along with all crowdsourcing scripts, for future evaluations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Serial Recall Effects in Neural Language Modeling",
        "paper_url": "https://aclanthology.org/N19-1073/",
        "paper_authors": [
            "Hassan Hajipoor",
            "Hadi Amiri",
            "Maseud Rahgozar",
            "Farhad Oroumchian"
        ],
        "paper_abstract": "Serial recall experiments study the ability of humans to recall words in the order in which they occurred. The following serial recall effects are generally investigated in studies with humans: word length and frequency, primacy and recency, semantic confusion, repetition, and transposition effects. In this research, we investigate neural language models in the context of these serial recall effects. Our work provides a framework to better understand and analyze neural language models and opens a new window to develop accurate language models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fast Concept Mention Grouping for Concept Map-based Multi-Document Summarization",
        "paper_url": "https://aclanthology.org/N19-1074/",
        "paper_authors": [
            "Tobias Falke",
            "Iryna Gurevych"
        ],
        "paper_abstract": "Concept map-based multi-document summarization has recently been proposed as a variant of the traditional summarization task with graph-structured summaries. As shown by previous work, the grouping of coreferent concept mentions across documents is a crucial subtask of it. However, while the current state-of-the-art method suggested a new grouping method that was shown to improve the summary quality, its use of pairwise comparisons leads to polynomial runtime complexity that prohibits the application to large document collections. In this paper, we propose two alternative grouping techniques based on locality sensitive hashing, approximate nearest neighbor search and a fast clustering algorithm. They exhibit linear and log-linear runtime complexity, making them much more scalable. We report experimental results that confirm the improved runtime behavior while also showing that the quality of the summary concept maps remains comparable.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Syntax-aware Neural Semantic Role Labeling with Supertags",
        "paper_url": "https://aclanthology.org/N19-1075/",
        "paper_authors": [
            "Jungo Kasai",
            "Dan Friedman",
            "Robert Frank",
            "Dragomir Radev",
            "Owen Rambow"
        ],
        "paper_abstract": "We introduce a new syntax-aware model for dependency-based semantic role labeling that outperforms syntax-agnostic models for English and Spanish. We use a BiLSTM to tag the text with supertags extracted from dependency parses, and we feed these supertags, along with words and parts of speech, into a deep highway BiLSTM for semantic role labeling. Our model combines the strengths of earlier models that performed SRL on the basis of a full dependency parse with more recent models that use no syntactic information at all. Our local and non-ensemble model achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets. SRL models benefit from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Left-to-Right Dependency Parsing with Pointer Networks",
        "paper_url": "https://aclanthology.org/N19-1076/",
        "paper_authors": [
            "Daniel Fern\u00e1ndez-Gonz\u00e1lez",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "paper_abstract": "We propose a novel transition-based algorithm that straightforwardly parses sentences from left to right by building n attachments, with n being the length of the input sentence. Similarly to the recent stack-pointer parser by Ma et al. (2018), we use the pointer network framework that, given a word, can directly point to a position from the sentence. However, our left-to-right approach is simpler than the original top-down stack-pointer parser (not requiring a stack) and reduces transition sequence length in half, from 2n-1 actions to n. This results in a quadratic non-projective parser that runs twice as fast as the original while achieving the best accuracy to date on the English PTB dataset (96.04% UAS, 94.43% LAS) among fully-supervised single-model dependency parsers, and improves over the former top-down transition system in the majority of languages tested.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Viable Dependency Parsing as Sequence Labeling",
        "paper_url": "https://aclanthology.org/N19-1077/",
        "paper_authors": [
            "Michalina Strzyz",
            "David Vilares",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "paper_abstract": "We recast dependency parsing as a sequence labeling problem, exploring several encodings of dependency trees as labels. While dependency parsing by means of sequence labeling had been attempted in existing work, results suggested that the technique was impractical. We show instead that with a conventional BILSTM-based model it is possible to obtain fast and accurate parsers. These parsers are conceptually simple, not needing traditional parsing algorithms or auxiliary structures. However, experiments on the PTB and a sample of UD treebanks show that they provide a good speed-accuracy tradeoff, with results competitive with more complex approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pooled Contextualized Embeddings for Named Entity Recognition",
        "paper_url": "https://aclanthology.org/N19-1078/",
        "paper_authors": [
            "Alan Akbik",
            "Tanja Bergmann",
            "Roland Vollgraf"
        ],
        "paper_abstract": "Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a \u201dglobal\u201d word representation from all contextualized instances. We evaluate these \u201dpooled contextualized embeddings\u201d on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Better Modeling of Incomplete Annotations for Named Entity Recognition",
        "paper_url": "https://aclanthology.org/N19-1079/",
        "paper_authors": [
            "Zhanming Jie",
            "Pengjun Xie",
            "Wei Lu",
            "Ruixue Ding",
            "Linlin Li"
        ],
        "paper_abstract": "Supervised approaches to named entity recognition (NER) are largely developed based on the assumption that the training data is fully annotated with named entity information. However, in practice, annotated data can often be imperfect with one typical issue being the training data may contain incomplete annotations. We highlight several pitfalls associated with learning under such a setup in the context of NER and identify limitations associated with existing approaches, proposing a novel yet easy-to-implement approach for recognizing named entities with incomplete data annotations. We demonstrate the effectiveness of our approach through extensive experiments.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Event Detection without Triggers",
        "paper_url": "https://aclanthology.org/N19-1080/",
        "paper_authors": [
            "Shulin Liu",
            "Yang Li",
            "Feng Zhang",
            "Tao Yang",
            "Xinpeng Zhou"
        ],
        "paper_abstract": "The goal of event detection (ED) is to detect the occurrences of events and categorize them. Previous work solved this task by recognizing and classifying event triggers, which is defined as the word or phrase that most clearly expresses an event occurrence. As a consequence, existing approaches required both annotated triggers and event types in training data. However, triggers are nonessential to event detection, and it is time-consuming for annotators to pick out the \u201cmost clearly\u201d word from a given sentence, especially from a long sentence. The expensive annotation of training corpus limits the application of existing approaches. To reduce manual effort, we explore detecting events without triggers. In this work, we propose a novel framework dubbed as Type-aware Bias Neural Network with Attention Mechanisms (TBNNAM), which encodes the representation of a sentence based on target event types. Experimental results demonstrate the effectiveness. Remarkably, the proposed approach even achieves competitive performances compared with state-of-the-arts that used annotated triggers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Sub-event detection from twitter streams as a sequence labeling problem",
        "paper_url": "https://aclanthology.org/N19-1081/",
        "paper_authors": [
            "Giannis Bekoulis",
            "Johannes Deleu",
            "Thomas Demeester",
            "Chris Develder"
        ],
        "paper_abstract": "This paper introduces improved methods for sub-event detection in social media streams, by applying neural sequence models not only on the level of individual posts, but also directly on the stream level. Current approaches to identify sub-events within a given event, such as a goal during a soccer match, essentially do not exploit the sequential nature of social media streams. We address this shortcoming by framing the sub-event detection problem in social media streams as a sequence labeling task and adopt a neural sequence architecture that explicitly accounts for the chronological order of posts. Specifically, we (i) establish a neural baseline that outperforms a graph-based state-of-the-art method for binary sub-event detection (2.7% micro-F1 improvement), as well as (ii) demonstrate superiority of a recurrent neural network model on the posts sequence level for labeled sub-events (2.4% bin-level F1 improvement over non-sequential models).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GraphIE: A Graph-Based Framework for Information Extraction",
        "paper_url": "https://aclanthology.org/N19-1082/",
        "paper_authors": [
            "Yujie Qian",
            "Enrico Santus",
            "Zhijing Jin",
            "Jiang Guo",
            "Regina Barzilay"
        ],
        "paper_abstract": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "OpenKI: Integrating Open Information Extraction and Knowledge Bases with Relation Inference",
        "paper_url": "https://aclanthology.org/N19-1083/",
        "paper_authors": [
            "Dongxu Zhang",
            "Subhabrata Mukherjee",
            "Colin Lockard",
            "Luna Dong",
            "Andrew McCallum"
        ],
        "paper_abstract": "In this paper, we consider advancing web-scale knowledge extraction and alignment by integrating OpenIE extractions in the form of (subject, predicate, object) triples with Knowledge Bases (KB). Traditional techniques from universal schema and from schema mapping fall in two extremes: either they perform instance-level inference relying on embedding for (subject, object) pairs, thus cannot handle pairs absent in any existing triples; or they perform predicate-level mapping and completely ignore background evidence from individual entities, thus cannot achieve satisfying quality. We propose OpenKI to handle sparsity of OpenIE extractions by performing instance-level inference: for each entity, we encode the rich information in its neighborhood in both KB and OpenIE extractions, and leverage this information in relation inference by exploring different methods of aggregation and attention. In order to handle unseen entities, our model is designed without creating entity-specific parameters. Extensive experiments show that this method not only significantly improves state-of-the-art for conventional OpenIE extractions like ReVerb, but also boosts the performance on OpenIE from semi-structured data, where new entity pairs are abundant and data are fairly sparse.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing",
        "paper_url": "https://aclanthology.org/N19-1084/",
        "paper_authors": [
            "Wenhan Xiong",
            "Jiawei Wu",
            "Deren Lei",
            "Mo Yu",
            "Shiyu Chang",
            "Xiaoxiao Guo",
            "William Yang Wang"
        ],
        "paper_abstract": "Existing entity typing systems usually exploit the type hierarchy provided by knowledge base (KB) schema to model label correlations and thus improve the overall performance. Such techniques, however, are not directly applicable to more open and practical scenarios where the type set is not restricted by KB schema and includes a vast number of free-form types. To model the underlying label correlations without access to manually annotated label structures, we introduce a novel label-relational inductive bias, represented by a graph propagation layer that effectively encodes both global label co-occurrence statistics and word-level similarities. On a large dataset with over 10,000 free-form types, the graph-enhanced model equipped with an attention-based matching module is able to achieve a much higher recall score while maintaining a high-level precision. Specifically, it achieves a 15.3% relative F1 improvement and also less inconsistency in the outputs. We further show that a simple modification of our proposed graph layer can also improve the performance on a conventional and widely-tested dataset that only includes KB-schema types.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Event Coreference Resolution by Learning Argument Compatibility from Unlabeled Data",
        "paper_url": "https://aclanthology.org/N19-1085/",
        "paper_authors": [
            "Yin Jou Huang",
            "Jing Lu",
            "Sadao Kurohashi",
            "Vincent Ng"
        ],
        "paper_abstract": "Argument compatibility is a linguistic condition that is frequently incorporated into modern event coreference resolution systems. If two event mentions have incompatible arguments in any of the argument roles, they cannot be coreferent. On the other hand, if these mentions have compatible arguments, then this may be used as information towards deciding their coreferent status. One of the key challenges in leveraging argument compatibility lies in the paucity of labeled data. In this work, we propose a transfer learning framework for event coreference resolution that utilizes a large amount of unlabeled data to learn argument compatibility of event mentions. In addition, we adopt an interactive inference network based model to better capture the compatible and incompatible relations between the context words of event mentions. Our experiments on the KBP 2017 English dataset confirm the effectiveness of our model in learning argument compatibility, which in turn improves the performance of the overall event coreference model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Sentence Embedding Alignment for Lifelong Relation Extraction",
        "paper_url": "https://aclanthology.org/N19-1086/",
        "paper_authors": [
            "Hong Wang",
            "Wenhan Xiong",
            "Mo Yu",
            "Xiaoxiao Guo",
            "Shiyu Chang",
            "William Yang Wang"
        ],
        "paper_abstract": "Conventional approaches to relation extraction usually require a fixed set of pre-defined relations. Such requirement is hard to meet in many real applications, especially when new data and relations are emerging incessantly and it is computationally expensive to store all data and re-train the whole model every time new data and relations come in. We formulate such challenging problem as lifelong relation extraction and investigate memory-efficient incremental learning methods without catastrophically forgetting knowledge learned from previous tasks. We first investigate a modified version of the stochastic gradient methods with a replay memory, which surprisingly outperforms recent state-of-the-art lifelong learning methods. We further propose to improve this approach to alleviate the forgetting problem by anchoring the sentence embedding space. Specifically, we utilize an explicit alignment model to mitigate the sentence embedding distortion of learned model when training on new data and new relations. Experiment results on multiple benchmarks show that our proposed method significantly outperforms the state-of-the-art lifelong learning approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Description-Based Zero-shot Fine-Grained Entity Typing",
        "paper_url": "https://aclanthology.org/N19-1087/",
        "paper_authors": [
            "Rasha Obeidat",
            "Xiaoli Fern",
            "Hamed Shahbazi",
            "Prasad Tadepalli"
        ],
        "paper_abstract": "Fine-grained Entity typing (FGET) is the task of assigning a fine-grained type from a hierarchy to entity mentions in the text. As the taxonomy of types evolves continuously, it is desirable for an entity typing system to be able to recognize novel types without additional training. This work proposes a zero-shot entity typing approach that utilizes the type description available from Wikipedia to build a distributed semantic representation of the types. During training, our system learns to align the entity mentions and their corresponding type representations on the known types. At test time, any new type can be incorporated into the system given its Wikipedia descriptions. We evaluate our approach on FIGER, a public benchmark entity tying dataset. Because the existing test set of FIGER covers only a small portion of the fine-grained types, we create a new test set by manually annotating a portion of the noisy training data. Our experiments demonstrate the effectiveness of the proposed method in recognizing novel types that are not present in the training data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adversarial Decomposition of Text Representation",
        "paper_url": "https://aclanthology.org/N19-1088/",
        "paper_authors": [
            "Alexey Romanov",
            "Anna Rumshisky",
            "Anna Rogers",
            "David Donahue"
        ],
        "paper_abstract": "In this paper, we present a method for adversarial decomposition of text representation. This method can be used to decompose a representation of an input sentence into several independent vectors, each of them responsible for a specific aspect of the input sentence. We evaluate the proposed method on two case studies: the conversion between different social registers and diachronic language change. We show that the proposed method is capable of fine-grained controlled change of these aspects of the input sentence. It is also learning a continuous (rather than categorical) representation of the style of the sentence, which is more linguistically realistic. The model uses adversarial-motivational training and includes a special motivational loss, which acts opposite to the discriminator and encourages a better decomposition. Furthermore, we evaluate the obtained meaning embeddings on a downstream task of paraphrase detection and show that they significantly outperform the embeddings of a regular autoencoder.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PoMo: Generating Entity-Specific Post-Modifiers in Context",
        "paper_url": "https://aclanthology.org/N19-1089/",
        "paper_authors": [
            "Jun Seok Kang",
            "Robert Logan",
            "Zewei Chu",
            "Yang Chen",
            "Dheeru Dua",
            "Kevin Gimpel",
            "Sameer Singh",
            "Niranjan Balasubramanian"
        ],
        "paper_abstract": "We introduce entity post-modifier generation as an instance of a collaborative writing task. Given a sentence about a target entity, the task is to automatically generate a post-modifier phrase that provides contextually relevant information about the entity. For example, for the sentence, \u201cBarack Obama, _______, supported the #MeToo movement.\u201d, the phrase \u201ca father of two girls\u201d is a contextually relevant post-modifier. To this end, we build PoMo, a post-modifier dataset created automatically from news articles reflecting a journalistic need for incorporating entity information that is relevant to a particular news event. PoMo consists of more than 231K sentences with post-modifiers and associated facts extracted from Wikidata for around 57K unique entities. We use crowdsourcing to show that modeling contextual relevance is necessary for accurate post-modifier generation. We adapt a number of existing generation approaches as baselines for this dataset. Our results show there is large room for improvement in terms of both identifying relevant facts to include (knowing which claims are relevant gives a >20% improvement in BLEU score), and generating appropriate post-modifier text for the context (providing relevant claims is not sufficient for accurate generation). We conduct an error analysis that suggests promising directions for future research.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting",
        "paper_url": "https://aclanthology.org/N19-1090/",
        "paper_authors": [
            "J. Edward Hu",
            "Huda Khayrallah",
            "Ryan Culkin",
            "Patrick Xia",
            "Tongfei Chen",
            "Matt Post",
            "Benjamin Van Durme"
        ],
        "paper_abstract": "Lexically-constrained sequence decoding allows for explicit positive or negative phrase-based constraints to be placed on target output strings in generation tasks such as machine translation or monolingual text rewriting. We describe vectorized dynamic beam allocation, which extends work in lexically-constrained decoding to work with batching, leading to a five-fold improvement in throughput when working with positive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Courteously Yours: Inducing courteous behavior in Customer Care responses using Reinforced Pointer Generator Network",
        "paper_url": "https://aclanthology.org/N19-1091/",
        "paper_authors": [
            "Hitesh Golchha",
            "Mauajama Firdaus",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "In this paper, we propose an effective deep learning framework for inducing courteous behavior in customer care responses. The interaction between a customer and the customer care representative contributes substantially to the overall customer experience. Thus it is imperative for customer care agents and chatbots engaging with humans to be personal, cordial and emphatic to ensure customer satisfaction and retention. Our system aims at automatically transforming neutral customer care responses into courteous replies. Along with stylistic transfer (of courtesy), our system ensures that responses are coherent with the conversation history, and generates courteous expressions consistent with the emotional state of the customer. Our technique is based on a reinforced pointer-generator model for the sequence to sequence task. The model is also conditioned on a hierarchically encoded and emotionally aware conversational context. We use real interactions on Twitter between customer care professionals and aggrieved customers to create a large conversational dataset having both forms of agent responses: \u2018generic\u2019 and \u2018courteous\u2019. We perform quantitative and qualitative analyses on established and task-specific metrics, both automatic and human evaluation based. Our evaluation shows that the proposed models can generate emotionally-appropriate courteous expressions while preserving the content. Experimental results also prove that our proposed approach performs better than the baseline models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "How to Avoid Sentences Spelling Boring? Towards a Neural Approach to Unsupervised Metaphor Generation",
        "paper_url": "https://aclanthology.org/N19-1092/",
        "paper_authors": [
            "Zhiwei Yu",
            "Xiaojun Wan"
        ],
        "paper_abstract": "Metaphor generation attempts to replicate human creativity with language, which is an attractive but challengeable text generation task. Previous efforts mainly focus on template-based or rule-based methods and result in a lack of linguistic subtlety. In order to create novel metaphors, we propose a neural approach to metaphor generation and explore the shared inferential structure of a metaphorical usage and a literal usage of a verb. Our approach does not require any manually annotated metaphors for training. We extract the metaphorically used verbs with their metaphorical senses in an unsupervised way and train a neural language model from wiki corpus. Then we generate metaphors conveying the assigned metaphorical senses with an improved decoding algorithm. Automatic metrics and human evaluations demonstrate that our approach can generate metaphors with good readability and creativity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Incorporating Context and External Knowledge for Pronoun Coreference Resolution",
        "paper_url": "https://aclanthology.org/N19-1093/",
        "paper_authors": [
            "Hongming Zhang",
            "Yan Song",
            "Yangqiu Song"
        ],
        "paper_abstract": "Linking pronominal expressions to the correct references requires, in many cases, better analysis of the contextual information and external knowledge. In this paper, we propose a two-layer model for pronoun coreference resolution that leverages both context and external knowledge, where a knowledge attention mechanism is designed to ensure the model leveraging the appropriate source of external knowledge based on different context. Experimental results demonstrate the validity and effectiveness of our model, where it outperforms state-of-the-art models by a large margin.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Deep Structured Semantic Models for Commonsense Reasoning",
        "paper_url": "https://aclanthology.org/N19-1094/",
        "paper_authors": [
            "Shuohang Wang",
            "Sheng Zhang",
            "Yelong Shen",
            "Xiaodong Liu",
            "Jingjing Liu",
            "Jianfeng Gao",
            "Jing Jiang"
        ],
        "paper_abstract": "Commonsense reasoning is fundamental to natural language understanding. While traditional methods rely heavily on human-crafted features and knowledge bases, we explore learning commonsense knowledge from a large amount of raw text via unsupervised learning. We propose two neural network models based on the Deep Structured Semantic Models (DSSM) framework to tackle two classic commonsense reasoning tasks, Winograd Schema challenges (WSC) and Pronoun Disambiguation (PDP). Evaluation shows that the proposed models effectively capture contextual information in the sentence and co-reference information between pronouns and nouns, and achieve significant improvement over previous state-of-the-art approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Recovering dropped pronouns in Chinese conversations via modeling their referents",
        "paper_url": "https://aclanthology.org/N19-1095/",
        "paper_authors": [
            "Jingxuan Yang",
            "Jianzhuo Tong",
            "Si Li",
            "Sheng Gao",
            "Jun Guo",
            "Nianwen Xue"
        ],
        "paper_abstract": "Pronouns are often dropped in Chinese sentences, and this happens more frequently in conversational genres as their referents can be easily understood from context. Recovering dropped pronouns is essential to applications such as Information Extraction where the referents of these dropped pronouns need to be resolved, or Machine Translation when Chinese is the source language. In this work, we present a novel end-to-end neural network model to recover dropped pronouns in conversational data. Our model is based on a structured attention mechanism that models the referents of dropped pronouns utilizing both sentence-level and word-level information. Results on three different conversational genres show that our approach achieves a significant improvement over the current state of the art.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The problem with probabilistic DAG automata for semantic graphs",
        "paper_url": "https://aclanthology.org/N19-1096/",
        "paper_authors": [
            "Ieva Vasiljeva",
            "Sorcha Gilroy",
            "Adam Lopez"
        ],
        "paper_abstract": "Semantic representations in the form of directed acyclic graphs (DAGs) have been introduced in recent years, and to model them, we need probabilistic models of DAGs. One model that has attracted some attention is the DAG automaton, but it has not been studied as a probabilistic model. We show that some DAG automata cannot be made into useful probabilistic models by the nearly universal strategy of assigning weights to transitions. The problem affects single-rooted, multi-rooted, and unbounded-degree variants of DAG automata, and appears to be pervasive. It does not affect planar variants, but these are problematic for other reasons.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Systematic Study of Leveraging Subword Information for Learning Word Representations",
        "paper_url": "https://aclanthology.org/N19-1097/",
        "paper_authors": [
            "Yi Zhu",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ],
        "paper_abstract": "The use of subword-level information (e.g., characters, character n-grams, morphemes) has become ubiquitous in modern word representation learning. Its importance is attested especially for morphologically rich languages which generate a large number of rare words. Despite a steadily increasing interest in such subword-informed word representations, their systematic comparative analysis across typologically diverse languages and different tasks is still missing. In this work, we deliver such a study focusing on the variation of two crucial components required for subword-level integration into word representation models: 1) segmentation of words into subword units, and 2) subword composition functions to obtain final word representations. We propose a general framework for learning subword-informed word representations that allows for easy experimentation with different segmentation and composition components, also including more advanced techniques based on position embeddings and self-attention. Using the unified framework, we run experiments over a large number of subword-informed word representation configurations (60 in total) on 3 tasks (general and rare word similarity, dependency parsing, fine-grained entity typing) for 5 languages representing 3 language types. Our main results clearly indicate that there is no \u201cone-size-fits-all\u201d configuration, as performance is both language- and task-dependent. We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Better Word Embeddings by Disentangling Contextual n-Gram Information",
        "paper_url": "https://aclanthology.org/N19-1098/",
        "paper_authors": [
            "Prakhar Gupta",
            "Matteo Pagliardini",
            "Martin Jaggi"
        ],
        "paper_abstract": "Pre-trained word vectors are ubiquitous in Natural Language Processing applications. In this paper, we show how training word embeddings jointly with bigram and even trigram embeddings, results in improved unigram embeddings. We claim that training word embeddings along with higher n-gram embeddings helps in the removal of the contextual information from the unigrams, resulting in better stand-alone word embeddings. We empirically show the validity of our hypothesis by outperforming other competing word representation models by a significant margin on a wide variety of tasks. We make our models publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process",
        "paper_url": "https://aclanthology.org/N19-1099/",
        "paper_authors": [
            "Dingcheng Li",
            "Siamak Zamani",
            "Jingyuan Zhang",
            "Ping Li"
        ],
        "paper_abstract": "Leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Correlation Coefficients and Semantic Textual Similarity",
        "paper_url": "https://aclanthology.org/N19-1100/",
        "paper_authors": [
            "Vitalii Zhelezniak",
            "Aleksandar Savkov",
            "April Shen",
            "Nils Hammerla"
        ],
        "paper_abstract": "A large body of research into semantic textual similarity has focused on constructing state-of-the-art embeddings using sophisticated modelling, careful choice of learning signals and many clever tricks. By contrast, little attention has been devoted to similarity measures between these embeddings, with cosine similarity being used unquestionably in the majority of cases. In this work, we illustrate that for all common word vectors, cosine similarity is essentially equivalent to the Pearson correlation coefficient, which provides some justification for its use. We thoroughly characterise cases where Pearson correlation (and thus cosine similarity) is unfit as similarity measure. Importantly, we show that Pearson correlation is appropriate for some word vectors but not others. When it is not appropriate, we illustrate how common non-parametric rank correlation coefficients can be used instead to significantly improve performance. We support our analysis with a series of evaluations on word-level and sentence-level semantic textual similarity benchmarks. On the latter, we show that even the simplest averaged word vectors compared by rank correlation easily rival the strongest deep representations compared by cosine similarity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generating Token-Level Explanations for Natural Language Inference",
        "paper_url": "https://aclanthology.org/N19-1101/",
        "paper_authors": [
            "James Thorne",
            "Andreas Vlachos",
            "Christos Christodoulopoulos",
            "Arpit Mittal"
        ],
        "paper_abstract": "The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of classifiers on a single piece of text, there have been no attempts to generate explanations of classifiers operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the entailment relation. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same accuracy as the black-box methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Strong Baselines for Complex Word Identification across Multiple Languages",
        "paper_url": "https://aclanthology.org/N19-1102/",
        "paper_authors": [
            "Pierre Finnimore",
            "Elisabeth Fritzsch",
            "Daniel King",
            "Alison Sneyd",
            "Aneeq Ur Rehman",
            "Fernando Alva-Manchego",
            "Andreas Vlachos"
        ],
        "paper_abstract": "Complex Word Identification (CWI) is the task of identifying which words or phrases in a sentence are difficult to understand by a target audience. The latest CWI Shared Task released data for two settings: monolingual (i.e. train and test in the same language) and cross-lingual (i.e. test in a language not seen during training). The best monolingual models relied on language-dependent features, which do not generalise in the cross-lingual setting, while the best cross-lingual model used neural networks with multi-task learning. In this paper, we present monolingual and cross-lingual CWI models that perform as well as (or better than) most models submitted to the latest CWI Shared Task. We show that carefully selected features and simple learning models can achieve state-of-the-art performance, and result in strong baselines for future development in this area. Finally, we discuss how inconsistencies in the annotation of the data can explain some of the results obtained.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptive Convolution for Multi-Relational Learning",
        "paper_url": "https://aclanthology.org/N19-1103/",
        "paper_authors": [
            "Xiaotian Jiang",
            "Quan Wang",
            "Bin Wang"
        ],
        "paper_abstract": "We consider the problem of learning distributed representations for entities and relations of multi-relational data so as to predict missing links therein. Convolutional neural networks have recently shown their superiority for this problem, bringing increased model expressiveness while remaining parameter efficient. Despite the success, previous convolution designs fail to model full interactions between input entities and relations, which potentially limits the performance of link prediction. In this work we introduce ConvR, an adaptive convolutional network designed to maximize entity-relation interactions in a convolutional fashion. ConvR adaptively constructs convolution filters from relation representations, and applies these filters across entity representations to generate convolutional features. As such, ConvR enables rich interactions between entity and relation representations at diverse regions, and all the convolutional features generated will be able to capture such interactions. We evaluate ConvR on multiple benchmark datasets. Experimental results show that: (1) ConvR performs substantially better than competitive baselines in almost all the metrics and on all the datasets; (2) Compared with state-of-the-art convolutional models, ConvR is not only more effective but also more efficient. It offers a 7% increase in MRR and a 6% increase in Hits@10, while saving 12% in parameter storage.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Graph Pattern Entity Ranking Model for Knowledge Graph Completion",
        "paper_url": "https://aclanthology.org/N19-1104/",
        "paper_authors": [
            "Takuma Ebisu",
            "Ryutaro Ichise"
        ],
        "paper_abstract": "Knowledge graphs have been developed rapidly in recent years and shown their usefulness for many artificial intelligence tasks. However, knowledge graphs often have lots of missing facts. To solve this problem, many knowledge graph embedding models to populate knowledge graphs have been developed and have shown outstanding performance these days. However, knowledge graph embedding models are so called-black box. Hence, we actually does not know how information of a knowledge graph is processed and the models are hard to interpret. In this paper, we utilize graph patterns in a knowledge graph to overcome such problems. Our proposed model, graph pattern entity ranking Model (GRank), constructs an entity ranking system for each graph pattern and evaluate them using a measure for a ranking system. By doing so, we can find helpful graph patterns for predicting facts. Then we conduct the link prediction tasks on standard data sets to evaluate GRank. We show our approach outperforms other state-of-the-art approaches such as ComplEx and TorusE on standard metrics such as HITS@n and MRR. Moreover, This model is easily interpretable because output facts are described by graph patterns.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adversarial Training for Weakly Supervised Event Detection",
        "paper_url": "https://aclanthology.org/N19-1105/",
        "paper_authors": [
            "Xiaozhi Wang",
            "Xu Han",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Peng Li"
        ],
        "paper_abstract": "Modern weakly supervised methods for event detection (ED) avoid time-consuming human annotation and achieve promising results by learning from auto-labeled data. However, these methods typically rely on sophisticated pre-defined rules as well as existing instances in knowledge bases for automatic annotation and thus suffer from low coverage, topic bias, and data noise. To address these issues, we build a large event-related candidate set with good coverage and then apply an adversarial training mechanism to iteratively identify those informative instances from the candidate set and filter out those noisy ones. The experiments on two real-world datasets show that our candidate selection and adversarial training can cooperate together to obtain more diverse and accurate training data for ED, and significantly outperform the state-of-the-art methods in various weakly supervised scenarios. The datasets and source code can be obtained from https://github.com/thunlp/Adv-ED.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Submodular Feature-Aware Framework for Label Subset Selection in Extreme Classification Problems",
        "paper_url": "https://aclanthology.org/N19-1106/",
        "paper_authors": [
            "Elham J. Barezi",
            "Ian D. Wood",
            "Pascale Fung",
            "Hamid R. Rabiee"
        ],
        "paper_abstract": "Extreme classification is a classification task on an extremely large number of labels (tags). User generated labels for any type of online data can be sparing per individual user but intractably large among all users. It would be useful to automatically select a smaller, standard set of labels to represent the whole label set. We can then solve efficiently the problem of multi-label learning with an intractably large number of interdependent labels, such as automatic tagging of Wikipedia pages. We propose a submodular maximization framework with linear cost to find informative labels which are most relevant to other labels yet least redundant with each other. A simple prediction model can then be trained on this label subset. Our framework includes both label-label and label-feature dependencies, which aims to find the labels with the most representation and prediction ability. In addition, to avoid information loss, we extract and predict outlier labels with weak dependency on other labels. We apply our model to four standard natural language data sets including Bibsonomy entries with users assigned tags, web pages with user assigned tags, legal texts with EUROVOC descriptors(A topic hierarchy with almost 4000 categories regarding different aspects of European law) and Wikipedia pages with tags from social bookmarking as well as news videos for automated label detection from a lexicon of semantic concepts. Experimental results show that our proposed approach improves label prediction quality, in terms of precision and nDCG, by 3% to 5% in three of the 5 tasks and is competitive in the others, even with a simple linear prediction model. An ablation study shows how different data sets benefit from different aspects of our model, with all aspects contributing substantially to at least one data set.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Relation Extraction with Temporal Reasoning Based on Memory Augmented Distant Supervision",
        "paper_url": "https://aclanthology.org/N19-1107/",
        "paper_authors": [
            "Jianhao Yan",
            "Lin He",
            "Ruqin Huang",
            "Jian Li",
            "Ying Liu"
        ],
        "paper_abstract": "Distant supervision (DS) is an important paradigm for automatically extracting relations. It utilizes existing knowledge base to collect examples for the relation we intend to extract, and then uses these examples to automatically generate the training data. However, the examples collected can be very noisy, and pose significant challenge for obtaining high quality labels. Previous work has made remarkable progress in predicting the relation from distant supervision, but typically ignores the temporal relations among those supervising instances. This paper formulates the problem of relation extraction with temporal reasoning and proposes a solution to predict whether two given entities participate in a relation at a given time spot. For this purpose, we construct a dataset called WIKI-TIME which additionally includes the valid period of a certain relation of two entities in the knowledge base. We propose a novel neural model to incorporate both the temporal information encoding and sequential reasoning. The experimental results show that, compared with the best of existing models, our model achieves better performance in both WIKI-TIME dataset and the well-studied NYT-10 dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Integrating Semantic Knowledge to Tackle Zero-shot Text Classification",
        "paper_url": "https://aclanthology.org/N19-1108/",
        "paper_authors": [
            "Jingqing Zhang",
            "Piyawat Lertvittayakumjorn",
            "Yike Guo"
        ],
        "paper_abstract": "Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including text classification. Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is therefore difficult and only limited previous works tackled this problem. In this paper, we propose a two-phase framework together with data augmentation and feature augmentation to solve this problem. Four kinds of semantic knowledge (word embeddings, class descriptions, class hierarchy, and a general knowledge graph) are incorporated into the proposed framework to deal with instances of unseen classes effectively. Experimental results show that each and the combination of the two phases achieve the best overall accuracy compared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Word-Node2Vec: Improving Word Embedding with Document-Level Non-Local Word Co-occurrences",
        "paper_url": "https://aclanthology.org/N19-1109/",
        "paper_authors": [
            "Procheta Sen",
            "Debasis Ganguly",
            "Gareth Jones"
        ],
        "paper_abstract": "A standard word embedding algorithm, such as word2vec and glove, makes a strong assumption that words are likely to be semantically related only if they co-occur locally within a window of fixed size. However, this strong assumption may not capture the semantic association between words that co-occur frequently but non-locally within documents. In this paper, we propose a graph-based word embedding method, named \u2018word-node2vec\u2019. By relaxing the strong constraint of locality, our method is able to capture both the local and non-local co-occurrences. Word-node2vec constructs a graph where every node represents a word and an edge between two nodes represents a combination of both local (e.g. word2vec) and document-level co-occurrences. Our experiments show that word-node2vec outperforms word2vec and glove on a range of different tasks, such as predicting word-pair similarity, word analogy and concept categorization.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-Topic Distributional Semantic Representations Via Unsupervised Mappings",
        "paper_url": "https://aclanthology.org/N19-1110/",
        "paper_authors": [
            "Eleftheria Briakou",
            "Nikos Athanasiou",
            "Alexandros Potamianos"
        ],
        "paper_abstract": "In traditional Distributional Semantic Models (DSMs) the multiple senses of a polysemous word are conflated into a single vector space representation. In this work, we propose a DSM that learns multiple distributional representations of a word based on different topics. First, a separate DSM is trained for each topic and then each of the topic-based DSMs is aligned to a common vector space. Our unsupervised mapping approach is motivated by the hypothesis that words preserving their relative distances in different topic semantic sub-spaces constitute robust semantic anchors that define the mappings between them. Aligned cross-topic representations achieve state-of-the-art results for the task of contextual word similarity. Furthermore, evaluation on NLP downstream tasks shows that multiple topic-based embeddings outperform single-prototype models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "What just happened? Evaluating retrofitted distributional word vectors",
        "paper_url": "https://aclanthology.org/N19-1111/",
        "paper_authors": [
            "Dmetri Hayes"
        ],
        "paper_abstract": "Recent work has attempted to enhance vector space representations using information from structured semantic resources. This process, dubbed retrofitting (Faruqui et al., 2015), has yielded improvements in word similarity performance. Research has largely focused on the retrofitting algorithm, or on the kind of structured semantic resources used, but little research has explored why some resources perform better than others. We conducted a fine-grained analysis of the original retrofitting process, and found that the utility of different lexical resources for retrofitting depends on two factors: the coverage of the resource and the evaluation metric. Our assessment suggests that the common practice of using correlation measures to evaluate increases in performance against full word similarity benchmarks 1) obscures the benefits offered by smaller resources, and 2) overlooks incremental gains in word similarity performance. We propose root-mean-square error (RMSE) as an alternative evaluation metric, and demonstrate that correlation measures and RMSE sometimes yield opposite conclusions concerning the efficacy of retrofitting. This point is illustrated by word vectors retrofitted with novel treatments of the FrameNet data (Fillmore and Baker, 2010).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Linguistic Knowledge and Transferability of Contextual Representations",
        "paper_url": "https://aclanthology.org/N19-1112/",
        "paper_authors": [
            "Nelson F. Liu",
            "Matt Gardner",
            "Yonatan Belinkov",
            "Matthew E. Peters",
            "Noah A. Smith"
        ],
        "paper_abstract": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction",
        "paper_url": "https://aclanthology.org/N19-1113/",
        "paper_authors": [
            "Karl Stratos"
        ],
        "paper_abstract": "We address part-of-speech (POS) induction by maximizing the mutual information between the induced label and its context. We focus on two training objectives that are amenable to stochastic gradient descent (SGD): a novel generalization of the classical Brown clustering objective and a recently proposed variational lower bound. While both objectives are subject to noise in gradient updates, we show through analysis and experiments that the variational lower bound is robust whereas the generalized Brown objective is vulnerable. We obtain strong performance on a multitude of datasets and languages with a simple architecture that encodes morphology and context.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Recurrent Neural Network Grammars",
        "paper_url": "https://aclanthology.org/N19-1114/",
        "paper_authors": [
            "Yoon Kim",
            "Alexander Rush",
            "Lei Yu",
            "Adhiguna Kuncoro",
            "Chris Dyer",
            "G\u00e1bor Melis"
        ],
        "paper_abstract": "Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cooperative Learning of Disjoint Syntax and Semantics",
        "paper_url": "https://aclanthology.org/N19-1115/",
        "paper_authors": [
            "Serhii Havrylov",
            "Germ\u00e1n Kruszewski",
            "Armand Joulin"
        ],
        "paper_abstract": "There has been considerable attention devoted to models that learn to jointly infer an expression\u2019s syntactic structure and its semantics. Yet, Nangia and Bowman (2018) has recently shown that the current best systems fail to learn the correct parsing strategy on mathematical expressions generated from a simple context-free grammar. In this work, we present a recursive model inspired by Choi et al. (2018) that reaches near perfect accuracy on this task. Our model is composed of two separated modules for syntax and semantics. They are cooperatively trained with standard continuous and discrete optimisation schemes. Our model does not require any linguistic structure for supervision, and its recursive nature allows for out-of-domain generalisation. Additionally, our approach performs competitively on several natural language tasks, such as Natural Language Inference and Sentiment Analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders",
        "paper_url": "https://aclanthology.org/N19-1116/",
        "paper_authors": [
            "Andrew Drozdov",
            "Patrick Verga",
            "Mohit Yadav",
            "Mohit Iyyer",
            "Andrew McCallum"
        ],
        "paper_abstract": "We introduce the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence. During training we use dynamic programming to consider all possible binary trees over the sentence, and for inference we use the CKY algorithm to extract the highest scoring parse. DIORA outperforms previously reported results for unsupervised binary constituency parsing on the benchmark WSJ dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Knowledge-Augmented Language Model and Its Application to Unsupervised Named-Entity Recognition",
        "paper_url": "https://aclanthology.org/N19-1117/",
        "paper_authors": [
            "Angli Liu",
            "Jingfei Du",
            "Veselin Stoyanov"
        ],
        "paper_abstract": "Traditional language models are unable to efficiently model entity names observed in text. All but the most popular named entities appear infrequently in text providing insufficient context. Recent efforts have recognized that context can be generalized between entity names that share the same type (e.g., person or location) and have equipped language models with an access to external knowledge base (KB). Our Knowledge-Augmented Language Model (KALM) continues this line of work by augmenting a traditional model with a KB. Unlike previous methods, however, we train with an end-to-end predictive objective optimizing the perplexity of text. We do not require any additional information such as named entity tags. In addition to improving language modeling performance, KALM learns to recognize named entities in an entirely unsupervised way by using entity type information latent in the model. On a Named Entity Recognition (NER) task, KALM achieves performance comparable with state-of-the-art supervised models. Our work demonstrates that named entities (and possibly other types of world knowledge) can be modeled successfully using predictive learning and training on large corpora of text without any additional information.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations",
        "paper_url": "https://aclanthology.org/N19-1118/",
        "paper_authors": [
            "Meishan Zhang",
            "Zhenghua Li",
            "Guohong Fu",
            "Min Zhang"
        ],
        "paper_abstract": "Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate syntax by representing 1-best tree outputs from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from error propagation. In this work, we propose a novel method to integrate source-side syntax implicitly for NMT. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary word embeddings to enhance basic NMT models. The method can be straightforwardly integrated into the widely-used sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two datasets compared with the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese translation, respectively. In addition, the approach also outperforms the explicit Tree-RNN and Tree-Linearization methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Competence-based Curriculum Learning for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1119/",
        "paper_authors": [
            "Emmanouil Antonios Platanios",
            "Otilia Stretcu",
            "Graham Neubig",
            "Barnabas Poczos",
            "Tom Mitchell"
        ],
        "paper_abstract": "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1120/",
        "paper_authors": [
            "Jiawei Wu",
            "Xin Wang",
            "William Yang Wang"
        ],
        "paper_abstract": "The overreliance on large parallel corpora significantly limits the applicability of machine translation systems to the majority of language pairs. Back-translation has been dominantly used in previous approaches for unsupervised neural machine translation, where pseudo sentence pairs are generated to train the models with a reconstruction loss. However, the pseudo sentences are usually of low quality as translation errors accumulate during training. To avoid this fundamental issue, we propose an alternative but more effective approach, extract-edit, to extract and then edit real sentences from the target monolingual corpora. Furthermore, we introduce a comparative translation loss to evaluate the translated target sentences and thus train the unsupervised translation systems. Experiments show that the proposed approach consistently outperforms the previous state-of-the-art unsupervised machine translation systems across two benchmarks (English-French and English-German) and two low-resource language pairs (English-Romanian and English-Russian) by more than 2 (up to 3.63) BLEU points.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Consistency by Agreement in Zero-Shot Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1121/",
        "paper_authors": [
            "Maruan Al-Shedivat",
            "Ankur Parikh"
        ],
        "paper_abstract": "Generalization and reliability of multilingual translation often highly depend on the amount of available parallel data for each language pair of interest. In this paper, we focus on zero-shot generalization\u2014a challenging setup that tests models on translation directions they have not been optimized for at training time. To solve the problem, we (i) reformulate multilingual translation as probabilistic inference, (ii) define the notion of zero-shot consistency and show why standard training often results in models unsuitable for zero-shot tasks, and (iii) introduce a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages. We test our multilingual NMT models on multiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl) and show that agreement-based learning often results in 2-3 BLEU zero-shot improvement over strong baselines without any loss in performance on supervised translation directions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modeling Recurrence for Transformer",
        "paper_url": "https://aclanthology.org/N19-1122/",
        "paper_authors": [
            "Jie Hao",
            "Xing Wang",
            "Baosong Yang",
            "Longyue Wang",
            "Jinfeng Zhang",
            "Zhaopeng Tu"
        ],
        "paper_abstract": "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence modeling hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. Experimental results on the widely-used WMT14 English\u21d2German and WMT17 Chinese\u21d2English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models",
        "paper_url": "https://aclanthology.org/N19-1123/",
        "paper_authors": [
            "Tiancheng Zhao",
            "Kaige Xie",
            "Maxine Eskenazi"
        ],
        "paper_abstract": "Defining action spaces for conversational agents and optimizing their decision-making process with reinforcement learning is an enduring challenge. Common practice has been to use handcrafted dialog acts, or the output vocabulary, e.g. in neural encoder decoders, as the action spaces. Both have their own limitations. This paper proposes a novel latent action framework that treats the action spaces of an end-to-end dialog agent as latent variables and develops unsupervised methods in order to induce its own action space from the data. Comprehensive experiments are conducted examining both continuous and discrete action types and two different optimization methods based on stochastic variational inference. Results show that the proposed latent actions achieve superior empirical performance improvement over previous word-level policy gradient methods on both DealOrNoDeal and MultiWoz dialogs. Our detailed analysis also provides insights about various latent variable approaches for policy learning and can serve as a foundation for developing better latent actions in future research.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory",
        "paper_url": "https://aclanthology.org/N19-1124/",
        "paper_authors": [
            "Deng Cai",
            "Yan Wang",
            "Wei Bi",
            "Zhaopeng Tu",
            "Xiaojiang Liu",
            "Wai Lam",
            "Shuming Shi"
        ],
        "paper_abstract": "Traditional generative dialogue models generate responses solely from input queries. Such information is insufficient for generating a specific response since a certain query could be answered in multiple ways. Recently, researchers have attempted to fill the information gap by exploiting information retrieval techniques. For a given query, similar dialogues are retrieved from the entire training data and considered as an additional knowledge source. While the use of retrieval may harvest extensive information, the generative models could be overwhelmed, leading to unsatisfactory performance. In this paper, we propose a new framework which exploits retrieval results via a skeleton-to-response paradigm. At first, a skeleton is extracted from the retrieved dialogues. Then, both the generated skeleton and the original query are used for response generation via a novel response generator. Experimental results show that our approach significantly improves the informativeness of the generated responses",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Jointly Optimizing Diversity and Relevance in Neural Response Generation",
        "paper_url": "https://aclanthology.org/N19-1125/",
        "paper_authors": [
            "Xiang Gao",
            "Sungjin Lee",
            "Yizhe Zhang",
            "Chris Brockett",
            "Michel Galley",
            "Jianfeng Gao",
            "Bill Dolan"
        ],
        "paper_abstract": "Although recent neural conversation models have shown great potential, they often generate bland and generic responses. While various approaches have been explored to diversify the output of the conversation model, the improvement often comes at the cost of decreased relevance. In this paper, we propose a SpaceFusion model to jointly optimize diversity and relevance that essentially fuses the latent space of a sequence-to-sequence model and that of an autoencoder model by leveraging novel regularization terms. As a result, our approach induces a latent space in which the distance and direction from the predicted response vector roughly match the relevance and diversity, respectively. This property also lends itself well to an intuitive visualization of the latent space. Both automatic and human evaluation results demonstrate that the proposed approach brings significant improvement compared to strong baselines in both diversity and relevance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Disentangling Language and Knowledge in Task-Oriented Dialogs",
        "paper_url": "https://aclanthology.org/N19-1126/",
        "paper_authors": [
            "Dinesh Raghu",
            "Nikhil Gupta",
            "Mausam"
        ],
        "paper_abstract": "The Knowledge Base (KB) used for real-world applications, such as booking a movie or restaurant reservation, keeps changing over time. End-to-end neural networks trained for these task-oriented dialogs are expected to be immune to any changes in the KB. However, existing approaches breakdown when asked to handle such changes. We propose an encoder-decoder architecture (BoSsNet) with a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled learning of the response\u2019s language model and its knowledge incorporation. Consequently, the KB can be modified with new knowledge without a drop in interpretability. We find that BoSsNeT outperforms state-of-the-art models, with considerable improvements (>10%) on bAbI OOV test sets and other human-human datasets. We also systematically modify existing datasets to measure disentanglement and show BoSsNeT to be robust to KB modifications.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together",
        "paper_url": "https://aclanthology.org/N19-1127/",
        "paper_authors": [
            "Tao Shen",
            "Tianyi Zhou",
            "Guodong Long",
            "Jing Jiang",
            "Chengqi Zhang"
        ],
        "paper_abstract": "Neural networks equipped with self-attention have parallelizable computation, light-weight structure, and the ability to capture both long-range and local dependencies. Further, their expressive power and performance can be boosted by using a vector to measure pairwise dependency, but this requires to expand the alignment matrix to a tensor, which results in memory and computation bottlenecks. In this paper, we propose a novel attention mechanism called \u201cMulti-mask Tensorized Self-Attention\u201d (MTSA), which is as fast and as memory-efficient as a CNN, but significantly outperforms previous CNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token) and global (source2token) dependencies by a novel compatibility function composed of dot-product and additive attentions, 2) uses a tensor to represent the feature-wise alignment scores for better expressive power but only requires parallelizable matrix multiplications, and 3) combines multi-head with multi-dimensional attentions, and applies a distinct positional mask to each head (subspace), so the memory and computation can be distributed to multiple heads, each with sequential information encoded independently. The experiments show that a CNN/RNN-free model based on MTSA achieves state-of-the-art or competitive performance on nine NLP benchmarks with compelling memory- and time-efficiency.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations",
        "paper_url": "https://aclanthology.org/N19-1128/",
        "paper_authors": [
            "Mohammad Taher Pilehvar",
            "Jose Camacho-Collados"
        ],
        "paper_abstract": "By design, word embeddings are unable to model the dynamic nature of words\u2019 semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Does My Rebuttal Matter? Insights from a Major NLP Conference",
        "paper_url": "https://aclanthology.org/N19-1129/",
        "paper_authors": [
            "Yang Gao",
            "Steffen Eger",
            "Ilia Kuznetsov",
            "Iryna Gurevych",
            "Yusuke Miyao"
        ],
        "paper_abstract": "Peer review is a core element of the scientific process, particularly in conference-centered fields such as ML and NLP. However, only few studies have evaluated its properties empirically. Aiming to fill this gap, we present a corpus that contains over 4k reviews and 1.2k author responses from ACL-2018. We quantitatively and qualitatively assess the corpus. This includes a pilot study on paper weaknesses given by reviewers and on quality of author responses. We then focus on the role of the rebuttal phase, and propose a novel task to predict after-rebuttal (i.e., final) scores from initial reviews and author responses. Although author responses do have a marginal (and statistically significant) influence on the final scores, especially for borderline papers, our results suggest that a reviewer\u2019s final score is largely determined by her initial score and the distance to the other reviewers\u2019 initial scores. In this context, we discuss the conformity bias inherent to peer reviewing, a bias that has largely been overlooked in previous research. We hope our analyses will help better assess the usefulness of the rebuttal phase in NLP conferences.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism",
        "paper_url": "https://aclanthology.org/N19-1130/",
        "paper_authors": [
            "Shufan Wang",
            "Mohit Iyyer"
        ],
        "paper_abstract": "Literary critics often attempt to uncover meaning in a single work of literature through careful reading and analysis. Applying natural language processing methods to aid in such literary analyses remains a challenge in digital humanities. While most previous work focuses on \u201cdistant reading\u201d by algorithmically discovering high-level patterns from large collections of literary works, here we sharpen the focus of our methods to a single literary theory about Italo Calvino\u2019s postmodern novel Invisible Cities, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these cities into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate: we leverage pretrained contextualized representations to embed each city\u2019s description and use unsupervised methods to cluster these embeddings. Additionally, we compare results of our computational approach to similarity judgments generated by human readers. Our work is a first step towards incorporating natural language processing into literary criticism.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PAWS: Paraphrase Adversaries from Word Scrambling",
        "paper_url": "https://aclanthology.org/N19-1131/",
        "paper_authors": [
            "Yuan Zhang",
            "Jason Baldridge",
            "Luheng He"
        ],
        "paper_abstract": "Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (<40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models \u2014 Is Single-Corpus Evaluation Enough?",
        "paper_url": "https://aclanthology.org/N19-1132/",
        "paper_authors": [
            "Masato Mita",
            "Tomoya Mizumoto",
            "Masahiro Kaneko",
            "Ryo Nagata",
            "Kentaro Inui"
        ],
        "paper_abstract": "This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models. GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark. However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics. To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models\u2019 rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Star-Transformer",
        "paper_url": "https://aclanthology.org/N19-1133/",
        "paper_authors": [
            "Qipeng Guo",
            "Xipeng Qiu",
            "Pengfei Liu",
            "Yunfan Shao",
            "Xiangyang Xue",
            "Zheng Zhang"
        ],
        "paper_abstract": "Although Transformer has achieved great successes on many NLP tasks, its heavy structure with fully-connected attention connections leads to dependencies on large training data. In this paper, we present Star-Transformer, a lightweight alternative by careful sparsification. To reduce model complexity, we replace the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node. Thus, complexity is reduced from quadratic to linear, while preserving the capacity to capture both local composition and long-range dependency. The experiments on four tasks (22 datasets) show that Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptation of Hierarchical Structured Models for Speech Act Recognition in Asynchronous Conversation",
        "paper_url": "https://aclanthology.org/N19-1134/",
        "paper_authors": [
            "Tasnim Mohiuddin",
            "Thanh-Tung Nguyen",
            "Shafiq Joty"
        ],
        "paper_abstract": "We address the problem of speech act recognition (SAR) in asynchronous conversations (forums, emails). Unlike synchronous conversations (e.g., meetings, phone), asynchronous domains lack large labeled datasets to train an effective SAR model. In this paper, we propose methods to effectively leverage abundant unlabeled conversational data and the available labeled data from synchronous domains. We carry out our research in three main steps. First, we introduce a neural architecture based on hierarchical LSTMs and conditional random fields (CRF) for SAR, and show that our method outperforms existing methods when trained on in-domain data only. Second, we improve our initial SAR models by semi-supervised learning in the form of pretrained word embeddings learned from a large unlabeled conversational corpus. Finally, we employ adversarial training to improve the results further by leveraging the labeled data from synchronous domains and by explicitly modeling the distributional shift in two domains.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "From legal to technical concept: Towards an automated classification of German political Twitter postings as criminal offenses",
        "paper_url": "https://aclanthology.org/N19-1135/",
        "paper_authors": [
            "Frederike Zufall",
            "Tobias Horsmann",
            "Torsten Zesch"
        ],
        "paper_abstract": "Advances in the automated detection of offensive Internet postings make this mechanism very attractive to social media companies, who are increasingly under pressure to monitor and action activity on their sites. However, these advances also have important implications as a threat to the fundamental right of free expression. In this article, we analyze which Twitter posts could actually be deemed offenses under German criminal law. German law follows the deductive method of the Roman law tradition based on abstract rules as opposed to the inductive reasoning in Anglo-American common law systems. This allows us to show how legal conclusions can be reached and implemented without relying on existing court decisions. We present a data annotation schema, consisting of a series of binary decisions, for determining whether a specific post would constitute a criminal offense. This schema serves as a step towards an inexpensive creation of a sufficient amount of data for an automated classification. We find that the majority of posts deemed offensive actually do not constitute a criminal offense and still contribute to public discourse. Furthermore, laymen can provide sufficiently reliable data to an expert reference but are, for instance, more lenient in the interpretation of what constitutes a disparaging statement.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Joint Multi-Label Attention Networks for Social Text Annotation",
        "paper_url": "https://aclanthology.org/N19-1136/",
        "paper_authors": [
            "Hang Dong",
            "Wei Wang",
            "Kaizhu Huang",
            "Frans Coenen"
        ],
        "paper_abstract": "We propose a novel attention network for document annotation with user-generated tags. The network is designed according to the human reading and annotation behaviour. Usually, users try to digest the title and obtain a rough idea about the topic first, and then read the content of the document. Present research shows that the title metadata could largely affect the social annotation. To better utilise this information, we design a framework that separates the title from the content of a document and apply a title-guided attention mechanism over each sentence in the content. We also propose two semantic-based loss regularisers that enforce the output of the network to conform to label semantics, i.e. similarity and subsumption. We analyse each part of the proposed system with two real-world open datasets on publication and question annotation. The integrated approach, Joint Multi-label Attention Network (JMAN), significantly outperformed the Bidirectional Gated Recurrent Unit (Bi-GRU) by around 13%-26% and the Hierarchical Attention Network (HAN) by around 4%-12% on both datasets, with around 10%-30% reduction of training time.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Channel Convolutional Neural Network for Twitter Emotion and Sentiment Recognition",
        "paper_url": "https://aclanthology.org/N19-1137/",
        "paper_authors": [
            "Jumayel Islam",
            "Robert E. Mercer",
            "Lu Xiao"
        ],
        "paper_abstract": "The advent of micro-blogging sites has paved the way for researchers to collect and analyze huge volumes of data in recent years. Twitter, being one of the leading social networking sites worldwide, provides a great opportunity to its users for expressing their states of mind via short messages which are called tweets. The urgency of identifying emotions and sentiments conveyed through tweets has led to several research works. It provides a great way to understand human psychology and impose a challenge to researchers to analyze their content easily. In this paper, we propose a novel use of a multi-channel convolutional neural architecture which can effectively use different emotion and sentiment indicators such as hashtags, emoticons and emojis that are present in the tweets and improve the performance of emotion and sentiment identification. We also investigate the incorporation of different lexical features in the neural network model and its effect on the emotion and sentiment identification task. We analyze our model on some standard datasets and compare its effectiveness with existing techniques.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting Cybersecurity Events from Noisy Short Text",
        "paper_url": "https://aclanthology.org/N19-1138/",
        "paper_authors": [
            "Semih Yagcioglu",
            "Mehmet Saygin Seyfioglu",
            "Begum Citamak",
            "Batuhan Bardak",
            "Seren Guldamlasioglu",
            "Azmi Yuksel",
            "Emin Islam Tatli"
        ],
        "paper_abstract": "It is very critical to analyze messages shared over social networks for cyber threat intelligence and cyber-crime prevention. In this study, we propose a method that leverages both domain-specific word embeddings and task-specific features to detect cyber security events from tweets. Our model employs a convolutional neural network (CNN) and a long short-term memory (LSTM) recurrent neural network which takes word level meta-embeddings as inputs and incorporates contextual embeddings to classify noisy short text. We collected a new dataset of cyber security related tweets from Twitter and manually annotated a subset of 2K of them. We experimented with this dataset and concluded that the proposed model outperforms both traditional and neural baselines. The results suggest that our method works well for detecting cyber security events from noisy short text.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks",
        "paper_url": "https://aclanthology.org/N19-1139/",
        "paper_authors": [
            "Yotam Gil",
            "Yoav Chai",
            "Or Gorodissky",
            "Jonathan Berant"
        ],
        "paper_abstract": "Adversarial examples are important for understanding the behavior of neural models, and can improve their robustness through adversarial training. Recent work in natural language processing generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the optimization procedure can be distilled into another more efficient neural network. We train a model to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, it reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability. Our attack flips the API-predicted label in 42% of the generated examples, while humans maintain high-accuracy in predicting the gold label.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Analyzing the Perceived Severity of Cybersecurity Threats Reported on Social Media",
        "paper_url": "https://aclanthology.org/N19-1140/",
        "paper_authors": [
            "Shi Zong",
            "Alan Ritter",
            "Graham Mueller",
            "Evan Wright"
        ],
        "paper_abstract": "Breaking cybersecurity events are shared across a range of websites, including security blogs (FireEye, Kaspersky, etc.), in addition to social media platforms such as Facebook and Twitter. In this paper, we investigate methods to analyze the severity of cybersecurity threats based on the language that is used to describe them online. A corpus of 6,000 tweets describing software vulnerabilities is annotated with authors\u2019 opinions toward their severity. We show that our corpus supports the development of automatic classifiers with high precision for this task. Furthermore, we demonstrate the value of analyzing users\u2019 opinions about the severity of threats reported online as an early indicator of important software vulnerabilities. We present a simple, yet effective method for linking software vulnerabilities reported in tweets to Common Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD). Using our predicted severity scores, we show that it is possible to achieve a Precision@50 of 0.86 when forecasting high severity vulnerabilities, significantly outperforming a baseline that is based on tweet volume. Finally we show how reports of severe vulnerabilities online are predictive of real-world exploits.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fake News Detection using Deep Markov Random Fields",
        "paper_url": "https://aclanthology.org/N19-1141/",
        "paper_authors": [
            "Duc Minh Nguyen",
            "Tien Huu Do",
            "Robert Calderbank",
            "Nikos Deligiannis"
        ],
        "paper_abstract": "Deep-learning-based models have been successfully applied to the problem of detecting fake news on social media. While the correlations among news articles have been shown to be effective cues for online news analysis, existing deep-learning-based methods often ignore this information and only consider each news article individually. To overcome this limitation, we develop a graph-theoretic method that inherits the power of deep learning while at the same time utilizing the correlations among the articles. We formulate fake news detection as an inference problem in a Markov random field (MRF) which can be solved by the iterative mean-field algorithm. We then unfold the mean-field algorithm into hidden layers that are composed of common neural network operations. By integrating these hidden layers on top of a deep network, which produces the MRF potentials, we obtain our deep MRF model for fake news detection. Experimental results on well-known datasets show that the proposed model improves upon various state-of-the-art models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Issue Framing in Online Discussion Fora",
        "paper_url": "https://aclanthology.org/N19-1142/",
        "paper_authors": [
            "Mareike Hartmann",
            "Tallulah Jansen",
            "Isabelle Augenstein",
            "Anders S\u00f8gaard"
        ],
        "paper_abstract": "In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Vector of Locally Aggregated Embeddings for Text Representation",
        "paper_url": "https://aclanthology.org/N19-1143/",
        "paper_authors": [
            "Hadi Amiri",
            "Mitra Mohtarami"
        ],
        "paper_abstract": "We present Vector of Locally Aggregated Embeddings (VLAE) for effective and, ultimately, lossless representation of textual content. Our model encodes each input text by effectively identifying and integrating the representations of its semantically-relevant parts. The proposed model generates high quality representation of textual content and improves the classification performance of current state-of-the-art deep averaging networks across several text classification tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Predicting the Type and Target of Offensive Posts in Social Media",
        "paper_url": "https://aclanthology.org/N19-1144/",
        "paper_authors": [
            "Marcos Zampieri",
            "Shervin Malmasi",
            "Preslav Nakov",
            "Sara Rosenthal",
            "Noura Farra",
            "Ritesh Kumar"
        ],
        "paper_abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Biomedical Event Extraction based on Knowledge-driven Tree-LSTM",
        "paper_url": "https://aclanthology.org/N19-1145/",
        "paper_authors": [
            "Diya Li",
            "Lifu Huang",
            "Heng Ji",
            "Jiawei Han"
        ],
        "paper_abstract": "Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting cognitive impairments by agreeing on interpretations of linguistic features",
        "paper_url": "https://aclanthology.org/N19-1146/",
        "paper_authors": [
            "Zining Zhu",
            "Jekaterina Novikova",
            "Frank Rudzicz"
        ],
        "paper_abstract": "Linguistic features have shown promising applications for detecting various cognitive impairments. To improve detection accuracies, increasing the amount of data or the number of linguistic features have been two applicable approaches. However, acquiring additional clinical data can be expensive, and hand-crafting features is burdensome. In this paper, we take a third approach, proposing Consensus Networks (CNs), a framework to classify after reaching agreements between modalities. We divide linguistic features into non-overlapping subsets according to their modalities, and let neural networks learn low-dimensional representations that agree with each other. These representations are passed into a classifier network. All neural networks are optimized iteratively. In this paper, we also present two methods that improve the performance of CNs. We then present ablation studies to illustrate the effectiveness of modality division. To understand further what happens in CNs, we visualize the representations during training. Overall, using all of the 413 linguistic features, our models significantly outperform traditional classifiers, which are used by the state-of-the-art papers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Relation Extraction using Explicit Context Conditioning",
        "paper_url": "https://aclanthology.org/N19-1147/",
        "paper_authors": [
            "Gaurav Singh",
            "Parminder Bhatia"
        ],
        "paper_abstract": "Relation extraction (RE) aims to label relations between groups of marked entities in raw text. Most current RE models learn context-aware representations of the target entities that are then used to establish relation between them. This works well for intra-sentence RE, and we call them first-order relations. However, this methodology can sometimes fail to capture complex and long dependencies. To address this, we hypothesize that at times the target entities can be connected via a context token. We refer to such indirect relations as second-order relations, and describe an efficient implementation for computing them. These second-order relation scores are then combined with first-order relation scores to obtain final relation scores. Our empirical results show that the proposed method leads to state-of-the-art performance over two biomedical datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Conversation Model Fine-Tuning for Classifying Client Utterances in Counseling Dialogues",
        "paper_url": "https://aclanthology.org/N19-1148/",
        "paper_authors": [
            "Sungjoon Park",
            "Donghyun Kim",
            "Alice Oh"
        ],
        "paper_abstract": "The recent surge of text-based online counseling applications enables us to collect and analyze interactions between counselors and clients. A dataset of those interactions can be used to learn to automatically classify the client utterances into categories that help counselors in diagnosing client status and predicting counseling outcome. With proper anonymization, we collect counselor-client dialogues, define meaningful categories of client utterances with professional counselors, and develop a novel neural network model for classifying the client utterances. The central idea of our model, ConvMFiT, is a pre-trained conversation model which consists of a general language model built from an out-of-domain corpus and two role-specific language models built from unlabeled in-domain dialogues. The classification result shows that ConvMFiT outperforms state-of-the-art comparison models. Further, the attention weights in the learned model confirm that the model finds expected linguistic patterns for each category.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Using Similarity Measures to Select Pretraining Data for NER",
        "paper_url": "https://aclanthology.org/N19-1149/",
        "paper_authors": [
            "Xiang Dai",
            "Sarvnaz Karimi",
            "Ben Hachey",
            "Cecile Paris"
        ],
        "paper_abstract": "Word vectors and Language Models (LMs) pretrained on a large amount of unlabelled data can dramatically improve various Natural Language Processing (NLP) tasks. However, the measure and impact of similarity between pretraining data and target task data are left to intuition. We propose three cost-effective measures to quantify different aspects of similarity between source pretraining and target task data. We demonstrate that these measures are good predictors of the usefulness of pretrained models for Named Entity Recognition (NER) over 30 data pairs. Results also suggest that pretrained LMs are more effective and more predictable than pretrained word vectors, but pretrained word vectors are better when pretraining data is dissimilar.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
        "paper_url": "https://aclanthology.org/N19-1150/",
        "paper_authors": [
            "Yinfei Yang",
            "Oshin Agarwal",
            "Chris Tar",
            "Byron C. Wallace",
            "Ani Nenkova"
        ],
        "paper_abstract": "Modern NLP systems require high-quality annotated data. For specialized domains, expert annotations may be prohibitively expensive; the alternative is to rely on crowdsourcing to reduce costs at the risk of introducing noise. In this paper we demonstrate that directly modeling instance difficulty can be used to improve model performance and to route instances to appropriate annotators. Our difficulty prediction model combines two learned representations: a \u2018universal\u2019 encoder trained on out of domain data, and a task-specific encoder. Experiments on a complex biomedical information extraction task using expert and lay annotators show that: (i) simply excluding from the training data instances predicted to be difficult yields a small boost in performance; (ii) using difficulty scores to weight instances during training provides further, consistent gains; (iii) assigning instances predicted to be difficult to domain experts is an effective strategy for task routing. Further, our experiments confirm the expectation that for such domain-specific tasks expert annotations are of much higher quality and preferable to obtain if practical and that augmenting small amounts of expert data with a larger set of lay annotations leads to further improvements in model performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting Depression in Social Media using Fine-Grained Emotions",
        "paper_url": "https://aclanthology.org/N19-1151/",
        "paper_authors": [
            "Mario Ezra Arag\u00f3n",
            "Adrian Pastor L\u00f3pez-Monroy",
            "Luis Carlos Gonz\u00e1lez-Gurrola",
            "Manuel Montes-y-G\u00f3mez"
        ],
        "paper_abstract": "Nowadays social media platforms are the most popular way for people to share information, from work issues to personal matters. For example, people with health disorders tend to share their concerns for advice, support or simply to relieve suffering. This provides a great opportunity to proactively detect these users and refer them as soon as possible to professional help. We propose a new representation called Bag of Sub-Emotions (BoSE), which represents social media documents by a set of fine-grained emotions automatically generated using a lexical resource of emotions and subword embeddings. The proposed representation is evaluated in the task of depression detection. The results are encouraging; the usage of fine-grained emotions improved the results from a representation based on the core emotions and obtained competitive results in comparison to state of the art approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Silver Standard Corpus of Human Phenotype-Gene Relations",
        "paper_url": "https://aclanthology.org/N19-1152/",
        "paper_authors": [
            "Diana Sousa",
            "Andre Lamurias",
            "Francisco M. Couto"
        ],
        "paper_abstract": "Human phenotype-gene relations are fundamental to fully understand the origin of some phenotypic abnormalities and their associated diseases. Biomedical literature is the most comprehensive source of these relations, however, we need Relation Extraction tools to automatically recognize them. Most of these tools require an annotated corpus and to the best of our knowledge, there is no corpus available annotated with human phenotype-gene relations. This paper presents the Phenotype-Gene Relations (PGR) corpus, a silver standard corpus of human phenotype and gene annotations and their relations. The corpus consists of 1712 abstracts, 5676 human phenotype annotations, 13835 gene annotations, and 4283 relations. We generated this corpus using Named-Entity Recognition tools, whose results were partially evaluated by eight curators, obtaining a precision of 87.01%. By using the corpus we were able to obtain promising results with two state-of-the-art deep learning tools, namely 78.05% of precision. The PGR corpus was made publicly available to the research community.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Lemmatization of Non-Standard Languages with Joint Learning",
        "paper_url": "https://aclanthology.org/N19-1153/",
        "paper_authors": [
            "Enrique Manjavacas",
            "\u00c1kos K\u00e1d\u00e1r",
            "Mike Kestemont"
        ],
        "paper_abstract": "Lemmatization of standard languages is concerned with (i) abstracting over morphological differences and (ii) resolving token-lemma ambiguities of inflected words in order to map them to a dictionary headword. In the present paper we aim to improve lemmatization performance on a set of non-standard historical languages in which the difficulty is increased by an additional aspect (iii): spelling variation due to lacking orthographic standards. We approach lemmatization as a string-transduction task with an Encoder-Decoder architecture which we enrich with sentence information using a hierarchical sentence encoder. We show significant improvements over the state-of-the-art by fine-tuning the sentence encodings to jointly optimize a bidirectional language model loss. Crucially, our architecture does not require POS or morphological annotations, which are not always available for historical corpora. Additionally, we also test the proposed model on a set of typologically diverse standard languages showing results on par or better than a model without fine-tuned sentence representations and previous state-of-the-art systems. Finally, to encourage future work on processing of non-standard varieties, we release the dataset of non-standard languages underlying the present study, which is based on openly accessible sources.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "One Size Does Not Fit All: Comparing NMT Representations of Different Granularities",
        "paper_url": "https://aclanthology.org/N19-1154/",
        "paper_authors": [
            "Nadir Durrani",
            "Fahim Dalvi",
            "Hassan Sajjad",
            "Yonatan Belinkov",
            "Preslav Nakov"
        ],
        "paper_abstract": "Recent work has shown that contextualized word representations derived from neural machine translation are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model morphology, syntax, and semantics. We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Simple Joint Model for Improved Contextual Neural Lemmatization",
        "paper_url": "https://aclanthology.org/N19-1155/",
        "paper_authors": [
            "Chaitanya Malaviya",
            "Shijie Wu",
            "Ryan Cotterell"
        ],
        "paper_abstract": "English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of lemmatization seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the model in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and lemmatization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Probabilistic Generative Model of Linguistic Typology",
        "paper_url": "https://aclanthology.org/N19-1156/",
        "paper_authors": [
            "Johannes Bjerva",
            "Yova Kementchedjhieva",
            "Ryan Cotterell",
            "Isabelle Augenstein"
        ],
        "paper_abstract": "In the principles-and-parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry\u2014we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, outperforming several baselines on the task of predicting held-out features. Furthermore, we show that language embeddings pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Quantifying the morphosyntactic content of Brown Clusters",
        "paper_url": "https://aclanthology.org/N19-1157/",
        "paper_authors": [
            "Manuel R. Ciosici",
            "Leon Derczynski",
            "Ira Assent"
        ],
        "paper_abstract": "Brown and Exchange word clusters have long been successfully used as word representations in Natural Language Processing (NLP) systems. Their success has been attributed to their seeming ability to represent both semantic and syntactic information. Using corpora representing several language families, we test the hypothesis that Brown and Exchange word clusters are highly effective at encoding morphosyntactic information. Our experiments show that word clusters are highly capable at distinguishing Parts of Speech. We show that increases in Average Mutual Information, the clustering algorithms\u2019 optimization goal, are highly correlated with improvements in encoding of morphosyntactic information. Our results provide empirical evidence that downstream NLP systems addressing tasks dependent on morphosyntactic information can benefit from word cluster features.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Analyzing Bayesian Crosslingual Transfer in Topic Models",
        "paper_url": "https://aclanthology.org/N19-1158/",
        "paper_authors": [
            "Shudong Hao",
            "Michael J. Paul"
        ],
        "paper_abstract": "We introduce a theoretical analysis of crosslingual transfer in probabilistic topic models. By formulating posterior inference through Gibbs sampling as a process of language transfer, we propose a new measure that quantifies the loss of knowledge across languages during this process. This measure enables us to derive a PAC-Bayesian bound that elucidates the factors affecting model quality, both during training and in downstream applications. We provide experimental validation of the analysis on a diverse set of five languages, and discuss best practices for data collection and model design based on our analysis.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Recursive Subtree Composition in LSTM-Based Dependency Parsing",
        "paper_url": "https://aclanthology.org/N19-1159/",
        "paper_authors": [
            "Miryam de Lhoneux",
            "Miguel Ballesteros",
            "Joakim Nivre"
        ],
        "paper_abstract": "The need for tree structure modelling on top of sequence modelling is an open issue in neural dependency parsing. We investigate the impact of adding a tree layer on top of a sequential model by recursively composing subtree representations (composition) in a transition-based parser that uses features extracted by a BiLSTM. Composition seems superfluous with such a model, suggesting that BiLSTMs capture information about subtrees. We perform model ablations to tease out the conditions under which composition helps. When ablating the backward LSTM, performance drops and composition does not recover much of the gap. When ablating the forward LSTM, performance drops less dramatically and composition recovers a substantial part of the gap, indicating that a forward LSTM and composition capture similar information. We take the backward LSTM to be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved lookahead of a backward LSTM is especially important for head-final languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-lingual CCG Induction",
        "paper_url": "https://aclanthology.org/N19-1160/",
        "paper_authors": [
            "Kilian Evang"
        ],
        "paper_abstract": "Combinatory categorial grammars are linguistically motivated and useful for semantic parsing, but costly to acquire in a supervised way and difficult to acquire in an unsupervised way. We propose an alternative making use of cross-lingual learning: an existing source-language parser is used together with a parallel corpus to induce a grammar and parsing model for a target language. On the PASCAL benchmark, cross-lingual CCG induction outperforms CCG induction from gold-standard POS tags on 3 out of 8 languages, and unsupervised CCG induction on 6 out of 8 languages. We also show that cross-lingually induced CCGs reflect syntactic properties of the target languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Density Matching for Bilingual Word Embedding",
        "paper_url": "https://aclanthology.org/N19-1161/",
        "paper_authors": [
            "Chunting Zhou",
            "Xuezhe Ma",
            "Di Wang",
            "Graham Neubig"
        ],
        "paper_abstract": "Recent approaches to cross-lingual word embedding have generally been based on linear transformations between the sets of embedding vectors in the two languages. In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving robustness and generalization to mappings between difficult language pairs or word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",
        "paper_url": "https://aclanthology.org/N19-1162/",
        "paper_authors": [
            "Tal Schuster",
            "Ori Ram",
            "Regina Barzilay",
            "Amir Globerson"
        ],
        "paper_abstract": "We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the context-dependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Early Rumour Detection",
        "paper_url": "https://aclanthology.org/N19-1163/",
        "paper_authors": [
            "Kaimin Zhou",
            "Chang Shu",
            "Binyang Li",
            "Jey Han Lau"
        ],
        "paper_abstract": "Rumours can spread quickly through social media, and malicious ones can bring about significant economical and social impact. Motivated by this, our paper focuses on the task of rumour detection; particularly, we are interested in understanding how early we can detect them. Although there are numerous studies on rumour detection, few are concerned with the timing of the detection. A successfully-detected malicious rumour can still cause significant damage if it isn\u2019t detected in a timely manner, and so timing is crucial. To address this, we present a novel methodology for early rumour detection. Our model treats social media posts (e.g. tweets) as a data stream and integrates reinforcement learning to learn the number minimum number of posts required before we classify an event as a rumour. Experiments on Twitter and Weibo demonstrate that our model identifies rumours earlier than state-of-the-art systems while maintaining a comparable accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Microblog Hashtag Generation via Encoding Conversation Contexts",
        "paper_url": "https://aclanthology.org/N19-1164/",
        "paper_authors": [
            "Yue Wang",
            "Jing Li",
            "Irwin King",
            "Michael R. Lyu",
            "Shuming Shi"
        ],
        "paper_abstract": "Automatic hashtag annotation plays an important role in content understanding for microblog posts. To date, progress made in this field has been restricted to phrase selection from limited candidates, or word-level hashtag discovery using topic models. Different from previous work considering hashtags to be inseparable, our work is the first effort to annotate hashtags with a novel sequence generation framework via viewing the hashtag as a short sequence of words. Moreover, to address the data sparsity issue in processing short microblog posts, we propose to jointly model the target posts and the conversation contexts initiated by them with bidirectional attention. Extensive experimental results on two large-scale datasets, newly collected from English Twitter and Chinese Weibo, show that our model significantly outperforms state-of-the-art models based on classification. Further studies demonstrate our ability to effectively generate rare and even unseen hashtags, which is however not possible for most existing methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems",
        "paper_url": "https://aclanthology.org/N19-1165/",
        "paper_authors": [
            "Steffen Eger",
            "G\u00f6zde G\u00fcl \u015eahin",
            "Andreas R\u00fcckl\u00e9",
            "Ji-Ung Lee",
            "Claudia Schulz",
            "Mohsen Mesgar",
            "Krishnkant Swarnkar",
            "Edwin Simpson",
            "Iryna Gurevych"
        ],
        "paper_abstract": "Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., \u201c!d10t\u201d) or as a writing style (\u201c1337\u201d in \u201cleet speak\u201d), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82%. We then explore three shielding methods\u2014visual character embeddings, adversarial training, and rule-based recovery\u2014which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Something\u2019s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features",
        "paper_url": "https://aclanthology.org/N19-1166/",
        "paper_authors": [
            "Jack Hessel",
            "Lillian Lee"
        ],
        "paper_abstract": "Controversial posts are those that split the preferences of a community, receiving both significant positive and significant negative feedback. Our inclusion of the word \u201ccommunity\u201d here is deliberate: what is controversial to some audiences may not be so to others. Using data from several different communities on reddit.com, we predict the ultimate controversiality of posts, leveraging features drawn from both the textual content and the tree structure of the early comments that initiate the discussion. We find that even when only a handful of comments are available, e.g., the first 5 comments made within 15 minutes of the original post, discussion features often add predictive capacity to strong content-and- rate only baselines. Additional experiments on domain transfer suggest that conversation- structure features often generalize to other communities better than conversation-content features do.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "No Permanent Friends or Enemies: Tracking Relationships between Nations from News",
        "paper_url": "https://aclanthology.org/N19-1167/",
        "paper_authors": [
            "Xiaochuang Han",
            "Eunsol Choi",
            "Chenhao Tan"
        ],
        "paper_abstract": "Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore unsupervised neural models to infer relations between nations from news articles. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding international relations requires carefully analyzing complex relationships, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our model and give insightful feedback that suggests future directions for human-centered models. Furthermore, our model reveals interesting regional differences in news coverage. For instance, with respect to US-China relations, Singaporean media focus more on \u201cstrengthening\u201d and \u201cpurchasing\u201d, while US media focus more on \u201ccriticizing\u201d and \u201cdenouncing\u201d.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Human Text Comprehension through Semi-Markov CRF-based Neural Section Title Generation",
        "paper_url": "https://aclanthology.org/N19-1168/",
        "paper_authors": [
            "Sebastian Gehrmann",
            "Steven Layne",
            "Franck Dernoncourt"
        ],
        "paper_abstract": "Titles of short sections within long documents support readers by guiding their focus towards relevant passages and by providing anchor-points that help to understand the progression of the document. The positive effects of section titles are even more pronounced when measured on readers with less developed reading abilities, for example in communities with limited labeled text resources. We, therefore, aim to develop techniques to generate section titles in low-resource environments. In particular, we present an extractive pipeline for section title generation by first selecting the most salient sentence and then applying deletion-based compression. Our compression approach is based on a Semi-Markov Conditional Random Field that leverages unsupervised word-representations such as ELMo or BERT, eliminating the need for a complex encoder-decoder architecture. The results show that this approach leads to competitive performance with sequence-to-sequence models with high resources, while strongly outperforming it with low resources. In a human-subject study across subjects with varying reading abilities, we find that our section titles improve the speed of completing comprehension tasks while retaining similar accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unifying Human and Statistical Evaluation for Natural Language Generation",
        "paper_url": "https://aclanthology.org/N19-1169/",
        "paper_authors": [
            "Tatsunori B. Hashimoto",
            "Hugh Zhang",
            "Percy Liang"
        ],
        "paper_abstract": "How can we measure whether a natural language generation system produces both high quality and diverse outputs? Human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized. In this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human- or machine-generated. We demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call HUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease HUSE due to decreased diversity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "What makes a good conversation? How controllable attributes affect human judgments",
        "paper_url": "https://aclanthology.org/N19-1170/",
        "paper_authors": [
            "Abigail See",
            "Stephen Roller",
            "Douwe Kiela",
            "Jason Weston"
        ],
        "paper_abstract": "A good conversation requires balance \u2013 between simplicity and detail; staying on topic and changing it; asking questions and answering them. Although dialogue agents are commonly evaluated via human judgments of overall quality, the relationship between quality and these individual factors is less well-studied. In this work, we examine two controllable neural text generation methods, conditional training and weighted decoding, in order to control four important attributes for chit-chat dialogue: repetition, specificity, response-relatedness and question-asking. We conduct a large-scale human evaluation to measure the effect of these control parameters on multi-turn interactive conversations on the PersonaChat task. We provide a detailed analysis of their relationship to high-level aspects of conversation, and show that by controlling combinations of these variables our models obtain clear improvements in human quality judgments.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Empirical Investigation of Global and Local Normalization for Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search",
        "paper_url": "https://aclanthology.org/N19-1171/",
        "paper_authors": [
            "Kartik Goyal",
            "Chris Dyer",
            "Taylor Berg-Kirkpatrick"
        ],
        "paper_abstract": "Globally normalized neural sequence models are considered superior to their locally normalized equivalents because they may ameliorate the effects of label bias. However, when considering high-capacity neural parametrizations that condition on the whole input sequence, both model classes are theoretically equivalent in terms of the distributions they are capable of representing. Thus, the practical advantage of global normalization in the context of modern neural methods remains unclear. In this paper, we attempt to shed light on this problem through an empirical study. We extend an approach for search-aware training via a continuous relaxation of beam search (Goyal et al., 2017b) in order to enable training of globally normalized recurrent sequence models through simple backpropagation. We then use this technique to conduct an empirical study of the interaction between global normalization, high-capacity encoders, and search-aware optimization. We observe that in the context of inexact search, globally normalized neural models are still more effective than their locally normalized counterparts. Further, since our training approach is sensitive to warm-starting with pre-trained models, we also propose a novel initialization strategy based on self-normalization for pre-training globally normalized models. We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global normalization under different conditions while using search-aware training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pun Generation with Surprise",
        "paper_url": "https://aclanthology.org/N19-1172/",
        "paper_authors": [
            "He He",
            "Nanyun Peng",
            "Percy Liang"
        ],
        "paper_abstract": "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., \u201cdied\u201d and \u201cdyed\u201d). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., \u201cdyed\u201d) and the distant context, but a strong association between the alternative word (e.g., \u201cdied\u201d) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30% of the time, doubling the success rate of a neural generation baseline.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Single Document Summarization as Tree Induction",
        "paper_url": "https://aclanthology.org/N19-1173/",
        "paper_authors": [
            "Yang Liu",
            "Ivan Titov",
            "Mirella Lapata"
        ],
        "paper_abstract": "In this paper, we conceptualize single-document extractive summarization as a tree induction problem. In contrast to previous approaches which have relied on linguistically motivated document representations to generate summaries, our model induces a multi-root dependency tree while predicting the output summary. Each root node in the tree is a summary sentence, and the subtrees attached to it are sentences whose content relates to or explains the summary sentence. We design a new iterative refinement algorithm: it induces the trees through repeatedly refining the structures predicted by previous iterations. We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fixed That for You: Generating Contrastive Claims with Semantic Edits",
        "paper_url": "https://aclanthology.org/N19-1174/",
        "paper_authors": [
            "Christopher Hidey",
            "Kathy McKeown"
        ],
        "paper_abstract": "Understanding contrastive opinions is a key component of argument generation. Central to an argument is the claim, a statement that is in dispute. Generating a counter-argument then requires generating a response in contrast to the main claim of the original argument. To generate contrastive claims, we create a corpus of Reddit comment pairs self-labeled by posters using the acronym FTFY (fixed that for you). We then train neural models on these pairs to edit the original claim and produce a new claim with a different view. We demonstrate significant improvement over a sequence-to-sequence baseline in BLEU score and a human evaluation for fluency, coherence, and contrast.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Box of Lies: Multimodal Deception Detection in Dialogues",
        "paper_url": "https://aclanthology.org/N19-1175/",
        "paper_authors": [
            "Felix Soldner",
            "Ver\u00f3nica P\u00e9rez-Rosas",
            "Rada Mihalcea"
        ],
        "paper_abstract": "Deception often takes place during everyday conversations, yet conversational dialogues remain largely unexplored by current work on automatic deception detection. In this paper, we address the task of detecting multimodal deceptive cues during conversational dialogues. We introduce a multimodal dataset containing deceptive conversations between participants playing the Box of Lies game from The Tonight Show Starring Jimmy Fallon, in which they try to guess whether an object description provided by their opponent is deceptive or not. We conduct annotations of multimodal communication behaviors, including facial and linguistic behaviors, and derive several learning features based on these annotations. Initial classification experiments show promising results, performing well above both a random and a human baseline, and reaching up to 69% accuracy in distinguishing deceptive and truthful behaviors.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric Interpretation",
        "paper_url": "https://aclanthology.org/N19-1176/",
        "paper_authors": [
            "Massimo Poesio",
            "Jon Chamberlain",
            "Silviu Paun",
            "Juntao Yu",
            "Alexandra Uma",
            "Udo Kruschwitz"
        ],
        "paper_abstract": "We present a corpus of anaphoric information (coreference) crowdsourced through a game-with-a-purpose. The corpus, containing annotations for about 108,000 markables, is one of the largest corpora for coreference for English, and one of the largest crowdsourced NLP corpora, but its main feature is the large number of judgments per markable: 20 on average, and over 2.2M in total. This characteristic makes the corpus a unique resource for the study of disagreements on anaphoric interpretation. A second distinctive feature is its rich annotation scheme, covering singletons, expletives, and split-antecedent plurals. Finally, the corpus also comes with labels inferred using a recently proposed probabilistic model of annotation for coreference. The labels are of high quality and make it possible to successfully train a state of the art coreference resolver, including training on singletons and non-referring expressions. The annotation model can also result in more than one label, or no label, being proposed for a markable, thus serving as a baseline method for automatically identifying ambiguous markables. A preliminary analysis of the results is presented.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Streamlined Method for Sourcing Discourse-level Argumentation Annotations from the Crowd",
        "paper_url": "https://aclanthology.org/N19-1177/",
        "paper_authors": [
            "Tristan Miller",
            "Maria Sukhareva",
            "Iryna Gurevych"
        ],
        "paper_abstract": "The study of argumentation and the development of argument mining tools depends on the availability of annotated data, which is challenging to obtain in sufficient quantity and quality. We present a method that breaks down a popular but relatively complex discourse-level argument annotation scheme into a simpler, iterative procedure that can be applied even by untrained annotators. We apply this method in a crowdsourcing setup and report on the reliability of the annotations obtained. The source code for a tool implementing our annotation method, as well as the sample data we obtained (4909 gold-standard annotations across 982 documents), are freely released to the research community. These are intended to serve the needs of qualitative research into argumentation, as well as of data-driven approaches to argument mining.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Dialog Structure Learning",
        "paper_url": "https://aclanthology.org/N19-1178/",
        "paper_authors": [
            "Weiyan Shi",
            "Tiancheng Zhao",
            "Zhou Yu"
        ],
        "paper_abstract": "Learning a shared dialog structure from a set of task-oriented dialogs is an important challenge in computational linguistics. The learned dialog structure can shed light on how to analyze human dialogs, and more importantly contribute to the design and evaluation of dialog systems. We propose to extract dialog structures using a modified VRNN model with discrete latent vectors. Different from existing HMM-based models, our model is based on variational-autoencoder (VAE). Such model is able to capture more dynamics in dialogs beyond the surface forms of the language. We find that qualitatively, our method extracts meaningful dialog structure, and quantitatively, outperforms previous models on the ability to predict unseen data. We further evaluate the model\u2019s effectiveness in a downstream task, the dialog system building task. Experiments show that, by integrating the learned dialog structure into the reward function design, the model converges faster and to a better outcome in a reinforcement learning setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modeling Document-level Causal Structures for Event Causal Relation Identification",
        "paper_url": "https://aclanthology.org/N19-1179/",
        "paper_authors": [
            "Lei Gao",
            "Prafulla Kumar Choubey",
            "Ruihong Huang"
        ],
        "paper_abstract": "We aim to comprehensively identify all the event causal relations in a document, both within a sentence and across sentences, which is important for reconstructing pivotal event structures. The challenges we identified are two: 1) event causal relations are sparse among all possible event pairs in a document, in addition, 2) few causal relations are explicitly stated. Both challenges are especially true for identifying causal relations between events across sentences. To address these challenges, we model rich aspects of document-level causal structures for achieving comprehensive causal relation identification. The causal structures include heavy involvements of document-level main events in causal relations as well as several types of fine-grained constraints that capture implications from certain sentential syntactic relations and discourse relations as well as interactions between event causal relations and event coreference relations. Our experimental results show that modeling the global and fine-grained aspects of causal structures using Integer Linear Programming (ILP) greatly improves the performance of causal relation identification, especially in identifying cross-sentence causal relations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Hierarchical User and Item Representation with Three-Tier Attention for Recommendation",
        "paper_url": "https://aclanthology.org/N19-1180/",
        "paper_authors": [
            "Chuhan Wu",
            "Fangzhao Wu",
            "Junxin Liu",
            "Yongfeng Huang"
        ],
        "paper_abstract": "Utilizing reviews to learn user and item representations is useful for recommender systems. Existing methods usually merge all reviews from the same user or for the same item into a long document. However, different reviews, sentences and even words usually have different informativeness for modeling users and items. In this paper, we propose a hierarchical user and item representation model with three-tier attention to learn user and item representations from reviews for recommendation. Our model contains three major components, i.e., a sentence encoder to learn sentence representations from words, a review encoder to learn review representations from sentences, and a user/item encoder to learn user/item representations from reviews. In addition, we incorporate a three-tier attention network in our model to select important words, sentences and reviews. Besides, we combine the user and item representations learned from the reviews with user and item embeddings based on IDs as the final representations to capture the latent factors of individual users and items. Extensive experiments on four benchmark datasets validate the effectiveness of our approach.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Text Similarity Estimation Based on Word Embeddings and Matrix Norms for Targeted Marketing",
        "paper_url": "https://aclanthology.org/N19-1181/",
        "paper_authors": [
            "Tim vor der Br\u00fcck",
            "Marc Pouly"
        ],
        "paper_abstract": "The prevalent way to estimate the similarity of two documents based on word embeddings is to apply the cosine similarity measure to the two centroids obtained from the embedding vectors associated with the words in each document. Motivated by an industrial application from the domain of youth marketing, where this approach produced only mediocre results, we propose an alternative way of combining the word vectors using matrix norms. The evaluation shows superior results for most of the investigated matrix norms in comparison to both the classical cosine measure and several other document similarity estimates.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Glocal: Incorporating Global Information in Local Convolution for Keyphrase Extraction",
        "paper_url": "https://aclanthology.org/N19-1182/",
        "paper_authors": [
            "Animesh Prasad",
            "Min-Yen Kan"
        ],
        "paper_abstract": "Graph Convolutional Networks (GCNs) are a class of spectral clustering techniques that leverage localized convolution filters to perform supervised classification directly on graphical structures. While such methods model nodes\u2019 local pairwise importance, they lack the capability to model global importance relative to other nodes of the graph. This causes such models to miss critical information in tasks where global ranking is a key component for the task, such as in keyphrase extraction. We address this shortcoming by allowing the proper incorporation of global information into the GCN family of models through the use of scaled node weights. In the context of keyphrase extraction, incorporating global random walk scores obtained from TextRank boosts performance significantly. With our proposed method, we achieve state-of-the-art results, bettering a strong baseline by an absolute 2% increase in F1 score.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Study of Latent Structured Prediction Approaches to Passage Reranking",
        "paper_url": "https://aclanthology.org/N19-1183/",
        "paper_authors": [
            "Iryna Haponchyk",
            "Alessandro Moschitti"
        ],
        "paper_abstract": "The structured output framework provides a helpful tool for learning to rank problems. In this paper, we propose a structured output approach which regards rankings as latent variables. Our approach addresses the complex optimization of Mean Average Precision (MAP) ranking metric. We provide an inference procedure to find the max-violating ranking based on the decomposition of the corresponding loss. The results of our experiments on WikiQA and TREC13 datasets show that our reranking based on structured prediction is a promising research direction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Combining Distant and Direct Supervision for Neural Relation Extraction",
        "paper_url": "https://aclanthology.org/N19-1184/",
        "paper_authors": [
            "Iz Beltagy",
            "Kyle Lo",
            "Waleed Ammar"
        ],
        "paper_abstract": "In relation extraction with distant supervision, noisy labels make it difficult to train quality models. Previous neural models addressed this problem using an attention mechanism that attends to sentences that are likely to express the relations. We improve such models by combining the distant supervision data with an additional directly-supervised data, which we use as supervision for the attention weights. We find that joint training on both types of supervision leads to a better model because it improves the model\u2019s ability to identify noisy sentences. In addition, we find that sigmoidal attention weights with max pooling achieves better performance over the commonly used weighted average attention in this setup. Our proposed method achieves a new state-of-the-art result on the widely used FB-NYT dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Tweet Stance Detection Using an Attention based Neural Ensemble Model",
        "paper_url": "https://aclanthology.org/N19-1185/",
        "paper_authors": [
            "Umme Aymun Siddiqua",
            "Abu Nowshed Chy",
            "Masaki Aono"
        ],
        "paper_abstract": "Stance detection in twitter aims at mining user stances expressed in a tweet towards a single or multiple target entities. To tackle this problem, most of the prior studies have been explored the traditional deep learning models, e.g., LSTM and GRU. However, in compared to these traditional approaches, recently proposed densely connected Bi-LSTM and nested LSTMs architectures effectively address the vanishing-gradient and overfitting problems as well as dealing with long-term dependencies. In this paper, we propose a neural ensemble model that adopts the strengths of these two LSTM variants to learn better long-term dependencies, where each module coupled with an attention mechanism that amplifies the contribution of important elements in the final representation. We also employ a multi-kernel convolution on top of them to extract the higher-level tweet representations. Results of extensive experiments on single and multi-target stance detection datasets show that our proposed method achieves substantial improvement over the current state-of-the-art deep learning based methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Word Embedding-Based Automatic MT Evaluation Metric using Word Position Information",
        "paper_url": "https://aclanthology.org/N19-1186/",
        "paper_authors": [
            "Hiroshi Echizen\u2019ya",
            "Kenji Araki",
            "Eduard Hovy"
        ],
        "paper_abstract": "We propose a new automatic evaluation metric for machine translation. Our proposed metric is obtained by adjusting the Earth Mover\u2019s Distance (EMD) to the evaluation task. The EMD measure is used to obtain the distance between two probability distributions consisting of some signatures having a feature and a weight. We use word embeddings, sentence-level tf-idf, and cosine similarity between two word embeddings, respectively, as the features, weight, and the distance between two features. Results show that our proposed metric can evaluate machine translation based on word meaning. Moreover, for distance, cosine similarity and word position information are used to address word-order differences. We designate this metric as Word Embedding-Based automatic MT evaluation using Word Position Information (WE_WPI). A meta-evaluation using WMT16 metrics shared task set indicates that our WE_WPI achieves the highest correlation with human judgment among several representative metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Stop in Structured Prediction for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1187/",
        "paper_authors": [
            "Mingbo Ma",
            "Renjie Zheng",
            "Liang Huang"
        ],
        "paper_abstract": "Beam search optimization (Wiseman and Rush, 2016) resolves many issues in neural machine translation. However, this method lacks principled stopping criteria and does not learn how to stop during training, and the model naturally prefers longer hypotheses during the testing time in practice since they use the raw score instead of the probability-based score. We propose a novel ranking method which enables an optimal beam search stop- ping criteria. We further introduce a structured prediction loss function which penalizes suboptimal finished candidates produced by beam search during training. Experiments of neural machine translation on both synthetic data and real languages (German\u2192English and Chinese\u2192English) demonstrate our pro- posed methods lead to better length and BLEU score.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Unsupervised Multilingual Word Embeddings with Incremental Multilingual Hubs",
        "paper_url": "https://aclanthology.org/N19-1188/",
        "paper_authors": [
            "Geert Heyman",
            "Bregt Verreet",
            "Ivan Vuli\u0107",
            "Marie-Francine Moens"
        ],
        "paper_abstract": "Recent research has discovered that a shared bilingual word embedding space can be induced by projecting monolingual word embedding spaces from two languages using a self-learning paradigm without any bilingual supervision. However, it has also been shown that for distant language pairs such fully unsupervised self-learning methods are unstable and often get stuck in poor local optima due to reduced isomorphism between starting monolingual spaces. In this work, we propose a new robust framework for learning unsupervised multilingual word embeddings that mitigates the instability issues. We learn a shared multilingual embedding space for a variable number of languages by incrementally adding new languages one by one to the current multilingual space. Through the gradual language addition the method can leverage the interdependencies between the new language and all other languages in the current multilingual space. We find that it is beneficial to project more distant languages later in the iterative process. Our fully unsupervised multilingual embedding spaces yield results that are on par with the state-of-the-art methods in the bilingual lexicon induction (BLI) task, and simultaneously obtain state-of-the-art scores on two downstream tasks: multilingual document classification and multilingual dependency parsing, outperforming even supervised baselines. This finding also accentuates the need to establish evaluation protocols for cross-lingual word embeddings beyond the omnipresent intrinsic BLI task in future work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1189/",
        "paper_authors": [
            "Xuan Zhang",
            "Pamela Shapiro",
            "Gaurav Kumar",
            "Paul McNamee",
            "Marine Carpuat",
            "Kevin Duh"
        ],
        "paper_abstract": "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Robustness of Machine Translation with Synthetic Noise",
        "paper_url": "https://aclanthology.org/N19-1190/",
        "paper_authors": [
            "Vaibhav Vaibhav",
            "Sumeet Singh",
            "Craig Stewart",
            "Graham Neubig"
        ],
        "paper_abstract": "Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Non-Parametric Adaptation for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1191/",
        "paper_authors": [
            "Ankur Bapna",
            "Orhan Firat"
        ],
        "paper_abstract": "Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our Semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The Semi-parametric nature of our approach also opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Online Distilling from Checkpoints for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1192/",
        "paper_authors": [
            "Hao-Ran Wei",
            "Shujian Huang",
            "Ran Wang",
            "Xin-yu Dai",
            "Jiajun Chen"
        ],
        "paper_abstract": "Current predominant neural machine translation (NMT) models often have a deep structure with large amounts of parameters, making these models hard to train and easily suffering from over-fitting. A common practice is to utilize a validation set to evaluate the training process and select the best checkpoint. Average and ensemble techniques on checkpoints can lead to further performance improvement. However, as these methods do not affect the training process, the system performance is restricted to the checkpoints generated in original training procedure. In contrast, we propose an online knowledge distillation method. Our method on-the-fly generates a teacher model from checkpoints, guiding the training process to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against over-fitting. Furthermore, our method leads to an improvement in a machine reading experiment as well.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Value-based Search in Execution Space for Mapping Instructions to Programs",
        "paper_url": "https://aclanthology.org/N19-1193/",
        "paper_authors": [
            "Dor Muhlgay",
            "Jonathan Herzig",
            "Jonathan Berant"
        ],
        "paper_abstract": "Training models to map natural language instructions to programs, given target world supervision only, requires searching for good programs at training time. Search is commonly done using beam search in the space of partial programs or program trees, but as the length of the instructions grows finding a good program becomes difficult. In this work, we propose a search algorithm that uses the target world state, known at training time, to train a critic network that predicts the expected reward of every search state. We then score search states on the beam by interpolating their expected reward with the likelihood of programs represented by the search state. Moreover, we search not in the space of programs but in a more compressed state of program executions, augmented with recent entities and actions. On the SCONE dataset, we show that our algorithm dramatically improves performance on all three domains compared to standard beam search and other baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "VQD: Visual Query Detection In Natural Scenes",
        "paper_url": "https://aclanthology.org/N19-1194/",
        "paper_authors": [
            "Manoj Acharya",
            "Karan Jariwala",
            "Christopher Kanan"
        ],
        "paper_abstract": "We propose a new visual grounding task called Visual Query Detection (VQD). In VQD, the task is to localize a variable number of objects in an image where the objects are specified in natural language. VQD is related to visual referring expression comprehension, where the task is to localize only one object. We propose the first algorithms for VQD, and we evaluate them on both visual referring expression datasets and our new VQDv1 dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Natural Language Interaction with Robots Using Advice",
        "paper_url": "https://aclanthology.org/N19-1195/",
        "paper_authors": [
            "Nikhil Mehta",
            "Dan Goldwasser"
        ],
        "paper_abstract": "Over the last few years, there has been growing interest in learning models for physically grounded language understanding tasks, such as the popular blocks world domain. These works typically view this problem as a single-step process, in which a human operator gives an instruction and an automated agent is evaluated on its ability to execute it. In this paper we take the first step towards increasing the bandwidth of this interaction, and suggest a protocol for including advice, high-level observations about the task, which can help constrain the agent\u2019s prediction. We evaluate our approach on the blocks world task, and show that even simple advice can help lead to significant performance improvements. To help reduce the effort involved in supplying the advice, we also explore model self-generated advice which can still improve results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence Models",
        "paper_url": "https://aclanthology.org/N19-1196/",
        "paper_authors": [
            "Victor Prokhorov",
            "Mohammad Taher Pilehvar",
            "Nigel Collier"
        ],
        "paper_abstract": "We present a novel method for mapping unrestricted text to knowledge graph entities by framing the task as a sequence-to-sequence problem. Specifically, given the encoded state of an input text, our decoder directly predicts paths in the knowledge graph, starting from the root and ending at the the target node following hypernym-hyponym relationships. In this way, and in contrast to other text-to-entity mapping systems, our model outputs hierarchically structured predictions that are fully interpretable in the context of the underlying ontology, in an end-to-end manner. We present a proof-of-concept experiment with encouraging results, comparable to those of state-of-the-art systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA",
        "paper_url": "https://aclanthology.org/N19-1197/",
        "paper_authors": [
            "Jesse Thomason",
            "Daniel Gordon",
            "Yonatan Bisk"
        ],
        "paper_abstract": "We demonstrate the surprising strength of unimodal baselines in multimodal domains, and make concrete recommendations for best practices in future research. Where existing work often compares against random or majority class baselines, we argue that unimodal approaches better capture and reflect dataset biases and therefore provide an important comparison when assessing the performance of multimodal techniques. We present unimodal ablations on three recent datasets in visual navigation and QA, seeing an up to 29% absolute gain in performance over published baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ExCL: Extractive Clip Localization Using Natural Language Descriptions",
        "paper_url": "https://aclanthology.org/N19-1198/",
        "paper_authors": [
            "Soham Ghosh",
            "Anuva Agarwal",
            "Zarana Parekh",
            "Alexander Hauptmann"
        ],
        "paper_abstract": "The task of retrieving clips within videos based on a given natural language query requires cross-modal reasoning over multiple frames. Prior approaches such as sliding window classifiers are inefficient, while text-clip similarity driven ranking-based approaches such as segment proposal networks are far more complicated. In order to select the most relevant video clip corresponding to the given text description, we propose a novel extractive approach that predicts the start and end frames by leveraging cross-modal interactions between the text and video - this removes the need to retrieve and re-rank multiple proposal segments. Using recurrent networks we encode the two modalities into a joint representation which is then used in different variants of start-end frame predictor networks. Through extensive experimentation and ablative analysis, we demonstrate that our simple and elegant approach significantly outperforms state of the art on two datasets and has comparable performance on a third.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus",
        "paper_url": "https://aclanthology.org/N19-1199/",
        "paper_authors": [
            "Bai Li",
            "Yi-Te Hsu",
            "Frank Rudzicz"
        ],
        "paper_abstract": "Machine learning has shown promise for automatic detection of Alzheimer\u2019s disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of out-of-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-lingual Visual Verb Sense Disambiguation",
        "paper_url": "https://aclanthology.org/N19-1200/",
        "paper_authors": [
            "Spandana Gella",
            "Desmond Elliott",
            "Frank Keller"
        ],
        "paper_abstract": "Recent work has shown that visual context improves cross-lingual sense disambiguation for nouns. We extend this line of work to the more challenging task of cross-lingual verb sense disambiguation, introducing the MultiSense dataset of 9,504 images annotated with English, German, and Spanish verbs. Each image in MultiSense is annotated with an English verb and its translation in German or Spanish. We show that cross-lingual verb sense disambiguation models benefit from visual context, compared to unimodal baselines. We also show that the verb sense predicted by our best disambiguation model can improve the results of a text-only machine translation system when used for a multimodal translation task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Subword-Level Language Identification for Intra-Word Code-Switching",
        "paper_url": "https://aclanthology.org/N19-1201/",
        "paper_authors": [
            "Manuel Mager",
            "\u00d6zlem \u00c7etino\u011flu",
            "Katharina Kann"
        ],
        "paper_abstract": "Language identification for code-switching (CS), the phenomenon of alternating between two or more languages in conversations, has traditionally been approached under the assumption of a single language per token. However, if at least one language is morphologically rich, a large number of words can be composed of morphemes from more than one language (intra-word CS). In this paper, we extend the language identification task to the subword-level, such that it includes splitting mixed words while tagging each part with a language ID. We further propose a model for this task, which is based on a segmental recurrent neural network. In experiments on a new Spanish\u2013Wixarika dataset and on an adapted German\u2013Turkish dataset, our proposed model performs slightly better than or roughly on par with our best baseline, respectively. Considering only mixed words, however, it strongly outperforms all baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MuST-C: a Multilingual Speech Translation Corpus",
        "paper_url": "https://aclanthology.org/N19-1202/",
        "paper_authors": [
            "Mattia A. Di Gangi",
            "Roldano Cattoni",
            "Luisa Bentivogli",
            "Matteo Negri",
            "Marco Turchi"
        ],
        "paper_abstract": "Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with a state-of-the-art approach on each language direction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Contextualization of Morphological Inflection",
        "paper_url": "https://aclanthology.org/N19-1203/",
        "paper_authors": [
            "Ekaterina Vylomova",
            "Ryan Cotterell",
            "Trevor Cohn",
            "Timothy Baldwin",
            "Jason Eisner"
        ],
        "paper_abstract": "Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide \u201cgold\u201d tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Robust Abstractive System for Cross-Lingual Summarization",
        "paper_url": "https://aclanthology.org/N19-1204/",
        "paper_authors": [
            "Jessica Ouyang",
            "Boya Song",
            "Kathy McKeown"
        ],
        "paper_abstract": "We present a robust neural abstractive summarization system for cross-lingual summarization. We construct summarization corpora for documents automatically translated from three low-resource languages, Somali, Swahili, and Tagalog, using machine translation and the New York Times summarization corpus. We train three language-specific abstractive summarizers and evaluate on documents originally written in the source languages, as well as on a fourth, unseen language: Arabic. Our systems achieve significantly higher fluency than a standard copy-attention summarizer on automatically translated input documents, as well as comparable content selection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Neural Machine Translation with Neural Syntactic Distance",
        "paper_url": "https://aclanthology.org/N19-1205/",
        "paper_authors": [
            "Chunpeng Ma",
            "Akihiro Tamura",
            "Masao Utiyama",
            "Eiichiro Sumita",
            "Tiejun Zhao"
        ],
        "paper_abstract": "The explicit use of syntactic information has been proved useful for neural machine translation (NMT). However, previous methods resort to either tree-structured neural networks or long linearized sequences, both of which are inefficient. Neural syntactic distance (NSD) enables us to represent a constituent tree using a sequence whose length is identical to the number of words in the sentence. NSD has been used for constituent parsing, but not in machine translation. We propose five strategies to improve NMT with NSD. Experiments show that it is not trivial to improve NMT with NSD; however, the proposed strategies are shown to improve translation performance of the baseline model (+2.1 (En\u2013Ja), +1.3 (Ja\u2013En), +1.2 (En\u2013Ch), and +1.0 (Ch\u2013En) BLEU).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Measuring Immediate Adaptation Performance for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1206/",
        "paper_authors": [
            "Patrick Simianer",
            "Joern Wuebker",
            "John DeNero"
        ],
        "paper_abstract": "Incremental domain adaptation, in which a system learns from the correct output for each input immediately after making its prediction for that input, can dramatically improve system performance for interactive machine translation. Users of interactive systems are sensitive to the speed of adaptation and how often a system repeats mistakes, despite being corrected. Adaptation is most commonly assessed using corpus-level BLEU- or TER-derived metrics that do not explicitly take adaptation speed into account. We find that these metrics often do not capture immediate adaptation effects, such as zero-shot and one-shot learning of domain-specific lexical items. To this end, we propose new metrics that directly evaluate immediate adaptation performance for machine translation. We use these metrics to choose the most suitable adaptation method from a range of different adaptation techniques for neural machine translation systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Differentiable Sampling with Flexible Reference Word Order for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1207/",
        "paper_authors": [
            "Weijia Xu",
            "Xing Niu",
            "Marine Carpuat"
        ],
        "paper_abstract": "Despite some empirical success at correcting exposure bias in machine translation, scheduled sampling algorithms suffer from a major drawback: they incorrectly assume that words in the reference translations and in sampled sequences are aligned at each time step. Our new differentiable sampling algorithm addresses this issue by optimizing the probability that the reference can be aligned with the sampled output, based on a soft alignment predicted by the model itself. As a result, the output distribution at each time step is evaluated with respect to the whole predicted sequence. Experiments on IWSLT translation tasks show that our approach improves BLEU compared to maximum likelihood and scheduled sampling baselines. In addition, our approach is simpler to train with no need for sampling schedule and yields models that achieve larger improvements with smaller beam sizes.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Reinforcement Learning based Curriculum Optimization for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1208/",
        "paper_authors": [
            "Gaurav Kumar",
            "George Foster",
            "Colin Cherry",
            "Maxim Krikun"
        ],
        "paper_abstract": "We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as noise, we seek an optimal curriculum, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes data weighting, filtering, and fine-tuning schemes. Rather than relying on prior knowledge to design a curriculum, we use reinforcement learning to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1209/",
        "paper_authors": [
            "Brian Thompson",
            "Jeremy Gwinnup",
            "Huda Khayrallah",
            "Kevin Duh",
            "Philipp Koehn"
        ],
        "paper_abstract": "Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)\u2014a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Short-Term Meaning Shift: A Distributional Exploration",
        "paper_url": "https://aclanthology.org/N19-1210/",
        "paper_authors": [
            "Marco Del Tredici",
            "Raquel Fern\u00e1ndez",
            "Gemma Boleda"
        ],
        "paper_abstract": "We present the first exploration of meaning shift over short periods of time in online communities using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard model for meaning shift detection on short-term meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting Derogatory Compounds \u2013 An Unsupervised Approach",
        "paper_url": "https://aclanthology.org/N19-1211/",
        "paper_authors": [
            "Michael Wiegand",
            "Maximilian Wolf",
            "Josef Ruppenhofer"
        ],
        "paper_abstract": "We examine the new task of detecting derogatory compounds (e.g. \u201ccurry muncher\u201d). Derogatory compounds are much more difficult to detect than derogatory unigrams (e.g. \u201cidiot\u201d) since they are more sparsely represented in lexical resources previously found effective for this task (e.g. Wiktionary). We propose an unsupervised classification approach that incorporates linguistic properties of compounds. It mostly depends on a simple distributional representation. We compare our approach against previously established methods proposed for extracting derogatory unigrams.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Personalized Neural Embeddings for Collaborative Filtering with Text",
        "paper_url": "https://aclanthology.org/N19-1212/",
        "paper_authors": [
            "Guangneng Hu"
        ],
        "paper_abstract": "Collaborative filtering (CF) is a core technique for recommender systems. Traditional CF approaches exploit user-item relations (e.g., clicks, likes, and views) only and hence they suffer from the data sparsity issue. Items are usually associated with unstructured text such as article abstracts and product reviews. We develop a Personalized Neural Embedding (PNE) framework to exploit both interactions and words seamlessly. We learn such embeddings of users, items, and words jointly, and predict user preferences on items based on these learned representations. PNE estimates the probability that a user will like an item by two terms\u2014behavior factors and semantic factors. On two real-world datasets, PNE shows better performance than four state-of-the-art baselines in terms of three metrics. We also show that PNE learns meaningful word embeddings by visualization.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models",
        "paper_url": "https://aclanthology.org/N19-1213/",
        "paper_authors": [
            "Alexandra Chronopoulou",
            "Christos Baziotis",
            "Alexandros Potamianos"
        ],
        "paper_abstract": "A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Incorporating Emoji Descriptions Improves Tweet Classification",
        "paper_url": "https://aclanthology.org/N19-1214/",
        "paper_authors": [
            "Abhishek Singh",
            "Eduardo Blanco",
            "Wei Jin"
        ],
        "paper_abstract": "Tweets are short messages that often include specialized language such as hashtags and emojis. In this paper, we present a simple strategy to process emojis: replace them with their natural language description and use pretrained word embeddings as normally done with standard words. We show that this strategy is more effective than using pretrained emoji embeddings for tweet classification. Specifically, we obtain new state-of-the-art results in irony detection and sentiment analysis despite our neural network is simpler than previous proposals.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modeling Personal Biases in Language Use by Inducing Personalized Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-1215/",
        "paper_authors": [
            "Daisuke Oba",
            "Naoki Yoshinaga",
            "Shoetsu Sato",
            "Satoshi Akasaki",
            "Masashi Toyoda"
        ],
        "paper_abstract": "There exist biases in individual\u2019s language use; the same word (e.g., cool) is used for expressing different meanings (e.g., temperature range) or different words (e.g., cloudy, hazy) are used for describing the same meaning. In this study, we propose a method of modeling such personal biases in word meanings (hereafter, semantic variations) with personalized word embeddings obtained by solving a task on subjective text while regarding words used by different individuals as different words. To prevent personalized word embeddings from being contaminated by other irrelevant biases, we solve a task of identifying a review-target (objective output) from a given review. To stabilize the training of this extreme multi-class classification, we perform a multi-task learning with metadata identification. Experimental results with reviews retrieved from RateBeer confirmed that the obtained personalized word embeddings improved the accuracy of sentiment analysis as well as the target task. Analysis of the obtained personalized word embeddings revealed trends in semantic variations related to frequent and adjective words.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Task Ordinal Regression for Jointly Predicting the Trustworthiness and the Leading Political Ideology of News Media",
        "paper_url": "https://aclanthology.org/N19-1216/",
        "paper_authors": [
            "Ramy Baly",
            "Georgi Karadzhov",
            "Abdelrhman Saleh",
            "James Glass",
            "Preslav Nakov"
        ],
        "paper_abstract": "In the context of fake news, bias, and propaganda, we study two important but relatively under-explored problems: (i) trustworthiness estimation (on a 3-point scale) and (ii) political ideology detection (left/right bias on a 7-point scale) of entire news outlets, as opposed to evaluating individual articles. In particular, we propose a multi-task ordinal regression framework that models the two problems jointly. This is motivated by the observation that hyper-partisanship is often linked to low trustworthiness, e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and trustworthy. We further use several auxiliary tasks, modeling centrality, hyper-partisanship, as well as left-vs.-right bias on a coarse-grained scale. The evaluation results show sizable performance gains by the joint models over models that target the problems in isolation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Joint Detection and Location of English Puns",
        "paper_url": "https://aclanthology.org/N19-1217/",
        "paper_authors": [
            "Yanyan Zou",
            "Wei Lu"
        ],
        "paper_abstract": "A pun is a form of wordplay for an intended humorous or rhetorical effect, where a word suggests two or more meanings by exploiting polysemy (homographic pun) or phonological similarity to another word (heterographic pun). This paper presents an approach that addresses pun detection and pun location jointly from a sequence labeling perspective. We employ a new tagging scheme such that the model is capable of performing such a joint task, where useful structural information can be properly captured. We show that our proposed model is effective in handling both homographic and heterographic puns. Empirical results on the benchmark datasets demonstrate that our approach can achieve new state-of-the-art results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Harry Potter and the Action Prediction Challenge from Natural Language",
        "paper_url": "https://aclanthology.org/N19-1218/",
        "paper_authors": [
            "David Vilares",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "paper_abstract": "We explore the challenge of action prediction from textual descriptions of scenes, a testbed to approximate whether text inference can be used to predict upcoming actions. As a case of study, we consider the world of the Harry Potter fantasy novels and inferring what spell will be cast next given a fragment of a story. Spells act as keywords that abstract actions (e.g. \u2018Alohomora\u2019 to open a door) and denote a response to the environment. This idea is used to automatically build HPAC, a corpus containing 82,836 samples and 85 actions. We then evaluate different baselines. Among the tested models, an LSTM-based approach obtains the best performance for frequent actions and large scene descriptions, but approaches such as logistic regression behave well on infrequent actions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Argument Mining for Understanding Peer Reviews",
        "paper_url": "https://aclanthology.org/N19-1219/",
        "paper_authors": [
            "Xinyu Hua",
            "Mitko Nikolov",
            "Nikhil Badugu",
            "Lu Wang"
        ],
        "paper_abstract": "Peer-review plays a critical role in the scientific writing and publication ecosystem. To assess the efficiency and efficacy of the reviewing process, one essential element is to understand and evaluate the reviews themselves. In this work, we study the content and structure of peer reviews under the argument mining framework, through automatically detecting (1) the argumentative propositions put forward by reviewers, and (2) their types (e.g., evaluating the work or making suggestions for improvement). We first collect 14.2K reviews from major machine learning and natural language processing venues. 400 reviews are annotated with 10,386 propositions and corresponding types of Evaluation, Request, Fact, Reference, or Quote. We then train state-of-the-art proposition segmentation and classification models on the data to evaluate their utilities and identify new challenges for this new domain, motivating future directions for argument mining. Further experiments show that proposition usage varies across venues in amount, type, and topic.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An annotated dataset of literary entities",
        "paper_url": "https://aclanthology.org/N19-1220/",
        "paper_authors": [
            "David Bamman",
            "Sejal Popat",
            "Sheng Shen"
        ],
        "paper_abstract": "We present a new dataset comprised of 210,532 tokens evenly drawn from 100 different English-language literary texts annotated for ACE entity categories (person, location, geo-political entity, facility, organization, and vehicle). These categories include non-named entities (such as \u201cthe boy\u201d, \u201cthe kitchen\u201d) and nested structure (such as [[the cook]\u2019s sister]). In contrast to existing datasets built primarily on news (focused on geo-political entities and organizations), literary texts offer strikingly different distributions of entity categories, with much stronger emphasis on people and description of settings. We present empirical results demonstrating the performance of nested entity recognition models in this domain; training natively on in-domain literary data yields an improvement of over 20 absolute points in F-score (from 45.7 to 68.3), and mitigates a disparate impact in performance for male and female entities present in models trained on news data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Abusive Language Detection with Graph Convolutional Networks",
        "paper_url": "https://aclanthology.org/N19-1221/",
        "paper_authors": [
            "Pushkar Mishra",
            "Marco Del Tredici",
            "Helen Yannakoudakis",
            "Ekaterina Shutova"
        ],
        "paper_abstract": "Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower\u2013following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On the Importance of Distinguishing Word Meaning Representations: A Case Study on Reverse Dictionary Mapping",
        "paper_url": "https://aclanthology.org/N19-1222/",
        "paper_authors": [
            "Mohammad Taher Pilehvar"
        ],
        "paper_abstract": "Meaning conflation deficiency is one of the main limiting factors of word representations which, given their widespread use at the core of many NLP systems, can lead to inaccurate semantic understanding of the input text and inevitably hamper the performance. Sense representations target this problem. However, their potential impact has rarely been investigated in downstream NLP applications. Through a set of experiments on a state-of-the-art reverse dictionary system based on neural networks, we show that a simple adjustment aimed at addressing the meaning conflation deficiency can lead to substantial improvements.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Factorising AMR generation through syntax",
        "paper_url": "https://aclanthology.org/N19-1223/",
        "paper_authors": [
            "Kris Cao",
            "Stephen Clark"
        ],
        "paper_abstract": "Generating from Abstract Meaning Representation (AMR) is an underspecified problem, as many syntactic decisions are not specified by the semantic graph. To explicitly account for this variation, we break down generating from AMR into two steps: first generate a syntactic structure, and then generate the surface form. We show that decomposing the generation process this way leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Crowdsourced Frame Disambiguation Corpus with Ambiguity",
        "paper_url": "https://aclanthology.org/N19-1224/",
        "paper_authors": [
            "Anca Dumitrache",
            "Lora Aroyo",
            "Chris Welty"
        ],
        "paper_abstract": "We present a resource for the task of FrameNet semantic frame disambiguation of over 5,000 word-sentence pairs from the Wikipedia corpus. The annotations were collected using a novel crowdsourcing approach with multiple workers per sentence to capture inter-annotator disagreement. In contrast to the typical approach of attributing the best single frame to each word, we provide a list of frames with disagreement-based scores that express the confidence with which each frame applies to the word. This is based on the idea that inter-annotator disagreement is at least partly caused by ambiguity that is inherent to the text and frames. We have found many examples where the semantics of individual frames overlap sufficiently to make them acceptable alternatives for interpreting a sentence. We have argued that ignoring this ambiguity creates an overly arbitrary target for training and evaluating natural language processing systems - if humans cannot agree, why would we expect the correct answer from a machine to be any different? To process this data we also utilized an expanded lemma-set provided by the Framester system, which merges FN with WordNet to enhance coverage. Our dataset includes annotations of 1,000 sentence-word pairs whose lemmas are not part of FN. Finally we present metrics for evaluating frame disambiguation systems that account for ambiguity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets",
        "paper_url": "https://aclanthology.org/N19-1225/",
        "paper_authors": [
            "Nelson F. Liu",
            "Roy Schwartz",
            "Noah A. Smith"
        ],
        "paper_abstract": "Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model\u2019s specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI \u201cstress tests\u201d (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization",
        "paper_url": "https://aclanthology.org/N19-1226/",
        "paper_authors": [
            "Dai Quoc Nguyen",
            "Thanh Vu",
            "Tu Dinh Nguyen",
            "Dat Quoc Nguyen",
            "Dinh Phung"
        ],
        "paper_abstract": "In this paper, we introduce an embedding model, named CapsE, exploring a capsule network to model relationship triples (subject, relation, object). Our CapsE represents each triple as a 3-column matrix where each column vector represents the embedding of an element in the triple. This 3-column matrix is then fed to a convolution layer where multiple filters are operated to generate different feature maps. These feature maps are reconstructed into corresponding capsules which are then routed to another capsule to produce a continuous vector. The length of this vector is used to measure the plausibility score of the triple. Our proposed CapsE obtains better performance than previous state-of-the-art embedding models for knowledge graph completion on two benchmark datasets WN18RR and FB15k-237, and outperforms strong search personalization baselines on SEARCH17.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Partial Or Complete, That\u2019s The Question",
        "paper_url": "https://aclanthology.org/N19-1227/",
        "paper_authors": [
            "Qiang Ning",
            "Hangfeng He",
            "Chuchu Fan",
            "Dan Roth"
        ],
        "paper_abstract": "For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that structures consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of supervision, while allowing for more structures to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Sequential Attention with Keyword Mask Model for Community-based Question Answering",
        "paper_url": "https://aclanthology.org/N19-1228/",
        "paper_authors": [
            "Jianxin Yang",
            "Wenge Rong",
            "Libin Shi",
            "Zhang Xiong"
        ],
        "paper_abstract": "In Community-based Question Answering system(CQA), Answer Selection(AS) is a critical task, which focuses on finding a suitable answer within a list of candidate answers. For neural network models, the key issue is how to model the representations of QA text pairs and calculate the interactions between them. We propose a Sequential Attention with Keyword Mask model(SAKM) for CQA to imitate human reading behavior. Question and answer text regard each other as context within keyword-mask attention when encoding the representations, and repeat multiple times(hops) in a sequential style. So the QA pairs capture features and information from both question text and answer text, interacting and improving vector representations iteratively through hops. The flexibility of the model allows to extract meaningful keywords from the sentences and enhance diverse mutual information. We perform on answer selection tasks and multi-level answer ranking tasks. Experiment results demonstrate the superiority of our proposed model on community-based QA datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Simple Attention-Based Representation Learning for Ranking Short Social Media Posts",
        "paper_url": "https://aclanthology.org/N19-1229/",
        "paper_authors": [
            "Peng Shi",
            "Jinfeng Rao",
            "Jimmy Lin"
        ],
        "paper_abstract": "This paper explores the problem of ranking short social media posts with respect to user queries using neural networks. Instead of starting with a complex architecture, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic \u201csoft\u201d matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but are also much faster.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "AttentiveChecker: A Bi-Directional Attention Flow Mechanism for Fact Verification",
        "paper_url": "https://aclanthology.org/N19-1230/",
        "paper_authors": [
            "Santosh Tokala",
            "Vishal G",
            "Avirup Saha",
            "Niloy Ganguly"
        ],
        "paper_abstract": "The recently released FEVER dataset provided benchmark results on a fact-checking task in which given a factual claim, the system must extract textual evidence (sets of sentences from Wikipedia pages) that support or refute the claim. In this paper, we present a completely task-agnostic pipelined system, AttentiveChecker, consisting of three homogeneous Bi-Directional Attention Flow (BIDAF) networks, which are multi-layer hierarchical networks that represent the context at different levels of granularity. We are the first to apply to this task a bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. AttentiveChecker can be used to perform document retrieval, sentence selection, and claim verification. Experiments on the FEVER dataset indicate that AttentiveChecker is able to achieve the state-of-the-art results on the FEVER test set.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Practical, Efficient, and Customizable Active Learning for Named Entity Recognition in the Digital Humanities",
        "paper_url": "https://aclanthology.org/N19-1231/",
        "paper_authors": [
            "Alexander Erdmann",
            "David Joseph Wrisley",
            "Benjamin Allen",
            "Christopher Brown",
            "Sophie Cohen-Bod\u00e9n\u00e8s",
            "Micha Elsner",
            "Yukun Feng",
            "Brian Joseph",
            "B\u00e9atrice Joyeux-Prunel",
            "Marie-Catherine de Marneffe"
        ],
        "paper_abstract": "Scholars in inter-disciplinary fields like the Digital Humanities are increasingly interested in semantic annotation of specialized corpora. Yet, under-resourced languages, imperfect or noisily structured data, and user-specific classification tasks make it difficult to meet their needs using off-the-shelf models. Manual annotation of large corpora from scratch, meanwhile, can be prohibitively expensive. Thus, we propose an active learning solution for named entity recognition, attempting to maximize a custom model\u2019s improvement per additional unit of manual annotation. Our system robustly handles any domain or user-defined label set and requires no external resources, enabling quality named entity recognition for Humanities corpora where such resources are not available. Evaluating on typologically disparate languages and datasets, we reduce required annotation by 20-60% and greatly outperform a competitive active learning baseline.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Doc2hash: Learning Discrete Latent variables for Documents Retrieval",
        "paper_url": "https://aclanthology.org/N19-1232/",
        "paper_authors": [
            "Yifei Zhang",
            "Hao Zhu"
        ],
        "paper_abstract": "Learning to hash via generative model has become a powerful paradigm for fast similarity search in documents retrieval. To get binary representation (i.e., hash codes), the discrete distribution prior (i.e., Bernoulli Distribution) is applied to train the variational autoencoder (VAE). However, the discrete stochastic layer is usually incompatible with the backpropagation in the training stage, and thus causes a gradient flow problem because of non-differentiable operators. The reparameterization trick of sampling from a discrete distribution usually inc non-differentiable operators. In this paper, we propose a method, Doc2hash, that solves the gradient flow problem of the discrete stochastic layer by using continuous relaxation on priors, and trains the generative model in an end-to-end manner to generate hash codes. In qualitative and quantitative experiments, we show the proposed model outperforms other state-of-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating Text GANs as Language Models",
        "paper_url": "https://aclanthology.org/N19-1233/",
        "paper_authors": [
            "Guy Tevet",
            "Gavriel Habib",
            "Vered Shwartz",
            "Jonathan Berant"
        ],
        "paper_abstract": "Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of \u201cexposure bias\u201d. However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than state-of-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Latent Code and Text-based Generative Adversarial Networks for Soft-text Generation",
        "paper_url": "https://aclanthology.org/N19-1234/",
        "paper_authors": [
            "Md. Akmal Haidar",
            "Mehdi Rezagholizadeh",
            "Alan Do Omri",
            "Ahmad Rashid"
        ],
        "paper_abstract": "Text generation with generative adversarial networks (GANs) can be divided into the text-based and code-based categories according to the type of signals used for discrimination. In this work, we introduce a novel text-based approach called Soft-GAN to effectively exploit GAN setup for text generation. We demonstrate how autoencoders (AEs) can be used for providing a continuous representation of sentences, which we will refer to as soft-text. This soft representation will be used in GAN discrimination to synthesize similar soft-texts. We also propose hybrid latent code and text-based GAN (LATEXT-GAN) approaches with one or more discriminators, in which a combination of the latent code and the soft-text is used for GAN discriminations. We perform a number of subjective and objective experiments on two well-known datasets (SNLI and Image COCO) to validate our techniques. We discuss the results using several evaluation metrics and show that the proposed techniques outperform the traditional GAN-based text-generation methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Text Generation from Rich Semantic Representations",
        "paper_url": "https://aclanthology.org/N19-1235/",
        "paper_authors": [
            "Valerie Hajdik",
            "Jan Buys",
            "Michael Wayne Goodman",
            "Emily M. Bender"
        ],
        "paper_abstract": "We propose neural models to generate high-quality text from structured representations based on Minimal Recursion Semantics (MRS). MRS is a rich semantic representation that encodes more precise semantic detail than other representations such as Abstract Meaning Representation (AMR). We show that a sequence-to-sequence model that maps a linearization of Dependency MRS, a graph-based representation of MRS, to text can achieve a BLEU score of 66.11 when trained on gold data. The performance of the model can be improved further using a high-precision, broad coverage grammar-based parser to generate a large silver training corpus, achieving a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation",
        "paper_url": "https://aclanthology.org/N19-1236/",
        "paper_authors": [
            "Amit Moryossef",
            "Yoav Goldberg",
            "Ido Dagan"
        ],
        "paper_abstract": "Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system\u2019s reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating Rewards for Question Generation Models",
        "paper_url": "https://aclanthology.org/N19-1237/",
        "paper_authors": [
            "Tom Hosking",
            "Sebastian Riedel"
        ],
        "paper_abstract": "Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have suggested that optimising for rewards less tightly coupled to the training data might counter this mismatch. We therefore optimise directly for various objectives beyond simply replicating the ground truth questions, including a novel approach using an adversarial discriminator that seeks to generate questions that are indistinguishable from real examples. We confirm that training with policy gradient methods leads to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Text Generation from Knowledge Graphs with Graph Transformers",
        "paper_url": "https://aclanthology.org/N19-1238/",
        "paper_authors": [
            "Rik Koncel-Kedziorski",
            "Dhanush Bekal",
            "Yi Luan",
            "Mirella Lapata",
            "Hannaneh Hajishirzi"
        ],
        "paper_abstract": "Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Open Information Extraction from Question-Answer Pairs",
        "paper_url": "https://aclanthology.org/N19-1239/",
        "paper_authors": [
            "Nikita Bhutani",
            "Yoshihiko Suhara",
            "Wang-Chiew Tan",
            "Alon Halevy",
            "H. V. Jagadish"
        ],
        "paper_abstract": "Open Information Extraction (OpenIE) extracts meaningful structured tuples from free-form text. Most previous work on OpenIE considers extracting data from one sentence at a time. We describe NeurON, a system for extracting tuples from question-answer pairs. One of the main motivations for NeurON is to be able to extend knowledge bases in a way that considers precisely the information that users care about. NeurON addresses several challenges. First, an answer text is often hard to understand without knowing the question, and second, relevant information can span multiple sentences. To address these, NeurON formulates extraction as a multi-source sequence-to-sequence learning task, wherein it combines distributed representations of a question and an answer to generate knowledge facts. We describe experiments on two real-world datasets that demonstrate that NeurON can find a significant number of new and interesting facts to extend a knowledge base compared to state-of-the-art OpenIE methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks",
        "paper_url": "https://aclanthology.org/N19-1240/",
        "paper_authors": [
            "Nicola De Cao",
            "Wilker Aziz",
            "Ivan Titov"
        ],
        "paper_abstract": "Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC",
        "paper_url": "https://aclanthology.org/N19-1241/",
        "paper_authors": [
            "Mark Yatskar"
        ],
        "paper_abstract": "We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers. We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third. Because of the datasets\u2019 structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA. Despite the similarity, models trained on one dataset are ineffective on another dataset, but we find moderate performance improvement through pretraining. To encourage cross-evaluation, we release code for conversion between datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis",
        "paper_url": "https://aclanthology.org/N19-1242/",
        "paper_authors": [
            "Hu Xu",
            "Bing Liu",
            "Lei Shu",
            "Philip Yu"
        ],
        "paper_abstract": "Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions. We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Old is Gold: Linguistic Driven Approach for Entity and Relation Linking of Short Text",
        "paper_url": "https://aclanthology.org/N19-1243/",
        "paper_authors": [
            "Ahmad Sakor",
            "Isaiah Onando Mulang\u2019",
            "Kuldeep Singh",
            "Saeedeh Shekarpour",
            "Maria Esther Vidal",
            "Jens Lehmann",
            "S\u00f6ren Auer"
        ],
        "paper_abstract": "Short texts challenge NLP tasks such as named entity recognition, disambiguation, linking and relation inference because they do not provide sufficient context or are partially malformed (e.g. wrt. capitalization, long tail entities, implicit relations). In this work, we present the Falcon approach which effectively maps entities and relations within a short text to its mentions of a background knowledge graph. Falcon overcomes the challenges of short text using a light-weight linguistic approach relying on a background knowledge graph. Falcon performs joint entity and relation linking of a short text by leveraging several fundamental principles of English morphology (e.g. compounding, headword identification) and utilizes an extended knowledge graph created by merging entities and relations from various knowledge sources. It uses the context of entities for finding relations and does not require training data. Our empirical study using several standard benchmarks and datasets show that Falcon significantly outperforms state-of-the-art entity and relation linking for short text query inventories.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Be Consistent! Improving Procedural Text Comprehension using Label Consistency",
        "paper_url": "https://aclanthology.org/N19-1244/",
        "paper_authors": [
            "Xinya Du",
            "Bhavana Dalvi",
            "Niket Tandon",
            "Antoine Bosselut",
            "Wen-tau Yih",
            "Peter Clark",
            "Claire Cardie"
        ],
        "paper_abstract": "Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
        "paper_url": "https://aclanthology.org/N19-1245/",
        "paper_authors": [
            "Aida Amini",
            "Saadia Gabriel",
            "Shanchuan Lin",
            "Rik Koncel-Kedziorski",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "paper_abstract": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
        "paper_url": "https://aclanthology.org/N19-1246/",
        "paper_authors": [
            "Dheeru Dua",
            "Yizhong Wang",
            "Pradeep Dasigi",
            "Gabriel Stanovsky",
            "Sameer Singh",
            "Matt Gardner"
        ],
        "paper_abstract": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4% F1 on our generalized accuracy metric, while expert human performance is 96%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Encoding Strategy Based Word-Character LSTM for Chinese NER",
        "paper_url": "https://aclanthology.org/N19-1247/",
        "paper_authors": [
            "Wei Liu",
            "Tongge Xu",
            "Qinghua Xu",
            "Jiayu Song",
            "Yueran Zu"
        ],
        "paper_abstract": "A recently proposed lattice model has demonstrated that words in character sequence can provide rich word boundary information for character-based Chinese NER model. In this model, word information is integrated into a shortcut path between the start and the end characters of the word. However, the existence of shortcut path may cause the model to degenerate into a partial word-based model, which will suffer from word segmentation errors. Furthermore, the lattice model can not be trained in batches due to its DAG structure. In this paper, we propose a novel word-character LSTM(WC-LSTM) model to add word information into the start or the end character of the word, alleviating the influence of word segmentation errors while obtaining the word boundary information. Four different strategies are explored in our model to encode word information into a fixed-sized representation for efficient batch training. Experiments on benchmark datasets show that our proposed model outperforms other state-of-the-arts models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Highly Effective Arabic Diacritization using Sequence to Sequence Modeling",
        "paper_url": "https://aclanthology.org/N19-1248/",
        "paper_authors": [
            "Hamdy Mubarak",
            "Ahmed Abdelali",
            "Hassan Sajjad",
            "Younes Samih",
            "Kareem Darwish"
        ],
        "paper_abstract": "Arabic text is typically written without short vowels (or diacritics). However, their presence is required for properly verbalizing Arabic and is hence essential for applications such as text to speech. There are two types of diacritics, namely core-word diacritics and case-endings. Most previous works on automatic Arabic diacritic recovery rely on a large number of manually engineered features, particularly for case-endings. In this work, we present a unified character level sequence-to-sequence deep learning model that recovers both types of diacritics without the use of explicit feature engineering. Specifically, we employ a standard neural machine translation setup on overlapping windows of words (broken down into characters), and then we use voting to select the most likely diacritized form of a word. The proposed model outperforms all previous state-of-the-art systems. Our best settings achieve a word error rate (WER) of 4.49% compared to the state-of-the-art of 12.25% on a standard dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SC-LSTM: Learning Task-Specific Representations in Multi-Task Learning for Sequence Labeling",
        "paper_url": "https://aclanthology.org/N19-1249/",
        "paper_authors": [
            "Peng Lu",
            "Ting Bai",
            "Philippe Langlais"
        ],
        "paper_abstract": "Multi-task learning (MTL) has been studied recently for sequence labeling. Typically, auxiliary tasks are selected specifically in order to improve the performance of a target task. Jointly learning multiple tasks in a way that benefit all of them simultaneously can increase the utility of MTL. In order to do so, we propose a new LSTM cell which contains both shared parameters that can learn from all tasks, and task-specific parameters that can learn task-specific information. We name it a Shared-Cell Long-Short Term Memory SC-LSTM. Experimental results on three sequence labeling benchmarks (named-entity recognition, text chunking, and part-of-speech tagging) demonstrate the effectiveness of our SC-LSTM cell.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Denoise Distantly-Labeled Data for Entity Typing",
        "paper_url": "https://aclanthology.org/N19-1250/",
        "paper_authors": [
            "Yasumasa Onoe",
            "Greg Durrett"
        ],
        "paper_abstract": "Distantly-labeled data can be used to scale up training of statistical models, but it is typically noisy and that noise can vary with the distant labeling technique. In this work, we propose a two-stage procedure for handling this type of data: denoise it with a learned model, then train our final model on clean and denoised distant data with standard supervised training. Our denoising approach consists of two parts. First, a filtering function discards examples from the distantly labeled data that are wholly unusable. Second, a relabeling function repairs noisy labels for the retained examples. Each of these components is a model trained on synthetically-noised examples generated from a small manually-labeled set. We investigate this approach on the ultra-fine entity typing task of Choi et al. (2018). Our baseline model is an extension of their model with pre-trained ELMo representations, which already achieves state-of-the-art performance. Adding distant data that has been denoised with our learned models gives further performance gains over this base model, outperforming models trained on raw distant data or heuristically-denoised distant data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Simple and Robust Approach to Detecting Subject-Verb Agreement Errors",
        "paper_url": "https://aclanthology.org/N19-1251/",
        "paper_authors": [
            "Simon Flachs",
            "Oph\u00e9lie Lacroix",
            "Marek Rei",
            "Helen Yannakoudakis",
            "Anders S\u00f8gaard"
        ],
        "paper_abstract": "While rule-based detection of subject-verb agreement (SVA) errors is sensitive to syntactic parsing errors and irregularities and exceptions to the main rules, neural sequential labelers have a tendency to overfit their training data. We observe that rule-based error generation is less sensitive to syntactic parsing errors and irregularities than error detection and explore a simple, yet efficient approach to getting the best of both worlds: We train neural sequential labelers on the combination of large volumes of silver standard data, obtained through rule-based error generation, and gold standard data. We show that our simple protocol leads to more robust detection of SVA errors on both in-domain and out-of-domain data, as well as in the context of other errors and long-distance dependencies; and across four standard benchmarks, the induced model on average achieves a new state of the art.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages",
        "paper_url": "https://aclanthology.org/N19-1252/",
        "paper_authors": [
            "Ronald Cardenas",
            "Ying Lin",
            "Heng Ji",
            "Jonathan May"
        ],
        "paper_abstract": "Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a \u2018ciphertext\u2019 and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagger\u2019s utility by incorporating it into a true \u2018zero-resource\u2019 variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our tagger makes up much of the accuracy lost when gold POS tags are unavailable.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing",
        "paper_url": "https://aclanthology.org/N19-1253/",
        "paper_authors": [
            "Wasi Ahmad",
            "Zhisong Zhang",
            "Xuezhe Ma",
            "Eduard Hovy",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "paper_abstract": "Different languages might have different word orders. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages. Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling word order. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations",
        "paper_url": "https://aclanthology.org/N19-1254/",
        "paper_authors": [
            "Mingda Chen",
            "Qingming Tang",
            "Sam Wiseman",
            "Kevin Gimpel"
        ],
        "paper_abstract": "We propose a generative model for a sentence that uses two latent variables, with one intended to represent the syntax of the sentence and the other to represent its semantics. We show we can achieve better disentanglement between semantic and syntactic representations by training with multiple losses, including losses that exploit aligned paraphrastic sentences and word-order information. We evaluate our models on standard semantic similarity tasks and novel syntactic similarity tasks. Empirically, we find that the model with the best performing syntactic and semantic representations also gives rise to the most disentangled representations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Self-Discriminative Learning for Unsupervised Document Embedding",
        "paper_url": "https://aclanthology.org/N19-1255/",
        "paper_authors": [
            "Hong-You Chen",
            "Chin-Hua Hu",
            "Leila Wehbe",
            "Shou-De Lin"
        ],
        "paper_abstract": "Unsupervised document representation learning is an important task providing pre-trained features for NLP applications. Unlike most previous work which learn the embedding based on self-prediction of the surface of text, we explicitly exploit the inter-document information and directly model the relations of documents in embedding space with a discriminative network and a novel objective. Extensive experiments on both small and large public datasets show the competitiveness of the proposed method. In evaluations on standard document classification, our model has errors that are 5 to 13% lower than state-of-the-art unsupervised embedding models. The reduction in error is even more pronounced in scarce label setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adaptive Convolution for Text Classification",
        "paper_url": "https://aclanthology.org/N19-1256/",
        "paper_authors": [
            "Byung-Ju Choi",
            "Jun-Hyung Park",
            "SangKeun Lee"
        ],
        "paper_abstract": "In this paper, we present an adaptive convolution for text classification to give flexibility to convolutional neural networks (CNNs). Unlike traditional convolutions which utilize the same set of filters regardless of different inputs, the adaptive convolution employs adaptively generated convolutional filters conditioned on inputs. We achieve this by attaching filter-generating networks, which are carefully designed to generate input-specific filters, to convolution blocks in existing CNNs. We show the efficacy of our approach in existing CNNs based on the performance evaluation. Our evaluation indicates that all of our baselines achieve performance improvements with adaptive convolutions as much as up to 2.6 percentage point in seven benchmark text classification datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Zero-Shot Cross-Lingual Opinion Target Extraction",
        "paper_url": "https://aclanthology.org/N19-1257/",
        "paper_authors": [
            "Soufian Jebbara",
            "Philipp Cimiano"
        ],
        "paper_abstract": "Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of these corpora is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction. Our experiments with 5 languages give promising results: We can successfully train a model on annotated data of a source language and perform accurate prediction on a target language without ever using any annotated samples in that target language. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77% of a model trained on target language data. Furthermore, we can increase this performance up to 87% of a baseline model trained on target language data by performing cross-lingual learning from multiple source languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adversarial Category Alignment Network for Cross-domain Sentiment Classification",
        "paper_url": "https://aclanthology.org/N19-1258/",
        "paper_authors": [
            "Xiaoye Qu",
            "Zhikang Zou",
            "Yu Cheng",
            "Yang Yang",
            "Pan Zhou"
        ],
        "paper_abstract": "Cross-domain sentiment classification aims to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. Most existing adversarial learning methods focus on aligning the global marginal distribution by fooling a domain discriminator, without taking category-specific decision boundaries into consideration, which can lead to the mismatch of category-level features. In this work, we propose an adversarial category alignment network (ACAN), which attempts to enhance category consistency between the source domain and the target domain. Specifically, we increase the discrepancy of two polarity classifiers to provide diverse views, locating ambiguous features near the decision boundaries. Then the generator learns to create better features away from the category boundaries by minimizing this discrepancy. Experimental results on benchmark datasets show that the proposed method can achieve state-of-the-art performance and produce more discriminative features.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling",
        "paper_url": "https://aclanthology.org/N19-1259/",
        "paper_authors": [
            "Zhifang Fan",
            "Zhen Wu",
            "Xin-Yu Dai",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "paper_abstract": "Opinion target extraction and opinion words extraction are two fundamental subtasks in Aspect Based Sentiment Analysis (ABSA). Recently, many methods have made progress on these two tasks. However, few works aim at extracting opinion targets and opinion words as pairs. In this paper, we propose a novel sequence labeling subtask for ABSA named TOWE (Target-oriented Opinion Words Extraction), which aims at extracting the corresponding opinion words for a given opinion target. A target-fused sequence labeling neural network model is designed to perform this task. The opinion target information is well encoded into context by an Inward-Outward LSTM. Then left and right contexts of the opinion target and the global context are combined to find the corresponding opinion words. We build four datasets for TOWE based on several popular ABSA benchmarks from laptop and restaurant reviews. The experimental results show that our proposed model outperforms the other compared methods significantly. We believe that our work may not only be helpful for downstream sentiment analysis task, but can also be used for pair-wise opinion summarization.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Abstractive Summarization of Reddit Posts with Multi-level Memory Networks",
        "paper_url": "https://aclanthology.org/N19-1260/",
        "paper_authors": [
            "Byeongchang Kim",
            "Hyunwoo Kim",
            "Gunhee Kim"
        ],
        "paper_abstract": "We address the problem of abstractive summarization in two directions: proposing a novel dataset and a new model. First, we collect Reddit TIFU dataset, consisting of 120K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing datasets that mostly use formal documents as source such as news articles. Thus, our dataset could less suffer from some biases that key sentences usually located at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks (MMN), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the Reddit TIFU dataset is highly abstractive and the MMN outperforms the state-of-the-art summarization models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Automatic learner summary assessment for reading comprehension",
        "paper_url": "https://aclanthology.org/N19-1261/",
        "paper_authors": [
            "Menglin Xia",
            "Ekaterina Kochmar",
            "Ted Briscoe"
        ],
        "paper_abstract": "Automating the assessment of learner summary provides a useful tool for assessing learner reading comprehension. We present a summarization task for evaluating non-native reading comprehension and propose three novel approaches to automatically assess the learner summaries. We evaluate our models on two datasets we created and show that our models outperform traditional approaches that rely on exact word match on this task. Our best model produces quality assessments close to professional examiners.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Data-efficient Neural Text Compression with Interactive Learning",
        "paper_url": "https://aclanthology.org/N19-1262/",
        "paper_authors": [
            "Avinesh P.V.S",
            "Christian M. Meyer"
        ],
        "paper_abstract": "Neural sequence-to-sequence models have been successfully applied to text compression. However, these models were trained on huge automatically induced parallel corpora, which are only available for a few domains and tasks. In this paper, we propose a novel interactive setup to neural text compression that enables transferring a model to new domains and compression tasks with minimal human supervision. This is achieved by employing active learning, which intelligently samples from a large pool of unlabeled data. Using this setup, we can successfully adapt a model trained on small data of 40k samples for a headline generation task to a general text compression dataset at an acceptable compression quality with just 500 sampled instances annotated by a human.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Text Generation with Exemplar-based Adaptive Decoding",
        "paper_url": "https://aclanthology.org/N19-1263/",
        "paper_authors": [
            "Hao Peng",
            "Ankur Parikh",
            "Manaal Faruqui",
            "Bhuwan Dhingra",
            "Dipanjan Das"
        ],
        "paper_abstract": "We propose a novel conditioned text generation model. It draws inspiration from traditional template-based text generation techniques, where the source provides the content (i.e., what to say), and the template influences how to say it. Building on the successful encoder-decoder paradigm, it first encodes the content representation from the given input text; to produce the output, it retrieves exemplar text from the training data as \u201csoft templates,\u201d which are then used to construct an exemplar-specific decoder. We evaluate the proposed model on abstractive text summarization and data-to-text generation. Empirical results show that this model achieves strong performance and outperforms comparable baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Guiding Extractive Summarization with Question-Answering Rewards",
        "paper_url": "https://aclanthology.org/N19-1264/",
        "paper_authors": [
            "Kristjan Arumae",
            "Fei Liu"
        ],
        "paper_abstract": "Highlighting while reading is a natural behavior for people to track salient content of a document. It would be desirable to teach an extractive summarizer to do the same. However, a major obstacle to the development of a supervised summarizer is the lack of ground-truth. Manual annotation of extraction units is cost-prohibitive, whereas acquiring labels by automatically aligning human abstracts and source documents can yield inferior results. In this paper we describe a novel framework to guide a supervised, extractive summarization system with question-answering rewards. We argue that quality summaries should serve as document surrogates to answer important questions, and such question-answer pairs can be conveniently obtained from human abstracts. The system learns to promote summaries that are informative, fluent, and perform competitively on question-answering. Our results compare favorably with those reported by strong summarization baselines as evaluated by automatic metrics and human assessors.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Beyond task success: A closer look at jointly learning to see, ask, and GuessWhat",
        "paper_url": "https://aclanthology.org/N19-1265/",
        "paper_authors": [
            "Ravi Shekhar",
            "Aashish Venkatesh",
            "Tim Baumg\u00e4rtner",
            "Elia Bruni",
            "Barbara Plank",
            "Raffaella Bernardi",
            "Raquel Fern\u00e1ndez"
        ],
        "paper_abstract": "We propose a grounded dialogue state encoder which addresses a foundational issue on how to integrate visual grounding with dialogue system components. As a test-bed, we focus on the GuessWhat?! game, a two-player game where the goal is to identify an object in a complex visual scene by asking a sequence of yes/no questions. Our visually-grounded encoder leverages synergies between guessing and asking questions, as it is trained jointly using multi-task learning. We further enrich our model via a cooperative learning regime. We show that the introduction of both the joint architecture and cooperative learning lead to accuracy improvements over the baseline system. We compare our approach to an alternative system which extends the baseline with reinforcement learning. Our in-depth analysis shows that the linguistic skills of the two models differ dramatically, despite approaching comparable performance levels. This points at the importance of analyzing the linguistic output of competing systems beyond numeric comparison solely based on task success.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The World in My Mind: Visual Dialog with Adversarial Multi-modal Feature Encoding",
        "paper_url": "https://aclanthology.org/N19-1266/",
        "paper_authors": [
            "Yiqun Yao",
            "Jiaming Xu",
            "Bo Xu"
        ],
        "paper_abstract": "Visual Dialog is a multi-modal task that requires a model to participate in a multi-turn human dialog grounded on an image, and generate correct, human-like responses. In this paper, we propose a novel Adversarial Multi-modal Feature Encoding (AMFE) framework for effective and robust auxiliary training of visual dialog systems. AMFE can force the language-encoding part of a model to generate hidden states in a distribution closely related to the distribution of real-world images, resulting in language features containing general knowledge from both modalities by nature, which can help generate both more correct and more general responses with reasonably low time cost. Experimental results show that AMFE can steadily bring performance gains to different models on different scales of data. Our method outperforms both the supervised learning baselines and other fine-tuning methods, achieving state-of-the-art results on most metrics of VisDial v0.5/v0.9 generative tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Strong and Simple Baselines for Multimodal Utterance Embeddings",
        "paper_url": "https://aclanthology.org/N19-1267/",
        "paper_authors": [
            "Paul Pu Liang",
            "Yao Chong Lim",
            "Yao-Hung Hubert Tsai",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency"
        ],
        "paper_abstract": "Human language is a rich multimodal signal consisting of spoken words, facial expressions, body gestures, and vocal intonations. Learning representations for these spoken utterances is a complex research problem due to the presence of multiple heterogeneous sources of information. Recent advances in multimodal learning have followed the general trend of building more complex models that utilize various attention, memory and recurrent components. In this paper, we propose two simple but strong baselines to learn embeddings of multimodal utterances. The first baseline assumes a conditional factorization of the utterance into unimodal factors. Each unimodal factor is modeled using the simple form of a likelihood function obtained via a linear transformation of the embedding. We show that the optimal embedding can be derived in closed form by taking a weighted average of the unimodal features. In order to capture richer representations, our second baseline extends the first by factorizing into unimodal, bimodal, and trimodal factors, while retaining simplicity and efficiency during learning and inference. From a set of experiments across two tasks, we show strong performance on both supervised and semi-supervised multimodal prediction, as well as significant (10 times) speedups over neural models during inference. Overall, we believe that our strong baseline models offer new benchmarking options for future research in multimodal learning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout",
        "paper_url": "https://aclanthology.org/N19-1268/",
        "paper_authors": [
            "Hao Tan",
            "Licheng Yu",
            "Mohit Bansal"
        ],
        "paper_abstract": "A grand goal in AI is to build a robot that can accurately navigate based on natural language instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environment. One key challenge here is to learn to navigate in new environments that are unseen during training. Most of the existing approaches perform dramatically worse in unseen environments as compared to seen ones. In this paper, we present a generalizable navigational agent. Our agent is trained in two stages. The first stage is training via mixed imitation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization. The second stage is fine-tuning via newly-introduced \u2018unseen\u2019 triplets (environment, path, instruction). To generate these unseen triplets, we propose a simple but effective \u2018environmental dropout\u2019 method to mimic unseen environments, which overcomes the problem of limited seen environment variability. Next, we apply semi-supervised learning (via back-translation) on these dropout environments to generate new paths and instructions. Empirically, we show that our agent is substantially better at generalizability when fine-tuned with these triplets, outperforming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards Content Transfer through Grounded Text Generation",
        "paper_url": "https://aclanthology.org/N19-1269/",
        "paper_authors": [
            "Shrimai Prabhumoye",
            "Chris Quirk",
            "Michel Galley"
        ],
        "paper_abstract": "Recent work in neural generation has attracted significant interest in controlling the form of text, such as style, persona, and politeness. However, there has been less work on controlling neural text generation for content. This paper introduces the notion of Content Transfer for long-form text generation, where the task is to generate a next sentence in a document that both fits its context and is grounded in a content-rich external textual source such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Machine Reading Comprehension with General Reading Strategies",
        "paper_url": "https://aclanthology.org/N19-1270/",
        "paper_authors": [
            "Kai Sun",
            "Dian Yu",
            "Dong Yu",
            "Claire Cardie"
        ],
        "paper_abstract": "Reading strategies have been shown to improve comprehension levels, especially for readers lacking adequate prior knowledge. Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a deep language model via pre-training. Inspired by reading strategies identified in cognitive science, and given limited computational resources - just a pre-trained model and a fixed number of training instances - we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice questions and candidate answers directly from the text in an unsupervised manner. By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a 5.8% absolute increase in accuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies. We further fine-tune the resulting model on a target MRC task, leading to an absolute improvement of 6.2% in average accuracy over previous state-of-the-art approaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of our fine-tuned models that incorporate these strategies. Core code is available at https://github.com/nlpdata/strategy/.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension",
        "paper_url": "https://aclanthology.org/N19-1271/",
        "paper_authors": [
            "Yichong Xu",
            "Xiaodong Liu",
            "Yelong Shen",
            "Jingjing Liu",
            "Jianfeng Gao"
        ],
        "paper_abstract": "We propose a multi-task learning framework to learn a joint Machine Reading Comprehension (MRC) model that can be applied to a wide range of MRC tasks in different domains. Inspired by recent ideas of data selection in machine translation, we develop a novel sample re-weighting scheme to assign sample-specific weights to the loss. Empirical study shows that our approach can be applied to many existing MRC models. Combined with contextual representations from pre-trained language models (such as ELMo), we achieve new state-of-the-art results on a set of MRC benchmark datasets. We release our code at https://github.com/xycforgithub/MultiTask-MRC.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "paper_url": "https://aclanthology.org/N19-1272/",
        "paper_authors": [
            "Ting-Rui Chiang",
            "Yun-Nung Chen"
        ],
        "paper_abstract": "Solving math word problems is a challenging task that requires accurate natural language understanding to bridge natural language texts and math expressions. Motivated by the intuition about how human generates the equations given the problem texts, this paper presents a neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in texts. This paper views the process of generating equation as a bridge between the semantic world and the symbolic world, where the proposed neural math solver is based on an encoder-decoder framework. In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next. The preliminary experiments are conducted in a dataset Math23K, and our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10% accuracy, demonstrating the effectiveness of bridging the symbolic and semantic worlds from math word problems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Iterative Search for Weakly Supervised Semantic Parsing",
        "paper_url": "https://aclanthology.org/N19-1273/",
        "paper_authors": [
            "Pradeep Dasigi",
            "Matt Gardner",
            "Shikhar Murty",
            "Luke Zettlemoyer",
            "Eduard Hovy"
        ],
        "paper_abstract": "Training semantic parsers from question-answer pairs typically involves searching over an exponentially large space of logical forms, and an unguided search can easily be misled by spurious logical forms that coincidentally evaluate to the correct answer. We propose a novel iterative training algorithm that alternates between searching for consistent logical forms and maximizing the marginal likelihood of the retrieved ones. This training scheme lets us iteratively train models that provide guidance to subsequent ones to search for logical forms of increasing complexity, thus dealing with the problem of spuriousness. We evaluate these techniques on two hard datasets: WikiTableQuestions (WTQ) and Cornell Natural Language Visual Reasoning (NLVR), and show that our training algorithm outperforms the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Alignment over Heterogeneous Embeddings for Question Answering",
        "paper_url": "https://aclanthology.org/N19-1274/",
        "paper_authors": [
            "Vikas Yadav",
            "Steven Bethard",
            "Mihai Surdeanu"
        ],
        "paper_abstract": "We propose a simple, fast, and mostly-unsupervised approach for non-factoid question answering (QA) called Alignment over Heterogeneous Embeddings (AHE). AHE simply aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency of the corresponding question/answer term. AHE\u2019s similarity function operates over embeddings that model the underlying text at different levels of abstraction: character (FLAIR), word (BERT and GloVe), and sentence (InferSent), where the latter is the only supervised component in the proposed approach. Despite its simplicity and lack of supervision, AHE obtains a new state-of-the-art performance on the \u201cEasy\u201d partition of the AI2 Reasoning Challenge (ARC) dataset (64.6% accuracy), top-two performance on the \u201cChallenge\u201d partition of ARC (34.1%), and top-three performance on the WikiQA dataset (74.08% MRR), outperforming many other complex, supervised approaches. Our error analysis indicates that alignments over character, word, and sentence embeddings capture substantially different semantic information. We exploit this with a simple meta-classifier that learns how much to trust the predictions over each representation, which further improves the performance of unsupervised AHE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bridging the Gap: Attending to Discontinuity in Identification of Multiword Expressions",
        "paper_url": "https://aclanthology.org/N19-1275/",
        "paper_authors": [
            "Omid Rohanian",
            "Shiva Taslimipoor",
            "Samaneh Kouchaki",
            "Le An Ha",
            "Ruslan Mitkov"
        ],
        "paper_abstract": "We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We finally propose a combined model that integrates complementary information from both, through a gating mechanism. The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Incorporating Word Attention into Character-Based Word Segmentation",
        "paper_url": "https://aclanthology.org/N19-1276/",
        "paper_authors": [
            "Shohei Higashiyama",
            "Masao Utiyama",
            "Eiichiro Sumita",
            "Masao Ideuchi",
            "Yoshiaki Oida",
            "Yohei Sakamoto",
            "Isaac Okada"
        ],
        "paper_abstract": "Neural network models have been actively applied to word segmentation, especially Chinese, because of the ability to minimize the effort in feature engineering. Typical segmentation models are categorized as character-based, for conducting exact inference, or word-based, for utilizing word-level information. We propose a character-based model utilizing word information to leverage the advantages of both types of models. Our model learns the importance of multiple candidate words for a character on the basis of an attention mechanism, and makes use of it for segmentation decisions. The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "VCWE: Visual Character-Enhanced Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-1277/",
        "paper_authors": [
            "Chi Sun",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "paper_abstract": "Chinese is a logographic writing system, and the shape of Chinese characters contain rich syntactic and semantic information. In this paper, we propose a model to learn Chinese word embeddings via three-level composition: (1) a convolutional neural network to extract the intra-character compositionality from the visual shape of a character; (2) a recurrent neural network with self-attention to compose character representation into word embeddings; (3) the Skip-Gram framework to capture non-compositionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks: word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Subword Encoding in Lattice LSTM for Chinese Word Segmentation",
        "paper_url": "https://aclanthology.org/N19-1278/",
        "paper_authors": [
            "Jie Yang",
            "Yue Zhang",
            "Shuailong Liang"
        ],
        "paper_abstract": "We investigate subword information for Chinese word segmentation, by integrating sub word embeddings trained using byte-pair encoding into a Lattice LSTM (LaLSTM) network over a character sequence. Experiments on standard benchmark show that subword information brings significant gains over strong character-based segmentation models. To our knowledge, this is the first research on the effectiveness of subwords on neural word segmentation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Cross-Domain Chinese Word Segmentation with Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-1279/",
        "paper_authors": [
            "Yuxiao Ye",
            "Weikang Li",
            "Yue Zhang",
            "Likun Qiu",
            "Jian Sun"
        ],
        "paper_abstract": "Cross-domain Chinese Word Segmentation (CWS) remains a challenge despite recent progress in neural-based CWS. The limited amount of annotated data in the target domain has been the key obstacle to a satisfactory performance. In this paper, we propose a semi-supervised word-based approach to improving cross-domain CWS given a baseline segmenter. Particularly, our model only deploys word embeddings trained on raw text in the target domain, discarding complex hand-crafted features and domain-specific dictionaries. Innovative subsampling and negative sampling methods are proposed to derive word embeddings optimized for CWS. We conduct experiments on five datasets in special domains, covering domains in novels, medicine, and patent. Results show that our model can obviously improve cross-domain CWS, especially in the segmentation of domain-specific noun entities. The word F-measure increases by over 3.0% on four datasets, outperforming state-of-the-art semi-supervised and unsupervised cross-domain CWS approaches with a large margin. We make our data and code available on Github.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Semi-Markov Conditional Random Fields for Robust Character-Based Part-of-Speech Tagging",
        "paper_url": "https://aclanthology.org/N19-1280/",
        "paper_authors": [
            "Apostolos Kemos",
            "Heike Adel",
            "Hinrich Sch\u00fctze"
        ],
        "paper_abstract": "Character-level models of tokens have been shown to be effective at dealing with within-token noise and out-of-vocabulary words. However, they often still rely on correct token boundaries. In this paper, we propose to eliminate the need for tokenizers with an end-to-end character-level semi-Markov conditional random field. It uses neural networks for its character and segment representations. We demonstrate its effectiveness in multilingual settings and when token boundaries are noisy: It matches state-of-the-art part-of-speech taggers for various languages and significantly outperforms them on a noisy English version of a benchmark dataset. Our code and the noisy dataset are publicly available at http://cistern.cis.lmu.de/semiCRF.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Shrinking Japanese Morphological Analyzers With Neural Networks and Semi-supervised Learning",
        "paper_url": "https://aclanthology.org/N19-1281/",
        "paper_authors": [
            "Arseny Tolmachev",
            "Daisuke Kawahara",
            "Sadao Kurohashi"
        ],
        "paper_abstract": "For languages without natural word boundaries, like Japanese and Chinese, word segmentation is a prerequisite for downstream analysis. For Japanese, segmentation is often done jointly with part of speech tagging, and this process is usually referred to as morphological analysis. Morphological analyzers are trained on data hand-annotated with segmentation boundaries and part of speech tags. A segmentation dictionary or character n-gram information is also provided as additional inputs to the model. Incorporating this extra information makes models large. Modern neural morphological analyzers can consume gigabytes of memory. We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations. The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network, and independently infers both segmentation and part of speech information. The model is trained in an end-to-end and semi-supervised fashion, on labels produced by a state-of-the-art analyzer. We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data. Our model itself is significantly smaller than the dictionary-based one: it uses less than 15 megabytes of space.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Constituency Parsing of Speech Transcripts",
        "paper_url": "https://aclanthology.org/N19-1282/",
        "paper_authors": [
            "Paria Jamshid Lou",
            "Yufei Wang",
            "Mark Johnson"
        ],
        "paper_abstract": "This paper studies the performance of a neural self-attentive parser on transcribed speech. Speech presents parsing challenges that do not appear in written text, such as the lack of punctuation and the presence of speech disfluencies (including filled pauses, repetitions, corrections, etc.). Disfluencies are especially problematic for conventional syntactic parsers, which typically fail to find any EDITED disfluency nodes at all. This motivated the development of special disfluency detection systems, and special mechanisms added to parsers specifically to handle disfluencies. However, we show here that neural parsers can find EDITED disfluency nodes, and the best neural parsers find them with an accuracy surpassing that of specialized disfluency detection systems, thus making these specialized mechanisms unnecessary. This paper also investigates a modified loss function that puts more weight on EDITED nodes. It also describes tree-transformations that simplify the disfluency detection task by providing alternative encodings of disfluencies and syntactic information.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Acoustic-to-Word Models with Conversational Context Information",
        "paper_url": "https://aclanthology.org/N19-1283/",
        "paper_authors": [
            "Suyoun Kim",
            "Florian Metze"
        ],
        "paper_abstract": "Conversational context information, higher-level knowledge that spans across sentences, can help to recognize a long conversation. However, existing speech recognition models are typically built at a sentence level, and thus it may not capture important conversational context information. The recent progress in end-to-end speech recognition enables integrating context with other available information (e.g., acoustic, linguistic resources) and directly recognizing words from speech. In this work, we present a direct acoustic-to-word, end-to-end speech recognition model capable of utilizing the conversational context to better process long conversations. We evaluate our proposed approach on the Switchboard conversational speech corpus and show that our system outperforms a standard end-to-end speech recognition system.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Dynamic Speaker Model for Conversational Interactions",
        "paper_url": "https://aclanthology.org/N19-1284/",
        "paper_authors": [
            "Hao Cheng",
            "Hao Fang",
            "Mari Ostendorf"
        ],
        "paper_abstract": "Individual differences in speakers are reflected in their language use as well as in their interests and opinions. Characterizing these differences can be useful in human-computer interaction, as well as analysis of human-human conversations. In this work, we introduce a neural model for learning a dynamically updated speaker embedding in a conversational context. Initial model training is unsupervised, using context-sensitive language generation as an objective, with the context being the conversation history. Further fine-tuning can leverage task-dependent supervised training. The learned neural representation of speakers is shown to be useful for content ranking in a socialbot and dialog act prediction in human-human conversations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fluent Translations from Disfluent Speech in End-to-End Speech Translation",
        "paper_url": "https://aclanthology.org/N19-1285/",
        "paper_authors": [
            "Elizabeth Salesky",
            "Matthias Sperber",
            "Alexander Waibel"
        ],
        "paper_abstract": "Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected \u2018copy-edited\u2019 references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, implicitly removing disfluencies in end-to-end translation of conversational speech.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Relation Classification Using Segment-Level Attention-based CNN and Dependency-based RNN",
        "paper_url": "https://aclanthology.org/N19-1286/",
        "paper_authors": [
            "Van-Hien Tran",
            "Van-Thuy Phi",
            "Hiroyuki Shindo",
            "Yuji Matsumoto"
        ],
        "paper_abstract": "Recently, relation classification has gained much success by exploiting deep neural networks. In this paper, we propose a new model effectively combining Segment-level Attention-based Convolutional Neural Networks (SACNNs) and Dependency-based Recurrent Neural Networks (DepRNNs). While SACNNs allow the model to selectively focus on the important information segment from the raw sequence, DepRNNs help to handle the long-distance relations from the shortest dependency path of relation entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the state-of-the-art without using any external lexical features.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Document-Level Event Factuality Identification via Adversarial Neural Network",
        "paper_url": "https://aclanthology.org/N19-1287/",
        "paper_authors": [
            "Zhong Qian",
            "Peifeng Li",
            "Qiaoming Zhu",
            "Guodong Zhou"
        ],
        "paper_abstract": "Document-level event factuality identification is an important subtask in event factuality and is crucial for discourse understanding in Natural Language Processing (NLP). Previous studies mainly suffer from the scarcity of suitable corpus and effective methods. To solve these two issues, we first construct a corpus annotated with both document- and sentence-level event factuality information on both English and Chinese texts. Then we present an LSTM neural network based on adversarial training with both intra- and inter-sequence attentions to identify document-level event factuality. Experimental results show that our neural network model can outperform various baselines on the constructed corpus.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions",
        "paper_url": "https://aclanthology.org/N19-1288/",
        "paper_authors": [
            "Zhi-Xiu Ye",
            "Zhen-Hua Ling"
        ],
        "paper_abstract": "This paper presents a neural relation extraction method to deal with the noisy training data generated by distant supervision. Previous studies mainly focus on sentence-level de-noising by designing neural networks with intra-bag attentions. In this paper, both intra-bag and inter-bag attentions are considered in order to deal with the noise at sentence-level and bag-level respectively. First, relation-aware bag representations are calculated by weighting sentence embeddings using intra-bag attentions. Here, each possible relation is utilized as the query for attention calculation instead of only using the target relation in conventional methods. Furthermore, the representation of a group of bags in the training set which share the same relation label is calculated by weighting bag representations using a similarity-based inter-bag attention module. Finally, a bag group is utilized as a training sample when building our relation extractor. Experimental results on the New York Times dataset demonstrate the effectiveness of our proposed intra-bag and inter-bag attention modules. Our method also achieves better relation extraction accuracy than state-of-the-art methods on this dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Ranking-Based Autoencoder for Extreme Multi-label Classification",
        "paper_url": "https://aclanthology.org/N19-1289/",
        "paper_authors": [
            "Bingyu Wang",
            "Li Chen",
            "Wei Sun",
            "Kechen Qin",
            "Kefeng Li",
            "Hui Zhou"
        ],
        "paper_abstract": "Extreme Multi-label classification (XML) is an important yet challenging machine learning task, that assigns to each instance its most relevant candidate labels from an extremely large label collection, where the numbers of labels, features and instances could be thousands or millions. XML is more and more on demand in the Internet industries, accompanied with the increasing business scale / scope and data accumulation. The extremely large label collections yield challenges such as computational complexity, inter-label dependency and noisy labeling. Many methods have been proposed to tackle these challenges, based on different mathematical formulations. In this paper, we propose a deep learning XML method, with a word-vector-based self-attention, followed by a ranking-based AutoEncoder architecture. The proposed method has three major advantages: 1) the autoencoder simultaneously considers the inter-label dependencies and the feature-label dependencies, by projecting labels and features onto a common embedding space; 2) the ranking loss not only improves the training efficiency and accuracy but also can be extended to handle noisy labeled data; 3) the efficient attention mechanism improves feature representation by highlighting feature importance. Experimental results on benchmark datasets show the proposed method is competitive to state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Posterior-regularized REINFORCE for Instance Selection in Distant Supervision",
        "paper_url": "https://aclanthology.org/N19-1290/",
        "paper_authors": [
            "Qi Zhang",
            "Siliang Tang",
            "Xiang Ren",
            "Fei Wu",
            "Shiliang Pu",
            "Yueting Zhuang"
        ],
        "paper_abstract": "This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply it to the task of instance selection in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However unbiased methods, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Scalable Collapsed Inference for High-Dimensional Topic Models",
        "paper_url": "https://aclanthology.org/N19-1291/",
        "paper_authors": [
            "Rashidul Islam",
            "James Foulds"
        ],
        "paper_abstract": "The bigger the corpus, the more topics it can potentially support. To truly make full use of massive text corpora, a topic model inference algorithm must therefore scale efficiently in 1) documents and 2) topics, while 3) achieving accurate inference. Previous methods have achieved two out of three of these criteria simultaneously, but never all three at once. In this paper, we develop an online inference algorithm for topic models which leverages stochasticity to scale well in the number of documents, sparsity to scale well in the number of topics, and which operates in the collapsed representation of the topic model for improved accuracy and run-time performance. We use a Monte Carlo inner loop in the online setting to approximate the collapsed variational Bayes updates in a sparse and efficient way, which we accomplish via the MetropolisHastings Walker method. We showcase our algorithm on LDA and the recently proposed mixed membership skip-gram topic model. Our method requires only amortized O(kd) computation per word token instead of O(K) operations, where the number of topics occurring for a particular document kd\u226a the total number of topics in the corpus K, to converge to a high-quality solution.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Integrated Approach for Keyphrase Generation via Exploring the Power of Retrieval and Extraction",
        "paper_url": "https://aclanthology.org/N19-1292/",
        "paper_authors": [
            "Wang Chen",
            "Hou Pong Chan",
            "Piji Li",
            "Lidong Bing",
            "Irwin King"
        ],
        "paper_abstract": "In this paper, we present a novel integrated approach for keyphrase generation (KG). Unlike previous works which are purely extractive or generative, we first propose a new multi-task learning framework that jointly learns an extractive model and a generative model. Besides extracting keyphrases, the output of the extractive model is also employed to rectify the copy probability distribution of the generative model, such that the generative model can better identify important contents from the given document. Moreover, we retrieve similar documents with the given document from training data and use their associated keyphrases as external knowledge for the generative model to produce more accurate keyphrases. For further exploiting the power of extraction and retrieval, we propose a neural-based merging module to combine and re-rank the predicted keyphrases from the enhanced generative model, the extractive model, and the retrieved keyphrases. Experiments on the five KG benchmarks demonstrate that our integrated approach outperforms the state-of-the-art methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Predicting Malware Attributes from Cybersecurity Texts",
        "paper_url": "https://aclanthology.org/N19-1293/",
        "paper_authors": [
            "Arpita Roy",
            "Youngja Park",
            "Shimei Pan"
        ],
        "paper_abstract": "Text analytics is a useful tool for studying malware behavior and tracking emerging threats. The task of automated malware attribute identification based on cybersecurity texts is very challenging due to a large number of malware attribute labels and a small number of training instances. In this paper, we propose a novel feature learning method to leverage diverse knowledge sources such as small amount of human annotations, unlabeled text and specifications about malware attribute labels. Our evaluation has demonstrated the effectiveness of our method over the state-of-the-art malware attribute prediction systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering",
        "paper_url": "https://aclanthology.org/N19-1294/",
        "paper_authors": [
            "Bo Chen",
            "Xiaotao Gu",
            "Yufeng Hu",
            "Siliang Tang",
            "Guoping Hu",
            "Yueting Zhuang",
            "Xiang Ren"
        ],
        "paper_abstract": "Recently, distant supervision has gained great success on Fine-grained Entity Typing (FET). Despite its efficiency in reducing manual labeling efforts, it also brings the challenge of dealing with false entity type labels, as distant supervision assigns labels in a context-agnostic manner. Existing works alleviated this issue with partial-label loss, but usually suffer from confirmation bias, which means the classifier fit a pseudo data distribution given by itself. In this work, we propose to regularize distantly supervised models with Compact Latent Space Clustering (CLSC) to bypass this problem and effectively utilize noisy data yet. Our proposed method first dynamically constructs a similarity graph of different entity mentions; infer the labels of noisy instances via label propagation. Based on the inferred labels, mention embeddings are updated accordingly to encourage entity mentions with close semantics to form a compact cluster in the embedding space, thus leading to better classification performance. Extensive experiments on standard benchmarks show that our CLSC model consistently outperforms state-of-the-art distantly supervised entity typing systems by a significant margin.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Modelling Instance-Level Annotator Reliability for Natural Language Labelling Tasks",
        "paper_url": "https://aclanthology.org/N19-1295/",
        "paper_authors": [
            "Maolin Li",
            "Arvid Fahlstr\u00f6m Myrman",
            "Tingting Mu",
            "Sophia Ananiadou"
        ],
        "paper_abstract": "When constructing models that learn from noisy labels produced by multiple annotators, it is important to accurately estimate the reliability of annotators. Annotators may provide labels of inconsistent quality due to their varying expertise and reliability in a domain. Previous studies have mostly focused on estimating each annotator\u2019s overall reliability on the entire annotation task. However, in practice, the reliability of an annotator may depend on each specific instance. Only a limited number of studies have investigated modelling per-instance reliability and these only considered binary labels. In this paper, we propose an unsupervised model which can handle both binary and multi-class labels. It can automatically estimate the per-instance reliability of each annotator and the correct label for each instance. We specify our model as a probabilistic model which incorporates neural networks to model the dependency between latent variables and instances. For evaluation, the proposed method is applied to both synthetic and real data, including two labelling tasks: text classification and textual entailment. Experimental results demonstrate our novel method can not only accurately estimate the reliability of annotators across different instances, but also achieve superior performance in predicting the correct labels and detecting the least reliable annotators compared to state-of-the-art baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations",
        "paper_url": "https://aclanthology.org/N19-1296/",
        "paper_authors": [
            "Guangxiang Zhao",
            "Jingjing Xu",
            "Qi Zeng",
            "Xuancheng Ren",
            "Xu Sun"
        ],
        "paper_abstract": "This paper explores a new natural languageprocessing task, review-driven multi-label musicstyle classification. This task requires systemsto identify multiple styles of music basedon its reviews on websites. The biggest challengelies in the complicated relations of musicstyles. To tackle this problem, we proposea novel deep learning approach to automaticallylearn and exploit style correlations. Experiment results show that our approachachieves large improvements over baselines onthe proposed dataset. Furthermore, the visualizedanalysis shows that our approach performswell in capturing style correlations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fact Discovery from Knowledge Base via Facet Decomposition",
        "paper_url": "https://aclanthology.org/N19-1297/",
        "paper_authors": [
            "Zihao Fu",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Wai Lam"
        ],
        "paper_abstract": "During the past few decades, knowledge bases (KBs) have experienced rapid growth. Nevertheless, most KBs still suffer from serious incompletion. Researchers proposed many tasks such as knowledge base completion and relation prediction to help build the representation of KBs. However, there are some issues unsettled towards enriching the KBs. Knowledge base completion and relation prediction assume that we know two elements of the fact triples and we are going to predict the missing one. This assumption is too restricted in practice and prevents it from discovering new facts directly. To address this issue, we propose a new task, namely, fact discovery from knowledge base. This task only requires that we know the head entity and the goal is to discover facts associated with the head entity. To tackle this new problem, we propose a novel framework that decomposes the discovery problem into several facet discovery components. We also propose a novel auto-encoder based facet component to estimate some facets of the fact. Besides, we propose a feedback learning component to share the information between each facet. We evaluate our framework using a benchmark dataset and the experimental results show that our framework achieves promising results. We also conduct an extensive analysis of our framework in discovering different kinds of facts. The source code of this paper can be obtained from https://github.com/thunlp/FFD.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for Relation Extraction",
        "paper_url": "https://aclanthology.org/N19-1298/",
        "paper_authors": [
            "Duy-Cat Can",
            "Hoang-Quynh Le",
            "Quang-Thuy Ha",
            "Nigel Collier"
        ],
        "paper_abstract": "To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https://github.com/catcd/RbSP.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases",
        "paper_url": "https://aclanthology.org/N19-1299/",
        "paper_authors": [
            "Yu Chen",
            "Lingfei Wu",
            "Mohammed J. Zaki"
        ],
        "paper_abstract": "When answering natural language questions over knowledge bases (KBs), different question components and KB aspects play different roles. However, most existing embedding-based methods for knowledge base question answering (KBQA) ignore the subtle inter-relationships between the question and the KB (e.g., entity types, relation paths and context). In this work, we propose to directly model the two-way flow of interactions between the questions and the KB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring no external resources and only very few hand-crafted features, on the WebQuestions benchmark, our method significantly outperforms existing information-retrieval based methods, and remains competitive with (hand-crafted) semantic parsing based methods. Also, since we use attention mechanisms, our method offers better interpretability compared to other baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
        "paper_url": "https://aclanthology.org/N19-1300/",
        "paper_authors": [
            "Christopher Clark",
            "Kenton Lee",
            "Ming-Wei Chang",
            "Tom Kwiatkowski",
            "Michael Collins",
            "Kristina Toutanova"
        ],
        "paper_abstract": "In this paper we study yes/no questions that are naturally occurring \u2014 meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering",
        "paper_url": "https://aclanthology.org/N19-1301/",
        "paper_authors": [
            "Kun Xu",
            "Yuxuan Lai",
            "Yansong Feng",
            "Zhiguo Wang"
        ],
        "paper_abstract": "Traditional Key-value Memory Neural Networks (KV-MemNNs) are proved to be effective to support shallow reasoning over a collection of documents in domain specific Question Answering or Reading Comprehension tasks. However, extending KV-MemNNs to Knowledge Based Question Answering (KB-QA) is not trivia, which should properly decompose a complex question into a sequence of queries against the memory, and update the query representations to support multi-hop reasoning over the memory. In this paper, we propose a novel mechanism to enable conventional KV-MemNNs models to perform interpretable reasoning for complex questions. To achieve this, we design a new query updating strategy to mask previously-addressed memory information from the query representations, and introduce a novel STOP strategy to avoid invalid or repeated memory reading without strong annotation signals. This also enables KV-MemNNs to produce structured queries and work in a semantic parsing fashion. Experimental results on benchmark datasets show that our solution, trained with question-answer pairs only, can provide conventional KV-MemNNs models with better reasoning abilities on complex questions, and achieve state-of-art performances.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Repurposing Entailment for Multi-Hop Question Answering Tasks",
        "paper_url": "https://aclanthology.org/N19-1302/",
        "paper_authors": [
            "Harsh Trivedi",
            "Heeyoung Kwon",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Niranjan Balasubramanian"
        ],
        "paper_abstract": "Question Answering (QA) naturally reduces to an entailment problem, namely, verifying whether some text entails the answer to a question. However, for multi-hop QA tasks, which require reasoning with multiple sentences, it remains unclear how best to utilize entailment models pre-trained on large scale datasets such as SNLI, which are based on sentence pairs. We introduce Multee, a general architecture that can effectively use entailment models for multi-hop QA tasks. Multee uses (i) a local module that helps locate important sentences, thereby avoiding distracting information, and (ii) a global module that aggregates information by effectively incorporating importance weights. Importantly, we show that both modules can use entailment functions pre-trained on a large scale NLI datasets. We evaluate performance on MultiRC and OpenBookQA, two multihop QA datasets. When using an entailment function pre-trained on NLI datasets, Multee outperforms QA models trained only on the target QA datasets and the OpenAI transformer models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GenderQuant: Quantifying Mention-Level Genderedness",
        "paper_url": "https://aclanthology.org/N19-1303/",
        "paper_authors": [
            "Ananya",
            "Nitya Parthasarthi",
            "Sameer Singh"
        ],
        "paper_abstract": "Language is gendered if the context surrounding a mention is suggestive of a particular binary gender for that mention. Detecting the different ways in which language is gendered is an important task since gendered language can bias NLP models (such as for coreference resolution). This task is challenging since genderedness is often expressed in subtle ways. Existing approaches need considerable annotation efforts for each language, domain, and author, and often require handcrafted lexicons and features. Additionally, these approaches do not provide a quantifiable measure of how gendered the text is, nor are they applicable at the fine-grained mention level. In this paper, we use existing NLP pipelines to automatically annotate gender of mentions in the text. On corpora labeled using this method, we train a supervised classifier to predict the gender of any mention from its context and evaluate it on unseen text. The model confidence for a mention\u2019s gender can be used as a proxy to indicate the level of genderedness of the context. We test this gendered language detector on movie summaries, movie reviews, news articles, and fiction novels, achieving an AUC-ROC of up to 0.71, and observe that the model predictions agree with human judgments collected for this task. We also provide examples of detected gendered sentences from aforementioned domains.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings",
        "paper_url": "https://aclanthology.org/N19-1304/",
        "paper_authors": [
            "Dorottya Demszky",
            "Nikhil Garg",
            "Rob Voigt",
            "James Zou",
            "Jesse Shapiro",
            "Matthew Gentzkow",
            "Dan Jurafsky"
        ],
        "paper_abstract": "We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Decipher Hate Symbols",
        "paper_url": "https://aclanthology.org/N19-1305/",
        "paper_authors": [
            "Jing Qian",
            "Mai ElSherief",
            "Elizabeth Belding",
            "William Yang Wang"
        ],
        "paper_abstract": "Existing computational models to understand hate speech typically frame the problem as a simple classification task, bypassing the understanding of hate symbols (e.g., 14 words, kigy) and their secret connotations. In this paper, we propose a novel task of deciphering hate symbols. To do this, we leveraged the Urban Dictionary and collected a new, symbol-rich Twitter corpus of hate speech. We investigate neural network latent context models for deciphering hate symbols. More specifically, we study Sequence-to-Sequence models and show how they are able to crack the ciphers based on context. Furthermore, we propose a novel Variational Decipher and show how it can generalize better to unseen hate symbols in a more challenging testing setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks",
        "paper_url": "https://aclanthology.org/N19-1306/",
        "paper_authors": [
            "Ningyu Zhang",
            "Shumin Deng",
            "Zhanlin Sun",
            "Guanying Wang",
            "Xi Chen",
            "Wei Zhang",
            "Huajun Chen"
        ],
        "paper_abstract": "We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate \u201cfew-shot\u201d models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that relational knowledge into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other baselines, especially for long-tail relations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GAN Driven Semi-distant Supervision for Relation Extraction",
        "paper_url": "https://aclanthology.org/N19-1307/",
        "paper_authors": [
            "Pengshuai Li",
            "Xinsong Zhang",
            "Weijia Jia",
            "Hai Zhao"
        ],
        "paper_abstract": "Distant supervision has been widely used in relation extraction tasks without hand-labeled datasets recently. However, the automatically constructed datasets comprise numbers of wrongly labeled negative instances due to the incompleteness of knowledge bases, which is neglected by current distant supervised methods resulting in seriously misleading in both training and testing processes. To address this issue, we propose a novel semi-distant supervision approach for relation extraction by constructing a small accurate dataset and properly leveraging numerous instances without relation labels. In our approach, we construct accurate instances by both knowledge base and entity descriptions determined to avoid wrong negative labeling and further utilize unlabeled instances sufficiently using generative adversarial network (GAN) framework. Experimental results on real-world datasets show that our approach can achieve significant improvements in distant supervised relation extraction over strong baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A general framework for information extraction using dynamic span graphs",
        "paper_url": "https://aclanthology.org/N19-1308/",
        "paper_authors": [
            "Yi Luan",
            "Dave Wadden",
            "Luheng He",
            "Amy Shah",
            "Mari Ostendorf",
            "Hannaneh Hajishirzi"
        ],
        "paper_abstract": "We introduce a general framework for several information extraction tasks that share span representations using dynamically constructed span graphs. The graphs are dynamically constructed by selecting the most confident entity spans and linking these nodes with confidence-weighted relation types and coreferences. The dynamic span graph allow coreference and relation type confidences to propagate through the graph to iteratively refine the span representations. This is unlike previous multi-task frameworks for information extraction in which the only interaction between tasks is in the shared first-layer LSTM. Our framework significantly outperforms state-of-the-art on multiple information extraction tasks across multiple datasets reflecting different domains. We further observe that the span enumeration approach is good at detecting nested span entities, with significant F1 score improvement on the ACE dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "OpenCeres: When Open Information Extraction Meets the Semi-Structured Web",
        "paper_url": "https://aclanthology.org/N19-1309/",
        "paper_authors": [
            "Colin Lockard",
            "Prashant Shiralkar",
            "Xin Luna Dong"
        ],
        "paper_abstract": "Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade. However, this research has largely ignored the vast quantity of facts available in semi-structured webpages. In this paper, we define the problem of OpenIE from semi-structured websites to extract such facts, and present an approach for solving it. We also introduce a labeled evaluation dataset to motivate research in this area. Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create training data for the relations present on the site. We then use this training data to learn a classifier for relation extraction. Experimental results of this method on our new benchmark dataset obtained a precision of over 70%. A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Structured Minimally Supervised Learning for Neural Relation Extraction",
        "paper_url": "https://aclanthology.org/N19-1310/",
        "paper_authors": [
            "Fan Bai",
            "Alan Ritter"
        ],
        "paper_abstract": "We present an approach to minimally supervised relation extraction that combines the benefits of learned representations and structured learning, and accurately predicts sentence-level relation mentions given only proposition-level supervision from a KB. By explicitly reasoning about missing data during learning, our approach enables large-scale training of 1D convolutional neural networks while mitigating the issue of label noise inherent in distant supervision. Our approach achieves state-of-the-art results on minimally supervised sentential relation extraction, outperforming a number of baselines, including a competitive approach that uses the attention layer of a purely neural model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Machine Translation of Text from Non-Native Speakers",
        "paper_url": "https://aclanthology.org/N19-1311/",
        "paper_authors": [
            "Antonios Anastasopoulos",
            "Alison Lui",
            "Toan Q. Nguyen",
            "David Chiang"
        ],
        "paper_abstract": "Neural Machine Translation (NMT) systems are known to degrade when confronted with noisy data, especially when the system is trained only on clean data. In this paper, we show that augmenting training data with sentences containing artificially-introduced grammatical errors can make the system more robust to such errors. In combination with an automatic grammar error correction system, we can recover 1.0 BLEU out of 2.4 BLEU lost due to grammatical errors. We also present a set of Spanish translations of the JFLEG grammar error correction corpus, which allows for testing NMT robustness to real grammatical errors.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Domain Adaptation Translation with Domain Invariant and Specific Information",
        "paper_url": "https://aclanthology.org/N19-1312/",
        "paper_authors": [
            "Shuhao Gu",
            "Yang Feng",
            "Qun Liu"
        ],
        "paper_abstract": "In domain adaptation for neural machine translation, translation performance can benefit from separating features into domain-specific features and common features. In this paper, we propose a method to explicitly model the two kinds of information in the encoder-decoder framework so as to exploit out-of-domain data in in-domain training. In our method, we maintain a private encoder and a private decoder for each domain which are used to model domain-specific information. In the meantime, we introduce a common encoder and a common decoder shared by all the domains which can only have domain-independent information flow through. Besides, we add a discriminator to the shared encoder and employ adversarial training for the whole model to reinforce the performance of information separation and machine translation simultaneously. Experiment results show that our method can outperform competitive baselines greatly on multiple data sets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Selective Attention for Context-aware Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1313/",
        "paper_authors": [
            "Sameen Maruf",
            "Andr\u00e9 F. T. Martins",
            "Gholamreza Haffari"
        ],
        "paper_abstract": "Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document. Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents. To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences. We also propose single-level attention approaches based on sentence or word-level information in the context. The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context. Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models",
        "paper_url": "https://aclanthology.org/N19-1314/",
        "paper_authors": [
            "Paul Michel",
            "Xian Li",
            "Graham Neubig",
            "Juan Pino"
        ],
        "paper_abstract": "Adversarial examples \u2014 perturbations to the input of a model that elicit large changes in the output \u2014 have been shown to be an effective way of assessing the robustness of sequence-to-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. A toolkit implementing our evaluation framework is released at https://github.com/pmichel31415/teapot-nlp.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction",
        "paper_url": "https://aclanthology.org/N19-1315/",
        "paper_authors": [
            "Kazuma Hashimoto",
            "Yoshimasa Tsuruoka"
        ],
        "paper_abstract": "A major obstacle in reinforcement learning-based sentence generation is the large action space whose size is equal to the vocabulary size of the target-side language. To improve the efficiency of reinforcement learning, we present a novel approach for reducing the action space based on dynamic vocabulary prediction. Our method first predicts a fixed-size small vocabulary for each input to generate its target sentence. The input-specific vocabularies are then used at supervised and reinforcement learning steps, and also at test time. In our experiments on six machine translation and two image captioning datasets, our method achieves faster reinforcement learning (~2.7x faster) with less GPU memory (~2.3x less) than the full-vocabulary counterpart. We also show that our method more effectively receives rewards with fewer iterations of supervised pre-training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mitigating Uncertainty in Document Classification",
        "paper_url": "https://aclanthology.org/N19-1316/",
        "paper_authors": [
            "Xuchao Zhang",
            "Fanglan Chen",
            "Chang-Tien Lu",
            "Naren Ramakrishnan"
        ],
        "paper_abstract": "The uncertainty measurement of classifiers\u2019 predictions is especially important in applications such as medical diagnoses that need to ensure limited human resources can focus on the most uncertain predictions returned by machine learning models. However, few existing uncertainty models attempt to improve overall prediction accuracy where human resources are involved in the text classification task. In this paper, we propose a novel neural-network-based model that applies a new dropout-entropy method for uncertainty measurement. We also design a metric learning method on feature representations, which can boost the performance of dropout-based uncertainty methods with smaller prediction variance in accurate prediction trials. Extensive experiments on real-world data sets demonstrate that our method can achieve a considerable improvement in overall prediction accuracy compared to existing approaches. In particular, our model improved the accuracy from 0.78 to 0.92 when 30% of the most uncertain predictions were handed over to human experts in \u201c20NewsGroup\u201d data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification",
        "paper_url": "https://aclanthology.org/N19-1317/",
        "paper_authors": [
            "Reno Kriz",
            "Jo\u00e3o Sedoc",
            "Marianna Apidianaki",
            "Carolina Zheng",
            "Gaurav Kumar",
            "Eleni Miltsakaki",
            "Chris Callison-Burch"
        ],
        "paper_abstract": "Sentence simplification is the task of rewriting texts so they are easier to understand. Recent research has applied sequence-to-sequence (Seq2Seq) models to this task, focusing largely on training-time improvements via reinforcement learning and memory augmentation. One of the main problems with applying generic Seq2Seq models for simplification is that these models tend to copy directly from the original sentence, resulting in outputs that are relatively long and complex. We aim to alleviate this issue through the use of two main techniques. First, we incorporate content word complexities, as predicted with a leveled word complexity model, into our loss function during training. Second, we generate a large set of diverse candidate simplifications at test time, and rerank these to promote fluency, adequacy, and simplicity. Here, we measure simplicity through a novel sentence complexity model. These extensions allow our models to perform competitively with state-of-the-art systems while generating simpler sentences. We report standard automatic and human evaluation metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Predicting Helpful Posts in Open-Ended Discussion Forums: A Neural Architecture",
        "paper_url": "https://aclanthology.org/N19-1318/",
        "paper_authors": [
            "Kishaloy Halder",
            "Min-Yen Kan",
            "Kazunari Sugiyama"
        ],
        "paper_abstract": "Users participate in online discussion forums to learn from others and share their knowledge with the community. They often start a thread with a question or by sharing their new findings on a certain topic. We find that, unlike Community Question Answering, where questions are mostly factoid based, the threads in a forum are often open-ended (e.g., asking for recommendations from others) without a single correct answer. In this paper, we address the task of identifying helpful posts in a forum thread to help users comprehend long running discussion threads, which often contain repetitive or irrelevant posts. We propose a recurrent neural network based architecture to model (i) the relevance of a post regarding the original post starting the thread and (ii) the novelty it brings to the discussion, compared to the previous posts in the thread. Experimental results on different types of online forum datasets show that our model significantly outperforms the state-of-the-art neural network models for text classification.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Text Classification with Few Examples using Controlled Generalization",
        "paper_url": "https://aclanthology.org/N19-1319/",
        "paper_authors": [
            "Abhijit Mahabal",
            "Jason Baldridge",
            "Burcu Karagol Ayan",
            "Vincent Perot",
            "Dan Roth"
        ],
        "paper_abstract": "Training data for text classification is often limited in practice, especially for applications with many output classes or involving many related classification problems. This means classifiers must generalize from limited evidence, but the manner and extent of generalization is task dependent. Current practice primarily relies on pre-trained word embeddings to map words unseen in training to similar seen ones. Unfortunately, this squishes many components of meaning into highly restricted capacity. Our alternative begins with sparse pre-trained representations derived from unlabeled parsed corpora; based on the available training data, we select features that offers the relevant generalizations. This produces task-specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods. By further pairing this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus",
        "paper_url": "https://aclanthology.org/N19-1320/",
        "paper_authors": [
            "Hongyu Gong",
            "Suma Bhat",
            "Lingfei Wu",
            "JinJun Xiong",
            "Wen-mei Hwu"
        ],
        "paper_abstract": "Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks\u2013sentiment transfer, and formality transfer\u2013show that our model outperforms state-of-the-art approaches. Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Adapting RNN Sequence Prediction Model to Multi-label Set Prediction",
        "paper_url": "https://aclanthology.org/N19-1321/",
        "paper_authors": [
            "Kechen Qin",
            "Cheng Li",
            "Virgil Pavlu",
            "Javed Aslam"
        ],
        "paper_abstract": "We present an adaptation of RNN sequence models to the problem of multi-label classification for text, where the target is a set of labels, not a sequence. Previous such RNN models define probabilities for sequences but not for sets; attempts to obtain a set probability are after-thoughts of the network design, including pre-specifying the label order, or relating the sequence probability to the set probability in ad hoc ways. Our formulation is derived from a principled notion of set probability, as the sum of probabilities of corresponding permutation sequences for the set. We provide a new training objective that maximizes this set probability, and a new prediction objective that finds the most probable set on a test document. These new objectives are theoretically appealing because they give the RNN model freedom to discover the best label order, which often is the natural one (but different among documents). We develop efficient procedures to tackle the computation difficulties involved in training and prediction. Experiments on benchmark datasets demonstrate that we outperform state-of-the-art methods for this task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Customizing Grapheme-to-Phoneme System for Non-Trivial Transcription Problems in Bangla Language",
        "paper_url": "https://aclanthology.org/N19-1322/",
        "paper_authors": [
            "Sudipta Saha Shubha",
            "Nafis Sadeq",
            "Shafayat Ahmed",
            "Md. Nahidul Islam",
            "Muhammad Abdullah Adnan",
            "Md. Yasin Ali Khan",
            "Mohammad Zuberul Islam"
        ],
        "paper_abstract": "Grapheme to phoneme (G2P) conversion is an integral part in various text and speech processing systems, such as: Text to Speech system, Speech Recognition system, etc. The existing methodologies for G2P conversion in Bangla language are mostly rule-based. However, data-driven approaches have proved their superiority over rule-based approaches for large-scale G2P conversion in other languages, such as: English, German, etc. As the performance of data-driven approaches for G2P conversion depend largely on pronunciation lexicon on which the system is trained, in this paper, we investigate on developing an improved training lexicon by identifying and categorizing the critical cases in Bangla language and include those critical cases in training lexicon for developing a robust G2P conversion system in Bangla language. Additionally, we have incorporated nasal vowels in our proposed phoneme list. Our methodology outperforms other state-of-the-art approaches for G2P conversion in Bangla language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Connecting Language and Knowledge with Heterogeneous Representations for Neural Relation Extraction",
        "paper_url": "https://aclanthology.org/N19-1323/",
        "paper_authors": [
            "Peng Xu",
            "Denilson Barbosa"
        ],
        "paper_abstract": "Knowledge Bases (KBs) require constant updating to reflect changes to the world they represent. For general purpose KBs, this is often done through Relation Extraction (RE), the task of predicting KB relations expressed in text mentioning entities known to the KB. One way to improve RE is to use KB Embeddings (KBE) for link prediction. However, despite clear connections between RE and KBE, little has been done toward properly unifying these models systematically. We help close the gap with a framework that unifies the learning of RE and KBE models leading to significant improvements over the state-of-the-art in RE. The code is available at https://github.com/billy-inn/HRERE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Segmentation-free compositional n-gram embedding",
        "paper_url": "https://aclanthology.org/N19-1324/",
        "paper_authors": [
            "Geewook Kim",
            "Kazuki Fukui",
            "Hidetoshi Shimodaira"
        ],
        "paper_abstract": "We propose a new type of representation learning method that models words, phrases and sentences seamlessly. Our method does not depend on word segmentation and any human-annotated resources (e.g., word dictionaries), yet it is very effective for noisy corpora written in unsegmented languages such as Chinese and Japanese. The main idea of our method is to ignore word boundaries completely (i.e., segmentation-free), and construct representations for all character n-grams in a raw corpus with embeddings of compositional sub-n-grams. Although the idea is simple, our experiments on various benchmarks and real-world datasets show the efficacy of our proposal.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Exploiting Noisy Data in Distant Supervision Relation Classification",
        "paper_url": "https://aclanthology.org/N19-1325/",
        "paper_authors": [
            "Kaijia Yang",
            "Liang He",
            "Xin-yu Dai",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "paper_abstract": "Distant supervision has obtained great progress on relation classification task. However, it still suffers from noisy labeling problem. Different from previous works that underutilize noisy data which inherently characterize the property of classification, in this paper, we propose RCEND, a novel framework to enhance Relation Classification by Exploiting Noisy Data. First, an instance discriminator with reinforcement learning is designed to split the noisy data into correctly labeled data and incorrectly labeled data. Second, we learn a robust relation classifier in semi-supervised learning way, whereby the correctly and incorrectly labeled data are treated as labeled and unlabeled data respectively. The experimental results show that our method outperforms the state-of-the-art models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Misspelling Oblivious Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-1326/",
        "paper_authors": [
            "Aleksandra Piktus",
            "Necati Bora Edizel",
            "Piotr Bojanowski",
            "Edouard Grave",
            "Rui Ferreira",
            "Fabrizio Silvestri"
        ],
        "paper_abstract": "In this paper we present a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of out-of-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Relational Representations by Analogy using Hierarchical Siamese Networks",
        "paper_url": "https://aclanthology.org/N19-1327/",
        "paper_authors": [
            "Gaetano Rossiello",
            "Alfio Gliozzo",
            "Robert Farrell",
            "Nicolas Fauceglia",
            "Michael Glass"
        ],
        "paper_abstract": "We address relation extraction as an analogy problem by proposing a novel approach to learn representations of relations expressed by their textual mentions. In our assumption, if two pairs of entities belong to the same relation, then those two pairs are analogous. Following this idea, we collect a large set of analogous pairs by matching triples in knowledge bases with web-scale corpora through distant supervision. We leverage this dataset to train a hierarchical siamese network in order to learn entity-entity embeddings which encode relational information through the different linguistic paraphrasing expressing the same relation. We evaluate our model in a one-shot learning task by showing a promising generalization capability in order to classify unseen relation types, which makes this approach suitable to perform automatic knowledge base population with minimal supervision. Moreover, the model can be used to generate pre-trained embeddings which provide a valuable signal when integrated into an existing neural-based model by outperforming the state-of-the-art methods on a downstream relation extraction task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "An Effective Label Noise Model for DNN Text Classification",
        "paper_url": "https://aclanthology.org/N19-1328/",
        "paper_authors": [
            "Ishan Jindal",
            "Daniel Pressel",
            "Brian Lester",
            "Matthew Nokleby"
        ],
        "paper_abstract": "Because large, human-annotated datasets suffer from labeling errors, it is crucial to be able to train deep neural networks in the presence of label noise. While training image classification models with label noise have received much attention, training text classification models have not. In this paper, we propose an approach to training deep networks that is robust to label noise. This approach introduces a non-linear processing layer (noise model) that models the statistics of the label noise into a convolutional neural network (CNN) architecture. The noise model and the CNN weights are learned jointly from noisy training data, which prevents the model from overfitting to erroneous labels. Through extensive experiments on several text classification datasets, we show that this approach enables the CNN to learn better sentence representations and is robust even to extreme label noise. We find that proper initialization and regularization of this noise model is critical. Further, by contrast to results focusing on large batch sizes for mitigating label noise for image classification, we find that altering the batch size does not have much effect on classification performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding Learning Dynamics Of Language Models with SVCCA",
        "paper_url": "https://aclanthology.org/N19-1329/",
        "paper_authors": [
            "Naomi Saphra",
            "Adam Lopez"
        ],
        "paper_abstract": "Research has shown that neural models implicitly encode linguistic features, but there has been no research showing how these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis (SVCCA), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations, finding, for example, that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language Models",
        "paper_url": "https://aclanthology.org/N19-1330/",
        "paper_authors": [
            "Yiben Yang",
            "Ji-Ping Wang",
            "Doug Downey"
        ],
        "paper_abstract": "Recurrent neural network language models (RNNLM) form a valuable foundation for many NLP systems, but training the models can be computationally expensive, and may take days to train on a large corpus. We explore a technique that uses large corpus n-gram statistics as a regularizer for training a neural network LM on a smaller corpus. In experiments with the Billion-Word and Wikitext corpora, we show that the technique is effective, and more time-efficient than simply training on a larger sequential corpus. We also introduce new strategies for selecting the most informative n-grams, and show that these boost efficiency.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Continual Learning for Sentence Representations Using Conceptors",
        "paper_url": "https://aclanthology.org/N19-1331/",
        "paper_authors": [
            "Tianlin Liu",
            "Lyle Ungar",
            "Jo\u00e3o Sedoc"
        ],
        "paper_abstract": "Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Relation Discovery with Out-of-Relation Knowledge Base as Supervision",
        "paper_url": "https://aclanthology.org/N19-1332/",
        "paper_authors": [
            "Yan Liang",
            "Xin Liu",
            "Jianwen Zhang",
            "Yangqiu Song"
        ],
        "paper_abstract": "Unsupervised relation discovery aims to discover new relations from a given text corpus without annotated data. However, it does not consider existing human annotated knowledge bases even when they are relevant to the relations to be discovered. In this paper, we study the problem of how to use out-of-relation knowledge bases to supervise the discovery of unseen relations, where out-of-relation means that relations to discover from the text corpus and those in knowledge bases are not overlapped. We construct a set of constraints between entity pairs based on the knowledge base embedding and then incorporate constraints into the relation discovery by a variational auto-encoder based algorithm. Experiments show that our new approach can improve the state-of-the-art relation discovery performance by a large margin.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Corpora Generation for Grammatical Error Correction",
        "paper_url": "https://aclanthology.org/N19-1333/",
        "paper_authors": [
            "Jared Lichtarge",
            "Chris Alberti",
            "Shankar Kumar",
            "Noam Shazeer",
            "Niki Parmar",
            "Simon Tong"
        ],
        "paper_abstract": "Grammatical Error Correction (GEC) has been recently modeled using the sequence-to-sequence framework. However, unlike sequence transduction problems such as machine translation, GEC suffers from the lack of plentiful parallel data. We describe two approaches for generating large parallel datasets for GEC using publicly available Wikipedia data. The first method extracts source-target pairs from Wikipedia edit histories with minimal filtration heuristics while the second method introduces noise into Wikipedia sentences via round-trip translation through bridge languages. Both strategies yield similar sized parallel corpora containing around 4B tokens. We employ an iterative decoding strategy that is tailored to the loosely supervised nature of our constructed corpora. We demonstrate that neural GEC models trained using either type of corpora give similar performance. Fine-tuning these models on the Lang-8 corpus and ensembling allows us to surpass the state of the art on both the CoNLL \u201814 benchmark and the JFLEG task. We present systematic analysis that compares the two approaches to data generation and highlights the effectiveness of ensembling.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Structural Supervision Improves Learning of Non-Local Grammatical Dependencies",
        "paper_url": "https://aclanthology.org/N19-1334/",
        "paper_authors": [
            "Ethan Wilcox",
            "Peng Qian",
            "Richard Futrell",
            "Miguel Ballesteros",
            "Roger Levy"
        ],
        "paper_abstract": "State-of-the-art LSTM language models trained on large corpora learn sequential contingencies in impressive detail, and have been shown to acquire a number of non-local grammatical dependencies with some success. Here we investigate whether supervision with hierarchical structure enhances learning of a range of grammatical dependencies, a question that has previously been addressed only for subject-verb agreement. Using controlled experimental methods from psycholinguistics, we compare the performance of word-based LSTM models versus Recurrent Neural Network Grammars (RNNGs) (Dyer et al. 2016) which represent hierarchical syntactic structure and use neural control to deploy it in left-to-right processing, on two classes of non-local grammatical dependencies in English\u2014Negative Polarity licensing and Filler-Gap Dependencies\u2014tested in a range of configurations. Using the same training data for both models, we find that the RNNG outperforms the LSTM on both types of grammatical dependencies and even learns many of the Island Constraints on the filler-gap dependency. Structural supervision thus provides data efficiency advantages over purely string-based training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Benchmarking Approximate Inference Methods for Neural Structured Prediction",
        "paper_url": "https://aclanthology.org/N19-1335/",
        "paper_authors": [
            "Lifu Tu",
            "Kevin Gimpel"
        ],
        "paper_abstract": "Exact structured inference with neural network scoring functions is computationally challenging but several methods have been proposed for approximating inference. One approach is to perform gradient descent with respect to the output structure directly (Belanger and McCallum, 2016). Another approach, proposed recently, is to train a neural network (an \u201cinference network\u201d) to perform inference (Tu and Gimpel, 2018). In this paper, we compare these two families of inference methods on three sequence labeling datasets. We choose sequence labeling because it permits us to use exact inference as a benchmark in terms of speed, accuracy, and search error. Across datasets, we demonstrate that inference networks achieve a better speed/accuracy/search error trade-off than gradient descent, while also being faster than exact inference at similar accuracy levels. We find further benefit by combining inference networks and gradient descent, using the former to provide a warm start for the latter.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating and Enhancing the Robustness of Dialogue Systems: A Case Study on a Negotiation Agent",
        "paper_url": "https://aclanthology.org/N19-1336/",
        "paper_authors": [
            "Minhao Cheng",
            "Wei Wei",
            "Cho-Jui Hsieh"
        ],
        "paper_abstract": "Recent research has demonstrated that goal-oriented dialogue agents trained on large datasets can achieve striking performance when interacting with human users. In real world applications, however, it is important to ensure that the agent performs smoothly interacting with not only regular users but also those malicious ones who would attack the system through interactions in order to achieve goals for their own advantage. In this paper, we develop algorithms to evaluate the robustness of a dialogue agent by carefully designed attacks using adversarial agents. Those attacks are performed in both black-box and white-box settings. Furthermore, we demonstrate that adversarial training using our attacks can significantly improve the robustness of a goal-oriented dialogue system. On a case-study of the negotiation agent developed by (Lewis et al., 2017), our attacks reduced the average advantage of rewards between the attacker and the trained RL-based agent from 2.68 to -5.76 on a scale from -10 to 10 for randomized goals. Moreover, we show that with the adversarial training, we are able to improve the robustness of negotiation agents by 1.5 points on average against all our attacks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications",
        "paper_url": "https://aclanthology.org/N19-1337/",
        "paper_authors": [
            "Pouya Pezeshkpour",
            "Yifan Tian",
            "Sameer Singh"
        ],
        "paper_abstract": "Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Transferable Neural Projection Representations",
        "paper_url": "https://aclanthology.org/N19-1339/",
        "paper_authors": [
            "Chinnadhurai Sankar",
            "Sujith Ravi",
            "Zornitsa Kozareva"
        ],
        "paper_abstract": "Neural word representations are at the core of many state-of-the-art natural language processing models. A widely used approach is to pre-train, store and look up word or character embedding matrices. While useful, such representations occupy huge memory making it hard to deploy on-device and often do not generalize to unknown words due to vocabulary pruning. In this paper, we propose a skip-gram based architecture coupled with Locality-Sensitive Hashing (LSH) projections to learn efficient dynamically computable representations. Our model does not need to store lookup tables as representations are computed on-the-fly and require low memory footprint. The representations can be trained in an unsupervised fashion and can be easily transferred to other NLP tasks. For qualitative evaluation, we analyze the nearest neighbors of the word representations and discover semantically similar words even with misspellings. For quantitative evaluation, we plug our transferable projections into a simple LSTM and run it on multiple NLP tasks and show how our transferable projections achieve better performance compared to prior work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Semantic Role Labeling with Associated Memory Network",
        "paper_url": "https://aclanthology.org/N19-1340/",
        "paper_authors": [
            "Chaoyu Guan",
            "Yuhao Cheng",
            "Hai Zhao"
        ],
        "paper_abstract": "Semantic role labeling (SRL) is a task to recognize all the predicate-argument pairs of a sentence, which has been in a performance improvement bottleneck after a series of latest works were presented. This paper proposes a novel syntax-agnostic SRL model enhanced by the proposed associated memory network (AMN), which makes use of inter-sentence attention of label-known associated sentences as a kind of memory to further enhance dependency-based SRL. In detail, we use sentences and their labels from train dataset as an associated memory cue to help label the target sentence. Furthermore, we compare several associated sentences selecting strategies and label merging methods in AMN to find and utilize the label of associated sentences while attending them. By leveraging the attentive memory from known training data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark datasets for syntax-agnostic setting, showing a new effective research line of SRL enhancement other than exploiting external resources such as well pre-trained language models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Better, Faster, Stronger Sequence Tagging Constituent Parsers",
        "paper_url": "https://aclanthology.org/N19-1341/",
        "paper_authors": [
            "David Vilares",
            "Mostafa Abdou",
            "Anders S\u00f8gaard"
        ],
        "paper_abstract": "Sequence tagging models for constituent parsing are faster, but less accurate than other types of parsers. In this work, we address the following weaknesses of such constituent parsers: (a) high error rates around closing brackets of long constituents, (b) large label sets, leading to sparsity, and (c) error propagation arising from greedy decoding. To effectively close brackets, we train a model that learns to switch between tagging schemes. To reduce sparsity, we decompose the label set and use multi-task learning to jointly learn to predict sublabels. Finally, we mitigate issues from greedy decoding through auxiliary losses and sentence-level fine-tuning with policy gradient. Combining these techniques, we clearly surpass the performance of sequence tagging constituent parsers on the English and Chinese Penn Treebanks, and reduce their parsing time even further. On the SPMRL datasets, we observe even greater improvements across the board, including a new state of the art on Basque, Hebrew, Polish and Swedish.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition",
        "paper_url": "https://aclanthology.org/N19-1342/",
        "paper_authors": [
            "Yuying Zhu",
            "Guoxin Wang"
        ],
        "paper_abstract": "Named entity recognition (NER) in Chinese is essential but difficult because of the lack of natural delimiters. Therefore, Chinese Word Segmentation (CWS) is usually considered as the first step for Chinese NER. However, models based on word-level embeddings and lexicon features often suffer from segmentation errors and out-of-vocabulary (OOV) words. In this paper, we investigate a Convolutional Attention Network called CAN for Chinese NER, which consists of a character-based convolutional neural network (CNN) with local-attention layer and a gated recurrent unit (GRU) with global self-attention layer to capture the information from adjacent characters and sentence contexts. Also, compared to other models, not depending on any external resources like lexicons and employing small size of char embeddings make our model more practical. Extensive experimental results show that our approach outperforms state-of-the-art methods without word embedding and external lexicon resources on different domain datasets including Weibo, MSRA and Chinese Resume NER dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Decomposed Local Models for Coordinate Structure Parsing",
        "paper_url": "https://aclanthology.org/N19-1343/",
        "paper_authors": [
            "Hiroki Teranishi",
            "Hiroyuki Shindo",
            "Yuji Matsumoto"
        ],
        "paper_abstract": "We propose a simple and accurate model for coordination boundary identification. Our model decomposes the task into three sub-tasks during training; finding a coordinator, identifying inside boundaries of a pair of conjuncts, and selecting outside boundaries of it. For inference, we make use of probabilities of coordinators and conjuncts in the CKY parsing to find the optimal combination of coordinate structures. Experimental results demonstrate that our model achieves state-of-the-art results, ensuring that the global structure of coordinations is consistent.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Task Learning for Japanese Predicate Argument Structure Analysis",
        "paper_url": "https://aclanthology.org/N19-1344/",
        "paper_authors": [
            "Hikaru Omori",
            "Mamoru Komachi"
        ],
        "paper_abstract": "An event-noun is a noun that has an argument structure similar to a predicate. Recent works, including those considered state-of-the-art, ignore event-nouns or build a single model for solving both Japanese predicate argument structure analysis (PASA) and event-noun argument structure analysis (ENASA). However, because there are interactions between predicates and event-nouns, it is not sufficient to target only predicates. To address this problem, we present a multi-task learning method for PASA and ENASA. Our multi-task models improved the performance of both tasks compared to a single-task model by sharing knowledge from each task. Moreover, in PASA, our models achieved state-of-the-art results in overall F1 scores on the NAIST Text Corpus. In addition, this is the first work to employ neural networks in ENASA.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Domain adaptation for part-of-speech tagging of noisy user-generated text",
        "paper_url": "https://aclanthology.org/N19-1345/",
        "paper_authors": [
            "Luisa M\u00e4rz",
            "Dietrich Trautmann",
            "Benjamin Roth"
        ],
        "paper_abstract": "The performance of a Part-of-speech (POS) tagger is highly dependent on the domain of the processed text, and for many domains there is no or only very little training data available. This work addresses the problem of POS tagging noisy user-generated text using a neural network. We propose an architecture that trains an out-of-domain model on a large newswire corpus, and transfers those weights by using them as a prior for a model trained on the target domain (a data-set of German Tweets) for which there is very little annotations available. The neural network has a standard bidirectional LSTM at its core. However, we find it crucial to also encode a set of task-specific features, and to obtain reliable (source-domain and target-domain) word representations. Experiments with different regularization techniques such as early stopping, dropout and fine-tuning the domain adaptation prior weights are conducted. Our best model uses external weights from the out-of-domain model, as well as feature embeddings, pre-trained word and sub-word embeddings and achieves a tagging accuracy of slightly over 90%, improving on the previous state of the art for this task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Chinese Address Parsing",
        "paper_url": "https://aclanthology.org/N19-1346/",
        "paper_authors": [
            "Hao Li",
            "Wei Lu",
            "Pengjun Xie",
            "Linlin Li"
        ],
        "paper_abstract": "This paper introduces a new task \u2013 Chinese address parsing \u2013 the task of mapping Chinese addresses into semantically meaningful chunks. While it is possible to model this problem using a conventional sequence labelling approach, our observation is that there exist complex dependencies between labels that cannot be readily captured by a simple linear-chain structure. We investigate neural structured prediction models with latent variables to capture such rich structural information within Chinese addresses. We create and publicly release a new dataset consisting of 15K Chinese addresses, and conduct extensive experiments on the dataset to investigate the model effectiveness and robustness. We release our code and data at http://statnlp.org/research/sp.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Hierarchical Discourse-level Structure for Fake News Detection",
        "paper_url": "https://aclanthology.org/N19-1347/",
        "paper_authors": [
            "Hamid Karimi",
            "Jiliang Tang"
        ],
        "paper_abstract": "On the one hand, nowadays, fake news articles are easily propagated through various online media platforms and have become a grand threat to the trustworthiness of information. On the other hand, our understanding of the language of fake news is still minimal. Incorporating hierarchical discourse-level structure of fake and real news articles is one crucial step toward a better understanding of how these articles are structured. Nevertheless, this has rarely been investigated in the fake news detection domain and faces tremendous challenges. First, existing methods for capturing discourse-level structure rely on annotated corpora which are not available for fake news datasets. Second, how to extract out useful information from such discovered structures is another challenge. To address these challenges, we propose Hierarchical Discourse-level Structure for Fake news detection. HDSF learns and constructs a discourse-level structure for fake/real news articles in an automated and data-driven manner. Moreover, we identify insightful structure-related properties, which can explain the discovered structures and boost our understating of fake news. Conducted experiments show the effectiveness of the proposed approach. Further structural analysis suggests that real and fake news present substantial differences in the hierarchical discourse-level structures.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion",
        "paper_url": "https://aclanthology.org/N19-1348/",
        "paper_authors": [
            "Mor Geva",
            "Eric Malmi",
            "Idan Szpektor",
            "Jonathan Berant"
        ],
        "paper_abstract": "Sentence fusion is the task of joining several independent sentences into a single coherent text. Current datasets for sentence fusion are small and insufficient for training modern neural models. In this paper, we propose a method for automatically-generating fusion examples from raw text and present DiscoFuse, a large scale dataset for discourse-based sentence fusion. We author a set of rules for identifying a diverse set of discourse phenomena in raw text, and decomposing the text into two independent sentences. We apply our approach on two document collections: Wikipedia and Sports articles, yielding 60 million fusion examples annotated with discourse information required to reconstruct the fused text. We develop a sequence-to-sequence model on DiscoFuse and thoroughly analyze its strengths and weaknesses with respect to the various discourse phenomena, using both automatic as well as human evaluation. Finally, we conduct transfer learning experiments with WebSplit, a recent dataset for text simplification. We show that pretraining on DiscoFuse substantially improves performance on WebSplit when viewed as a sentence fusion task.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Linguistically-Informed Specificity and Semantic Plausibility for Dialogue Generation",
        "paper_url": "https://aclanthology.org/N19-1349/",
        "paper_authors": [
            "Wei-Jen Ko",
            "Greg Durrett",
            "Junyi Jessy Li"
        ],
        "paper_abstract": "Sequence-to-sequence models for open-domain dialogue generation tend to favor generic, uninformative responses. Past work has focused on word frequency-based approaches to improving specificity, such as penalizing responses with only common words. In this work, we examine whether specificity is solely a frequency-related notion and find that more linguistically-driven specificity measures are better suited to improving response informativeness. However, we find that forcing a sequence-to-sequence model to be more specific can expose a host of other problems in the responses, including flawed discourse and implausible semantics. We rerank our model\u2019s outputs using externally-trained classifiers targeting each of these identified factors. Experiments show that our final model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Describe Unknown Phrases with Local and Global Contexts",
        "paper_url": "https://aclanthology.org/N19-1350/",
        "paper_authors": [
            "Shonosuke Ishiwatari",
            "Hiroaki Hayashi",
            "Naoki Yoshinaga",
            "Graham Neubig",
            "Shoetsu Sato",
            "Masashi Toyoda",
            "Masaru Kitsuregawa"
        ],
        "paper_abstract": "When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Mining Discourse Markers for Unsupervised Sentence Representation Learning",
        "paper_url": "https://aclanthology.org/N19-1351/",
        "paper_authors": [
            "Damien Sileo",
            "Tim Van De Cruys",
            "Camille Pradel",
            "Philippe Muller"
        ],
        "paper_abstract": "Current state of the art systems in NLP heavily rely on manually annotated datasets, which are expensive to construct. Very little work adequately exploits unannotated data \u2013 such as discourse markers between sentences \u2013 mainly because of data sparseness and ineffective extraction methods. In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data. Our resulting dataset contains 174 discourse markers with at least 10k examples each, even for rare markers such as \u201ccoincidentally\u201d or \u201camazingly\u201d. We use the resulting data as supervision for learning transferable sentence embeddings. In addition, we show that even though sentence representation learning through prediction of discourse marker yields state of the art results across different transfer tasks, it\u2019s not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection",
        "paper_url": "https://aclanthology.org/N19-1352/",
        "paper_authors": [
            "Wenhu Chen",
            "Yu Su",
            "Yilin Shen",
            "Zhiyu Chen",
            "Xifeng Yan",
            "William Yang Wang"
        ],
        "paper_abstract": "With the rapid development in deep learning, deep neural networks have been widely adopted in many real-life natural language applications. Under deep neural networks, a pre-defined vocabulary is required to vectorize text inputs. The canonical approach to select pre-defined vocabulary is based on the word frequency, where a threshold is selected to cut off the long tail distribution. However, we observed that such a simple approach could easily lead to under-sized vocabulary or over-sized vocabulary issues. Therefore, we are interested in understanding how the end-task classification accuracy is related to the vocabulary size and what is the minimum required vocabulary size to achieve a specific performance. In this paper, we provide a more sophisticated variational vocabulary dropout (VVD) based on variational dropout to perform vocabulary selection, which can intelligently select the subset of the vocabulary to achieve the required performance. To evaluate different algorithms on the newly proposed vocabulary selection problem, we propose two new metrics: Area Under Accuracy-Vocab Curve and Vocab Size under X% Accuracy Drop. Through extensive experiments on various NLP classification tasks, our variational framework is shown to significantly outperform the frequency-based and other selection baselines on these metrics.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Subword-based Compact Reconstruction of Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-1353/",
        "paper_authors": [
            "Shota Sasaki",
            "Jun Suzuki",
            "Kentaro Inui"
        ],
        "paper_abstract": "The idea of subword-based word embeddings has been proposed in the literature, mainly for solving the out-of-vocabulary (OOV) word problem observed in standard word-based word embeddings. In this paper, we propose a method of reconstructing pre-trained word embeddings using subword information that can effectively represent a large number of subword embeddings in a considerably small fixed space. The key techniques of our method are twofold: memory-shared embeddings and a variant of the key-value-query self-attention mechanism. Our experiments show that our reconstructed subword-based embeddings can successfully imitate well-trained word embeddings in a small fixed space while preventing quality degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bayesian Learning for Neural Dependency Parsing",
        "paper_url": "https://aclanthology.org/N19-1354/",
        "paper_authors": [
            "Ehsan Shareghi",
            "Yingzhen Li",
            "Yi Zhu",
            "Roi Reichart",
            "Anna Korhonen"
        ],
        "paper_abstract": "While neural dependency parsers provide state-of-the-art accuracy for several languages, they still rely on large amounts of costly labeled training data. We demonstrate that in the small data regime, where uncertainty around parameter estimation and model prediction matters the most, Bayesian neural modeling is very effective. In order to overcome the computational and statistical costs of the approximate inference step in this framework, we utilize an efficient sampling procedure via stochastic gradient Langevin dynamics to generate samples from the approximated posterior. Moreover, we show that our Bayesian neural parser can be further improved when integrated into a multi-task parsing and POS tagging framework, designed to minimize task interference via an adversarial procedure. When trained and tested on 6 languages with less than 5k training instances, our parser consistently outperforms the strong bilstm baseline (Kiperwasser and Goldberg, 2016). Compared with the biaffine parser (Dozat et al., 2017) our model achieves an improvement of up to 3% for Vietnames and Irish, while our multi-task model achieves an improvement of up to 9% across five languages: Farsi, Russian, Turkish, Vietnamese, and Irish.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning",
        "paper_url": "https://aclanthology.org/N19-1355/",
        "paper_authors": [
            "Han Guo",
            "Ramakanth Pasunuru",
            "Mohit Bansal"
        ],
        "paper_abstract": "Multi-task learning (MTL) has achieved success over a wide range of problems, where the goal is to improve the performance of a primary task using a set of relevant auxiliary tasks. However, when the usefulness of the auxiliary tasks w.r.t. the primary task is not known a priori, the success of MTL models depends on the correct choice of these auxiliary tasks and also a balanced mixing ratio of these tasks during alternate training. These two problems could be resolved via manual intuition or hyper-parameter tuning over all combinatorial task choices, but this introduces inductive bias or is not scalable when the number of candidate auxiliary tasks is very large. To address these issues, we present AutoSeM, a two-stage MTL pipeline, where the first stage automatically selects the most useful auxiliary tasks via a Beta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stage learns the training mixing ratio of these selected auxiliary tasks via a Gaussian Process based Bayesian optimization framework. We conduct several MTL experiments on the GLUE language understanding tasks, and show that our AutoSeM framework can successfully find relevant auxiliary tasks and automatically learn their mixing ratio, achieving significant performance boosts on several primary tasks. Finally, we present ablations for each stage of AutoSeM and analyze the learned auxiliary task choices.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages",
        "paper_url": "https://aclanthology.org/N19-1356/",
        "paper_authors": [
            "Shauli Ravfogel",
            "Yoav Goldberg",
            "Tal Linzen"
        ],
        "paper_abstract": "How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs\u2019 syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Attention is not Explanation",
        "paper_url": "https://aclanthology.org/N19-1357/",
        "paper_authors": [
            "Sarthak Jain",
            "Byron C. Wallace"
        ],
        "paper_abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \u201cexplanations\u201d for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
        "paper_url": "https://aclanthology.org/N19-1358/",
        "paper_authors": [
            "Prithviraj Ammanabrolu",
            "Mark Riedl"
        ],
        "paper_abstract": "Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Information Aggregation for Multi-Head Attention with Routing-by-Agreement",
        "paper_url": "https://aclanthology.org/N19-1359/",
        "paper_authors": [
            "Jian Li",
            "Baosong Yang",
            "Zi-Yi Dou",
            "Xing Wang",
            "Michael R. Lyu",
            "Zhaopeng Tu"
        ],
        "paper_abstract": "Multi-head attention is appealing for its ability to jointly extract different types of information from multiple representation subspaces. Concerning the information aggregation, a common practice is to use a concatenation followed by a linear transformation, which may not fully exploit the expressiveness of multi-head attention. In this work, we propose to improve the information aggregation for multi-head attention with a more powerful routing-by-agreement algorithm. Specifically, the routing algorithm iteratively updates the proportion of how much a part (i.e. the distinct information learned from a specific subspace) should be assigned to a whole (i.e. the final output representation), based on the agreement between parts and wholes. Experimental results on linguistic probing tasks and machine translation tasks prove the superiority of the advanced information aggregation over the standard linear transformation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Context Dependent Semantic Parsing over Temporally Structured Data",
        "paper_url": "https://aclanthology.org/N19-1360/",
        "paper_authors": [
            "Charles Chen",
            "Razvan Bunescu"
        ],
        "paper_abstract": "We describe a new semantic parsing setting that allows users to query the system using both natural language questions and actions within a graphical user interface. Multiple time series belonging to an entity of interest are stored in a database and the user interacts with the system to obtain a better understanding of the entity\u2019s state and behavior, entailing sequences of actions and questions whose answers may depend on previous factual or navigational interactions. We design an LSTM-based encoder-decoder architecture that models context dependency through copying mechanisms and multiple levels of attention over inputs and previous outputs. When trained to predict tokens using supervised learning, the proposed architecture substantially outperforms standard sequence generation baselines. Training the architecture using policy gradient leads to further improvements in performance, reaching a sequence-level accuracy of 88.7% on artificial data and 74.8% on real data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Structural Scaffolds for Citation Intent Classification in Scientific Publications",
        "paper_url": "https://aclanthology.org/N19-1361/",
        "paper_authors": [
            "Arman Cohan",
            "Waleed Ammar",
            "Madeleine van Zuylen",
            "Field Cady"
        ],
        "paper_abstract": "Identifying the intent of a citation in scientific papers (e.g., background information, use of methods, comparing results) is critical for machine reading of individual publications and automated analysis of the scientific literature. We propose structural scaffolds, a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents. Our model achieves a new state-of-the-art on an existing ACL anthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without relying on external linguistic resources or hand-engineered features as done in existing methods. In addition, we introduce a new dataset of citation intents (SciCite) which is more than five times larger and covers multiple scientific domains compared with existing datasets. Our code and data are available at: https://github.com/allenai/scicite.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference",
        "paper_url": "https://aclanthology.org/N19-1362/",
        "paper_authors": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Omer Levy",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "paper_abstract": "Reasoning about implied relationships (e.g. paraphrastic, common sense, encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. Our pairwise embeddings are computed as a compositional function of each word\u2019s representation, which is learned by maximizing the pointwise mutual information (PMI) with the contexts in which the the two words co-occur. We add these representations to the cross-sentence attention layer of existing inference models (e.g. BiDAF for QA, ESIM for NLI), instead of extending or replacing existing word embeddings. Experiments show a gain of 2.7% on the recently released SQuAD 2.0 and 1.3% on MultiNLI. Our representations also aid in better generalization with gains of around 6-7% on adversarial SQuAD datasets, and 8.8% on the adversarial entailment test set by Glockner et al. (2018).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Submodular Optimization-based Diverse Paraphrasing and its Effectiveness in Data Augmentation",
        "paper_url": "https://aclanthology.org/N19-1363/",
        "paper_authors": [
            "Ashutosh Kumar",
            "Satwik Bhattamishra",
            "Manik Bhandari",
            "Partha Talukdar"
        ],
        "paper_abstract": "Inducing diversity in the task of paraphrasing is an important problem in NLP with applications in data augmentation and conversational agents. Previous paraphrasing approaches have mainly focused on the issue of generating semantically similar paraphrases while paying little attention towards diversity. In fact, most of the methods rely solely on top-k beam search sequences to obtain a set of paraphrases. The resulting set, however, contains many structurally similar sentences. In this work, we focus on the task of obtaining highly diverse paraphrases while not compromising on paraphrasing quality. We provide a novel formulation of the problem in terms of monotone submodular function maximization, specifically targeted towards the task of paraphrasing. Additionally, we demonstrate the effectiveness of our method for data augmentation on multiple tasks such as intent classification and paraphrase recognition. In order to drive further research, we have made the source code available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Let\u2019s Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms",
        "paper_url": "https://aclanthology.org/N19-1364/",
        "paper_authors": [
            "Diyi Yang",
            "Jiaao Chen",
            "Zichao Yang",
            "Dan Jurafsky",
            "Eduard Hovy"
        ],
        "paper_abstract": "Modeling what makes a request persuasive - eliciting the desired response from a reader - is critical to the study of propaganda, behavioral economics, and advertising. Yet current models can\u2019t quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of persuasion, we propose a neural network to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies - offering increased interpretability of persuasive speech - and has applications for other situations with document-level supervision but only partial sentence supervision.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Recursive Routing Networks: Learning to Compose Modules for Language Understanding",
        "paper_url": "https://aclanthology.org/N19-1365/",
        "paper_authors": [
            "Ignacio Cases",
            "Clemens Rosenbaum",
            "Matthew Riemer",
            "Atticus Geiger",
            "Tim Klinger",
            "Alex Tamkin",
            "Olivia Li",
            "Sandhini Agarwal",
            "Joshua D. Greene",
            "Dan Jurafsky",
            "Christopher Potts",
            "Lauri Karttunen"
        ],
        "paper_abstract": "We introduce Recursive Routing Networks (RRNs), which are modular, adaptable models that learn effectively in diverse environments. RRNs consist of a set of functions, typically organized into a grid, and a meta-learner decision-making component called the router. The model jointly optimizes the parameters of the functions and the meta-learner\u2019s policy for routing inputs through those functions. RRNs can be incorporated into existing architectures in a number of ways; we explore adding them to word representation layers, recurrent network hidden layers, and classifier layers. Our evaluation task is natural language inference (NLI). Using the MultiNLI corpus, we show that an RRN\u2019s routing decisions reflect the high-level genre structure of that corpus. To show that RRNs can learn to specialize to more fine-grained semantic distinctions, we introduce a new corpus of NLI examples involving implicative predicates, and show that the model components become fine-tuned to the inferential signatures that are characteristic of these predicates.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Structural Neural Encoders for AMR-to-text Generation",
        "paper_url": "https://aclanthology.org/N19-1366/",
        "paper_authors": [
            "Marco Damonte",
            "Shay B. Cohen"
        ],
        "paper_abstract": "AMR-to-text generation is a problem recently introduced to the NLP community, in which the goal is to generate sentences from Abstract Meaning Representation (AMR) graphs. Sequence-to-sequence models can be used to this end by converting the AMR graphs to strings. Approaching the problem while working directly with graphs requires the use of graph-to-sequence models that encode the AMR graph into a vector representation. Such encoding has been shown to be beneficial in the past, and unlike sequential encoding, it allows us to explicitly capture reentrant structures in the AMR graphs. We investigate the extent to which reentrancies (nodes with multiple parents) have an impact on AMR-to-text generation by comparing graph encoders to tree encoders, where reentrancies are not preserved. We show that improvements in the treatment of reentrancies and long-range dependencies contribute to higher overall scores for graph encoders. Our best model achieves 24.40 BLEU on LDC2015E86, outperforming the state of the art by 1.1 points and 24.54 BLEU on LDC2017T10, outperforming the state of the art by 1.24 points.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multilingual prediction of Alzheimer\u2019s disease through domain adaptation and concept-based language modelling",
        "paper_url": "https://aclanthology.org/N19-1367/",
        "paper_authors": [
            "Kathleen C. Fraser",
            "Nicklas Linz",
            "Bai Li",
            "Kristina Lundholm Fors",
            "Frank Rudzicz",
            "Alexandra K\u00f6nig",
            "Jan Alexandersson",
            "Philippe Robert",
            "Dimitrios Kokkinakis"
        ],
        "paper_abstract": "There is growing evidence that changes in speech and language may be early markers of dementia, but much of the previous NLP work in this area has been limited by the size of the available datasets. Here, we compare several methods of domain adaptation to augment a small French dataset of picture descriptions (n = 57) with a much larger English dataset (n = 550), for the task of automatically distinguishing participants with dementia from controls. The first challenge is to identify a set of features that transfer across languages; in addition to previously used features based on information units, we introduce a new set of features to model the order in which information units are produced by dementia patients and controls. These concept-based language model features improve classification performance in both English and French separately, and the best result (AUC = 0.89) is achieved using the multilingual training set with a combination of information and language model features.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs",
        "paper_url": "https://aclanthology.org/N19-1368/",
        "paper_authors": [
            "Debjit Paul",
            "Anette Frank"
        ],
        "paper_abstract": "To make machines better understand sentiments, research needs to move from polarity identification to understanding the reasons that underlie the expression of sentiment. Categorizing the goals or needs of humans is one way to explain the expression of sentiment in text. Humans are good at understanding situations described in natural language and can easily connect them to the character\u2019s psychological needs using commonsense knowledge. We present a novel method to extract, rank, filter and select multi-hop relation paths from a commonsense knowledge resource to interpret the expression of sentiment in terms of their underlying human needs. We efficiently integrate the acquired knowledge paths in a neural model that interfaces context representations with knowledge using a gated attention mechanism. We assess the model\u2019s performance on a recently published dataset for categorizing human needs. Selectively integrating knowledge paths boosts performance and establishes a new state-of-the-art. Our model offers interpretability through the learned attention map over commonsense knowledge paths. Human evaluation highlights the relevance of the encoded knowledge.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "NLP Whack-A-Mole: Challenges in Cross-Domain Temporal Expression Extraction",
        "paper_url": "https://aclanthology.org/N19-1369/",
        "paper_authors": [
            "Amy Olex",
            "Luke Maffey",
            "Bridget McInnes"
        ],
        "paper_abstract": "Incorporating domain knowledge is vital in building successful natural language processing (NLP) applications. Many times, cross-domain application of a tool results in poor performance as the tool does not account for domain-specific attributes. The clinical domain is challenging in this aspect due to specialized medical terms and nomenclature, shorthand notation, fragmented text, and a variety of writing styles used by different medical units. Temporal resolution is an NLP task that, in general, is domain-agnostic because temporal information is represented using a limited lexicon. However, domain-specific aspects of temporal resolution are present in clinical texts. Here we explore parsing issues that arose when running our system, a tool built on Newswire text, on clinical notes in the THYME corpus. Many parsing issues were straightforward to correct; however, a few code changes resulted in a cascading series of parsing errors that had to be resolved before an improvement in performance was observed, revealing the complexity temporal resolution and rule-based parsing. Our system now outperforms current state-of-the-art systems on the THYME corpus with little change in its performance on Newswire texts.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
        "paper_url": "https://aclanthology.org/N19-1370/",
        "paper_authors": [
            "Robin Jia",
            "Cliff Wong",
            "Hoifung Poon"
        ],
        "paper_abstract": "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system\u2019s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Inferring Which Medical Treatments Work from Reports of Clinical Trials",
        "paper_url": "https://aclanthology.org/N19-1371/",
        "paper_authors": [
            "Eric Lehman",
            "Jay DeYoung",
            "Regina Barzilay",
            "Byron C. Wallace"
        ],
        "paper_abstract": "How do we know if a particular medical treatment actually works? Ideally one would consult all available evidence from relevant clinical trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them. In this paper, we present a new task and corpus for making this unstructured published scientific evidence actionable. The task entails inferring reported findings from a full-text article describing randomized controlled trials (RCT) with respect to a given intervention, comparator, and outcome of interest, e.g., inferring if a given article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo. We present a new corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. Results using a suite of baseline models \u2014 ranging from heuristic (rule-based) approaches to attentive neural architectures \u2014 demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts. To facilitate further work on this important, challenging problem we make the corpus, documentation, a website and leaderboard, and all source code for baselines and evaluation publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Decay-Function-Free Time-Aware Attention to Context and Speaker Indicator for Spoken Language Understanding",
        "paper_url": "https://aclanthology.org/N19-1372/",
        "paper_authors": [
            "Jonggu Kim",
            "Jong-Hyeok Lee"
        ],
        "paper_abstract": "To capture salient contextual information for spoken language understanding (SLU) of a dialogue, we propose time-aware models that automatically learn the latent time-decay function of the history without a manual time-decay function. We also propose a method to identify and label the current speaker to improve the SLU accuracy. In experiments on the benchmark dataset used in Dialog State Tracking Challenge 4, the proposed models achieved significantly higher F1 scores than the state-of-the-art contextual models. Finally, we analyze the effectiveness of the introduced models in detail. The analysis demonstrates that the proposed methods were effective to improve SLU accuracy individually.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Dialogue Act Classification with Context-Aware Self-Attention",
        "paper_url": "https://aclanthology.org/N19-1373/",
        "paper_authors": [
            "Vipul Raheja",
            "Joel Tetreault"
        ],
        "paper_abstract": "Recent work in Dialogue Act classification has treated the task as a sequence labeling problem using hierarchical deep neural networks. We build on this prior work by leveraging the effectiveness of a context-aware self-attention mechanism coupled with a hierarchical recurrent neural network. We conduct extensive evaluations on standard Dialogue Act classification datasets and show significant improvement over state-of-the-art results on the Switchboard Dialogue Act (SwDA) Corpus. We also investigate the impact of different utterance-level representation learning methods and show that our method is effective at capturing utterance-level semantic text representations while maintaining high accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Affect-Driven Dialog Generation",
        "paper_url": "https://aclanthology.org/N19-1374/",
        "paper_authors": [
            "Pierre Colombo",
            "Wojciech Witon",
            "Ashutosh Modi",
            "James Kennedy",
            "Mubbasir Kapadia"
        ],
        "paper_abstract": "The majority of current systems for end-to-end dialog generation focus on response quality without an explicit control over the affective content of the responses. In this paper, we present an affect-driven dialog system, which generates emotional responses in a controlled manner using a continuous representation of emotions. The system achieves this by modeling emotions at a word and sequence level using: (1) a vector representation of the desired emotion, (2) an affect regularizer, which penalizes neutral words, and (3) an affect sampling method, which forces the neural network to generate diverse words that are emotionally relevant. During inference, we use a re-ranking procedure that aims to extract the most emotionally relevant responses using a human-in-the-loop optimization process. We study the performance of our system in terms of both quantitative (BLEU score and response diversity), and qualitative (emotional appropriateness) measures.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Level Memory for Task Oriented Dialogs",
        "paper_url": "https://aclanthology.org/N19-1375/",
        "paper_authors": [
            "Revanth Gangi Reddy",
            "Danish Contractor",
            "Dinesh Raghu",
            "Sachindra Joshi"
        ],
        "paper_abstract": "Recent end-to-end task oriented dialog systems use memory architectures to incorporate external knowledge in their dialogs. Current work makes simplifying assumptions about the structure of the knowledge base, such as the use of triples to represent knowledge, and combines dialog utterances (context) as well as knowledge base (KB) results as part of the same memory. This causes an explosion in the memory size, and makes the reasoning over memory harder. In addition, such a memory design forces hierarchical properties of the data to be fit into a triple structure of memory. This requires the memory reader to infer relationships across otherwise connected attributes. In this paper we relax the strong assumptions made by existing architectures and separate memories used for modeling dialog context and KB results. Instead of using triples to store KB results, we introduce a novel multi-level memory architecture consisting of cells for each query and their corresponding results. The multi-level memory first addresses queries, followed by results and finally each key-value pair within a result. We conduct detailed experiments on three publicly available task oriented dialog data sets and we find that our method conclusively outperforms current state-of-the-art models. We report a 15-25% increase in both entity F1 and BLEU scores.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Topic Spotting using Hierarchical Networks with Self Attention",
        "paper_url": "https://aclanthology.org/N19-1376/",
        "paper_authors": [
            "Pooja Chitkara",
            "Ashutosh Modi",
            "Pravalika Avvaru",
            "Sepehr Janghorbani",
            "Mubbasir Kapadia"
        ],
        "paper_abstract": "Success of deep learning techniques have renewed the interest in development of dialogue systems. However, current systems struggle to have consistent long term conversations with the users and fail to build rapport. Topic spotting, the task of automatically inferring the topic of a conversation, has been shown to be helpful in making dialog system more engaging and efficient. We propose a hierarchical model with self attention for topic spotting. Experiments on the Switchboard corpus show the superior performance of our model over previously proposed techniques for topic spotting and deep models for text classification. Additionally, in contrast to offline processing of dialog, we also analyze the performance of our model in a more realistic setting i.e. in an online setting where the topic is identified in real time as the dialog progresses. Results show that our model is able to generalize even with limited information in the online setting.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Top-Down Structurally-Constrained Neural Response Generation with Lexicalized Probabilistic Context-Free Grammar",
        "paper_url": "https://aclanthology.org/N19-1377/",
        "paper_authors": [
            "Wenchao Du",
            "Alan W Black"
        ],
        "paper_abstract": "We consider neural language generation under a novel problem setting: generating the words of a sentence according to the order of their first appearance in its lexicalized PCFG parse tree, in a depth-first, left-to-right manner. Unlike previous tree-based language generation methods, our approach is both (i) top-down and (ii) explicitly generating syntactic structure at the same time. In addition, our method combines neural model with symbolic approach: word choice at each step is constrained by its predicted syntactic function. We applied our model to the task of dialog response generation, and found it significantly improves over sequence-to-sequence baseline, in terms of diversity and relevance. We also investigated the effect of lexicalization on language generation, and found that lexicalization schemes that give priority to content words have certain advantages over those focusing on dependency relations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue",
        "paper_url": "https://aclanthology.org/N19-1378/",
        "paper_authors": [
            "Laura Aina",
            "Carina Silberer",
            "Ionut-Teodor Sorodoc",
            "Matthijs Westera",
            "Gemma Boleda"
        ],
        "paper_abstract": "Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Continuous Learning for Large-scale Personalized Domain Classification",
        "paper_url": "https://aclanthology.org/N19-1379/",
        "paper_authors": [
            "Han Li",
            "Jihwan Lee",
            "Sidharth Mudgal",
            "Ruhi Sarikaya",
            "Young-Bum Kim"
        ],
        "paper_abstract": "Domain classification is the task to map spoken language utterances to one of the natural language understanding domains in intelligent personal digital assistants (IPDAs). This is observed in mainstream IPDAs in industry and third-party domains are developed to enhance the capability of the IPDAs. As more and more new domains are developed very frequently, how to continuously accommodate the new domains still remains challenging. Moreover, if one wants to use personalized information dynamically for better domain classification, it is infeasible to directly adopt existing continual learning approaches. In this paper, we propose CoNDA, a neural-based approach for continuous domain adaption with normalization and regularization. Unlike existing methods that often conduct full model parameter update, CoNDA only updates the necessary parameters in the model for the new domains. Empirical evaluation shows that CoNDA achieves high accuracy on both the accommodated new domains and the existing known domains for which input samples come with personal information, and outperforms the baselines by a large margin.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog",
        "paper_url": "https://aclanthology.org/N19-1380/",
        "paper_authors": [
            "Sebastian Schuster",
            "Sonal Gupta",
            "Rushin Shah",
            "Mike Lewis"
        ],
        "paper_abstract": "One of the first steps in the utterance interpretation pipeline of many task-oriented conversational AI systems is to identify user intents and the corresponding slots. Since data collection for machine learning models for this task is time-consuming, it is desirable to make use of existing data in a high-resource language to train models in low-resource languages. However, development of such models has largely been hindered by the lack of multilingual training data. In this paper, we present a new data set of 57k annotated utterances in English (43k), Spanish (8.6k) and Thai (5k) across the domains weather, alarm, and reminder. We use this data set to evaluate three different cross-lingual transfer methods: (1) translating the training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel method of using a multilingual machine translation encoder as contextual word representations. We find that given several hundred training examples in the the target language, the latter two methods outperform translating the training data. Further, in very low-resource settings, multilingual contextual word representations give better results than using cross-lingual static embeddings. We also compare the cross-lingual methods to using monolingual resources in the form of contextual ELMo representations and find that given just small amounts of target language data, this method outperforms all cross-lingual methods, which highlights the need for more sophisticated cross-lingual methods.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Evaluating Coherence in Dialogue Systems using Entailment",
        "paper_url": "https://aclanthology.org/N19-1381/",
        "paper_authors": [
            "Nouha Dziri",
            "Ehsan Kamalloo",
            "Kory Mathewson",
            "Osmar Zaiane"
        ],
        "paper_abstract": "Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On Knowledge distillation from complex networks for response prediction",
        "paper_url": "https://aclanthology.org/N19-1382/",
        "paper_authors": [
            "Siddhartha Arora",
            "Mitesh M. Khapra",
            "Harish G. Ramaswamy"
        ],
        "paper_abstract": "Recent advances in Question Answering have lead to the development of very complex models which compute rich representations for query and documents by capturing all pairwise interactions between query and document words. This makes these models expensive in space and time, and in practice one has to restrict the length of the documents that can be fed to these models. Such models have also been recently employed for the task of predicting dialog responses from available background documents (e.g., Holl-E dataset). However, here the documents are longer, thereby rendering these complex models infeasible except in select restricted settings. In order to overcome this, we use standard simple models which do not capture all pairwise interactions, but learn to emulate certain characteristics of a complex teacher network. Specifically, we first investigate the conicity of representations learned by a complex model and observe that it is significantly lower than that of simpler models. Based on this insight, we modify the simple architecture to mimic this characteristic. We go further by using knowledge distillation approaches, where the simple model acts as a student and learns to match the output from the complex teacher network. We experiment with the Holl-E dialog data set and show that by mimicking characteristics and matching outputs from a teacher, even a simple network can give improved performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource Name Tagging",
        "paper_url": "https://aclanthology.org/N19-1383/",
        "paper_authors": [
            "Lifu Huang",
            "Heng Ji",
            "Jonathan May"
        ],
        "paper_abstract": "We focus on improving name tagging for low-resource languages using annotations from related languages. Previous studies either directly project annotations from a source language to a target language using cross-lingual representations or use a shared encoder in a multitask network to transfer knowledge. These approaches inevitably introduce noise to the target language annotation due to mismatched source-target sentence structures. To effectively transfer the resources, we develop a new neural architecture that leverages multi-level adversarial transfer: (1) word-level adversarial training, which projects source language words into the same semantic space as those of the target language without using any parallel corpora or bilingual gazetteers, and (2) sentence-level adversarial training, which yields language-agnostic sequential features. Our neural architecture outperforms previous approaches on CoNLL data sets. Moreover, on 10 low-resource languages, our approach achieves up to 16% absolute F-score gain over all high-performing baselines on cross-lingual transfer without using any target-language resources.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Unsupervised Extraction of Partial Translations for Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1384/",
        "paper_authors": [
            "Benjamin Marie",
            "Atsushi Fujita"
        ],
        "paper_abstract": "In neural machine translation (NMT), monolingual data are usually exploited through a so-called back-translation: sentences in the target language are translated into the source language to synthesize new parallel data. While this method provides more training data to better model the target language, on the source side, it only exploits translations that the NMT system is already able to generate using a model trained on existing parallel data. In this work, we assume that new translation knowledge can be extracted from monolingual data, without relying at all on existing parallel data. We propose a new algorithm for extracting from monolingual data what we call partial translations: pairs of source and target sentences that contain sequences of tokens that are translations of each other. Our algorithm is fully unsupervised and takes only source and target monolingual data as input. Our empirical evaluation points out that our partial translations can be used in combination with back-translation to further improve NMT models. Furthermore, while partial translations are particularly useful for low-resource language pairs, they can also be successfully exploited in resource-rich scenarios to improve translation quality.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Low-Resource Syntactic Transfer with Unsupervised Source Reordering",
        "paper_url": "https://aclanthology.org/N19-1385/",
        "paper_authors": [
            "Mohammad Sadegh Rasooli",
            "Michael Collins"
        ],
        "paper_abstract": "We describe a cross-lingual transfer method for dependency parsing that takes into account the problem of word order differences between source and target languages. Our model only relies on the Bible, a considerably smaller parallel data than the commonly used parallel data in transfer methods. We use the concatenation of projected trees from the Bible corpus, and the gold-standard treebanks in multiple source languages along with cross-lingual word representations. We demonstrate that reordering the source treebanks before training on them for a target language improves the accuracy of languages outside the European language family. Our experiments on 68 treebanks (38 languages) in the Universal Dependencies corpus achieve a high accuracy for all languages. Among them, our experiments on 16 treebanks of 12 non-European languages achieve an average UAS absolute improvement of 3.3% over a state-of-the-art method.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Revisiting Adversarial Autoencoder for Unsupervised Word Translation with Cycle Consistency and Improved Training",
        "paper_url": "https://aclanthology.org/N19-1386/",
        "paper_authors": [
            "Tasnim Mohiuddin",
            "Shafiq Joty"
        ],
        "paper_abstract": "Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages",
        "paper_url": "https://aclanthology.org/N19-1387/",
        "paper_authors": [
            "Rudra Murthy",
            "Anoop Kunchukuttan",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality in extremely low-resource scenarios.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Massively Multilingual Neural Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1388/",
        "paper_authors": [
            "Roee Aharoni",
            "Melvin Johnson",
            "Orhan Firat"
        ],
        "paper_abstract": "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Large-Scale Comparison of Historical Text Normalization Systems",
        "paper_url": "https://aclanthology.org/N19-1389/",
        "paper_authors": [
            "Marcel Bollmann"
        ],
        "paper_abstract": "There is no consensus on the state-of-the-art approach to historical text normalization. Many techniques have been proposed, including rule-based methods, distance metrics, character-based statistical machine translation, and neural encoder\u2013decoder models, but studies have used different datasets, different evaluation methods, and have come to different conclusions. This paper presents the largest study of historical text normalization done so far. We critically survey the existing literature and report experiments on eight languages, comparing systems spanning all categories of proposed normalization techniques, analysing the effect of training data quantity, and using different evaluation methods. The datasets and scripts are made publicly available.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Combining Discourse Markers and Cross-lingual Embeddings for Synonym\u2013Antonym Classification",
        "paper_url": "https://aclanthology.org/N19-1390/",
        "paper_authors": [
            "Michael Roth",
            "Shyam Upadhyay"
        ],
        "paper_abstract": "It is well-known that distributional semantic approaches have difficulty in distinguishing between synonyms and antonyms (Grefenstette, 1992; Pad\u00f3 and Lapata, 2003). Recent work has shown that supervision available in English for this task (e.g., lexical resources) can be transferred to other languages via cross-lingual word embeddings. However, this kind of transfer misses monolingual distributional information available in a target language, such as contrast relations that are indicative of antonymy (e.g. hot ... while ... cold). In this work, we improve the transfer by exploiting monolingual information, expressed in the form of co-occurrences with discourse markers that convey contrast. Our approach makes use of less than a dozen markers, which can easily be obtained for many languages. Compared to a baseline using only cross-lingual embeddings, we show absolute improvements of 4\u201310% F1-score in Vietnamese and Hindi.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Context-Aware Cross-Lingual Mapping",
        "paper_url": "https://aclanthology.org/N19-1391/",
        "paper_authors": [
            "Hanan Aldarmaki",
            "Mona Diab"
        ],
        "paper_abstract": "Cross-lingual word vectors are typically obtained by fitting an orthogonal matrix that maps the entries of a bilingual dictionary from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate context in the transformation matrix by directly mapping the averaged embeddings of aligned sentences in a parallel corpus. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Polyglot Contextual Representations Improve Crosslingual Transfer",
        "paper_url": "https://aclanthology.org/N19-1392/",
        "paper_authors": [
            "Phoebe Mulcaire",
            "Jungo Kasai",
            "Noah A. Smith"
        ],
        "paper_abstract": "We introduce Rosita, a method to produce multilingual contextual word representations by training a single language model on text from multiple languages. Our method combines the advantages of contextual word representations with those of multilingual representation learning. We produce language models from dissimilar language pairs (English/Arabic and English/Chinese) and use them in dependency parsing, semantic role labeling, and named entity recognition, with comparisons to monolingual and non-contextual variants. Our results provide further evidence for the benefits of polyglot learning, in which representations are shared across multiple languages.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Typological Features for Multilingual Delexicalised Dependency Parsing",
        "paper_url": "https://aclanthology.org/N19-1393/",
        "paper_authors": [
            "Manon Scholivet",
            "Franck Dary",
            "Alexis Nasr",
            "Benoit Favre",
            "Carlos Ramisch"
        ],
        "paper_abstract": "The existence of universal models to describe the syntax of languages has been debated for decades. The availability of resources such as the Universal Dependencies treebanks and the World Atlas of Language Structures make it possible to study the plausibility of universal grammar from the perspective of dependency parsing. Our work investigates the use of high-level language descriptions in the form of typological features for multilingual dependency parsing. Our experiments on multilingual parsing for 40 languages show that typological information can indeed guide parsers to share information between similar languages beyond simple language identification.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Recommendations for Datasets for Source Code Summarization",
        "paper_url": "https://aclanthology.org/N19-1394/",
        "paper_authors": [
            "Alexander LeClair",
            "Collin McMillan"
        ],
        "paper_abstract": "Source Code Summarization is the task of writing short, natural language descriptions of source code. The main use for these descriptions is in software documentation e.g. the one-sentence Java method descriptions in JavaDocs. Code summarization is rapidly becoming a popular research problem, but progress is restrained due to a lack of suitable datasets. In addition, a lack of community standards for creating datasets leads to confusing and unreproducible research results \u2013 we observe swings in performance of more than 33% due only to changes in dataset design. In this paper, we make recommendations for these standards from experimental results. We release a dataset based on prior work of over 2.1m pairs of Java methods and one sentence method descriptions from over 28k Java projects. We describe the dataset and point out key differences from natural language data, to guide and support future researchers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Question Answering as an Automatic Evaluation Metric for News Article Summarization",
        "paper_url": "https://aclanthology.org/N19-1395/",
        "paper_authors": [
            "Matan Eyal",
            "Tal Baumel",
            "Michael Elhadad"
        ],
        "paper_abstract": "Recent work in the field of automatic summarization and headline generation focuses on maximizing ROUGE scores for various news datasets. We present an alternative, extrinsic, evaluation metric for this task, Answering Performance for Evaluation of Summaries. APES utilizes recent progress in the field of reading-comprehension to quantify the ability of a summary to answer a set of manually created questions regarding central entities in the source article. We first analyze the strength of this metric by comparing it to known manual evaluation metrics. We then present an end-to-end neural abstractive model that maximizes APES, while increasing ROUGE scores to competitive results.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding the Behaviour of Neural Abstractive Summarizers using Contrastive Examples",
        "paper_url": "https://aclanthology.org/N19-1396/",
        "paper_authors": [
            "Krtin Kumar",
            "Jackie Chi Kit Cheung"
        ],
        "paper_abstract": "Neural abstractive summarizers generate summary texts using a language model conditioned on the input source text, and have recently achieved high ROUGE scores on benchmark summarization datasets. We investigate how they achieve this performance with respect to human-written gold-standard abstracts, and whether the systems are able to understand deeper syntactic and semantic structures. We generate a set of contrastive summaries which are perturbed, deficient versions of human-written summaries, and test whether existing neural summarizers score them more highly than the human-written summaries. We analyze their performance on different datasets and find that these systems fail to understand the source text, in a majority of the cases.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Jointly Extracting and Compressing Documents with Summary State Representations",
        "paper_url": "https://aclanthology.org/N19-1397/",
        "paper_authors": [
            "Afonso Mendes",
            "Shashi Narayan",
            "Sebasti\u00e3o Miranda",
            "Zita Marinho",
            "Andr\u00e9 F. T. Martins",
            "Shay B. Cohen"
        ],
        "paper_abstract": "We present a new neural model for text summarization that first extracts sentences from a document and then compresses them. The pro-posed model offers a balance that sidesteps thedifficulties in abstractive methods while gener-ating more concise summaries than extractivemethods. In addition, our model dynamically determines the length of the output summary based on the gold summaries it observes during training and does not require length constraints typical to extractive summarization. The model achieves state-of-the-art results on the CNN/DailyMail and Newsroom datasets, improving over current extractive and abstractive methods. Human evaluations demonstratethat our model generates concise and informa-tive summaries. We also make available a new dataset of oracle compressive summaries derived automatically from the CNN/DailyMailreference summaries.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "News Article Teaser Tweets and How to Generate Them",
        "paper_url": "https://aclanthology.org/N19-1398/",
        "paper_authors": [
            "Sanjeev Kumar Karn",
            "Mark Buckley",
            "Ulli Waltinger",
            "Hinrich Sch\u00fctze"
        ],
        "paper_abstract": "In this work, we define the task of teaser generation and provide an evaluation benchmark and baseline systems for the process of generating teasers. A teaser is a short reading suggestion for an article that is illustrative and includes curiosity-arousing elements to entice potential readers to read particular news items. Teasers are one of the main vehicles for transmitting news to social media users. We compile a novel dataset of teasers by systematically accumulating tweets and selecting those that conform to the teaser definition. We have compared a number of neural abstractive architectures on the task of teaser generation and the overall best performing system is See et al. seq2seq with pointer network.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-referencing Using Fine-grained Topic Modeling",
        "paper_url": "https://aclanthology.org/N19-1399/",
        "paper_authors": [
            "Jeffrey Lund",
            "Piper Armstrong",
            "Wilson Fearn",
            "Stephen Cowley",
            "Emily Hales",
            "Kevin Seppi"
        ],
        "paper_abstract": "Cross-referencing, which links passages of text to other related passages, can be a valuable study aid for facilitating comprehension of a text. However, cross-referencing requires first, a comprehensive thematic knowledge of the entire corpus, and second, a focused search through the corpus specifically to find such useful connections. Due to this, cross-reference resources are prohibitively expensive and exist only for the most well-studied texts (e.g. religious texts). We develop a topic-based system for automatically producing candidate cross-references which can be easily verified by human annotators. Our system utilizes fine-grained topic modeling with thousands of highly nuanced and specific topics to identify verse pairs which are topically related. We demonstrate that our system can be cost effective compared to having annotators acquire the expertise necessary to produce cross-reference resources unaided.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Conversation Initiation by Diverse News Contents Introduction",
        "paper_url": "https://aclanthology.org/N19-1400/",
        "paper_authors": [
            "Satoshi Akasaki",
            "Nobuhiro Kaji"
        ],
        "paper_abstract": "In our everyday chit-chat, there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user\u2019s utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel large-scale dataset through crowd-sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Positional Encoding to Control Output Sequence Length",
        "paper_url": "https://aclanthology.org/N19-1401/",
        "paper_authors": [
            "Sho Takase",
            "Naoaki Okazaki"
        ],
        "paper_abstract": "Neural encoder-decoder models have been successful in natural language generation tasks. However, real applications of abstractive summarization must consider an additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) so that a neural encoder-decoder model preserves the length constraint. Unlike previous studies that learn length embeddings, the proposed method can generate a text of any length even if the target length is unseen in training data. The experimental results show that the proposed method is able not only to control generation length but also improve ROUGE scores.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The Lower The Simpler: Simplifying Hierarchical Recurrent Models",
        "paper_url": "https://aclanthology.org/N19-1402/",
        "paper_authors": [
            "Chao Wang",
            "Hui Jiang"
        ],
        "paper_abstract": "To improve the training efficiency of hierarchical recurrent models without compromising their performance, we propose a strategy named as \u201cthe lower the simpler\u201d, which is to simplify the baseline models by making the lower layers simpler than the upper layers. We carry out this strategy to simplify two typical hierarchical recurrent models, namely Hierarchical Recurrent Encoder-Decoder (HRED) and R-NET, whose basic building block is GRU. Specifically, we propose Scalar Gated Unit (SGU), which is a simplified variant of GRU, and use it to replace the GRUs at the middle layers of HRED and R-NET. Besides, we also use Fixed-size Ordinally-Forgetting Encoding (FOFE), which is an efficient encoding method without any trainable parameter, to replace the GRUs at the bottom layers of HRED and R-NET. The experimental results show that the simplified HRED and the simplified R-NET contain significantly less trainable parameters, consume significantly less training time, and achieve slightly better performance than their baseline models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Using Natural Language Relations between Answer Choices for Machine Comprehension",
        "paper_url": "https://aclanthology.org/N19-1403/",
        "paper_authors": [
            "Rajkumar Pujari",
            "Dan Goldwasser"
        ],
        "paper_abstract": "While evaluating an answer choice for Reading Comprehension task, other answer choices available for the question and the answers of related questions about the same paragraph often provide valuable information. In this paper, we propose a method to leverage the natural language relations between the answer choices, such as entailment and contradiction, to improve the performance of machine comprehension. We use a stand-alone question answering (QA) system to perform QA task and a Natural Language Inference (NLI) system to identify the relations between the choice pairs. Then we perform inference using an Integer Linear Programming (ILP)-based relational framework to re-evaluate the decisions made by the standalone QA system in light of the relations identified by the NLI system. We also propose a multitask learning model that learns both the tasks jointly.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Saliency Learning: Teaching the Model Where to Pay Attention",
        "paper_url": "https://aclanthology.org/N19-1404/",
        "paper_authors": [
            "Reza Ghaeini",
            "Xiaoli Fern",
            "Hamed Shahbazi",
            "Prasad Tadepalli"
        ],
        "paper_abstract": "Deep learning has emerged as a compelling solution to many NLP tasks with remarkable performances. However, due to their opacity, such models are hard to interpret and trust. Recent work on explaining deep models has introduced approaches to provide insights toward the model\u2019s behaviour and predictions, which are helpful for assessing the reliability of the model\u2019s predictions. However, such methods do not improve the model\u2019s reliability. In this paper, we aim to teach the model to make the right prediction for the right reason by providing explanation training and ensuring the alignment of the model\u2019s explanation with the ground truth explanation. Our experimental results on multiple tasks and datasets demonstrate the effectiveness of the proposed method, which produces more reliable predictions while delivering better results compared to traditionally trained models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding Dataset Design Choices for Multi-hop Reasoning",
        "paper_url": "https://aclanthology.org/N19-1405/",
        "paper_authors": [
            "Jifan Chen",
            "Greg Durrett"
        ],
        "paper_abstract": "Learning multi-hop reasoning has been a key challenge for reading comprehension models, leading to the design of datasets that explicitly focus on it. Ideally, a model should not be able to perform well on a multi-hop question answering task without doing multi-hop reasoning. In this paper, we investigate two recently proposed datasets, WikiHop and HotpotQA. First, we explore sentence-factored models for these tasks; by design, these models cannot do multi-hop reasoning, but they are still able to solve a large number of examples in both datasets. Furthermore, we find spurious correlations in the unmasked version of WikiHop, which make it easy to achieve high performance considering only the questions and answers. Finally, we investigate one key difference between these datasets, namely span-based vs. multiple-choice formulations of the QA task. Multiple-choice versions of both datasets can be easily gamed, and two models we examine only marginally exceed a baseline in this setting. Overall, while these datasets are useful testbeds, high-performing models may not be learning as much multi-hop reasoning as previously thought.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Grammatical Error Correction with Finite State Transducers",
        "paper_url": "https://aclanthology.org/N19-1406/",
        "paper_authors": [
            "Felix Stahlberg",
            "Christopher Bryant",
            "Bill Byrne"
        ],
        "paper_abstract": "Grammatical error correction (GEC) is one of the areas in natural language processing in which purely neural models have not yet superseded more traditional symbolic models. Hybrid systems combining phrase-based statistical machine translation (SMT) and neural sequence models are currently among the most effective approaches to GEC. However, both SMT and neural sequence-to-sequence models require large amounts of annotated data. Language model based GEC (LM-GEC) is a promising alternative which does not rely on annotated training data. We show how to improve LM-GEC by applying modelling techniques based on finite state transducers. We report further gains by rescoring with neural language models. We show that our methods developed for LM-GEC can also be used with SMT systems if annotated training data is available. Our best system outperforms the best published result on the CoNLL-2014 test set, and achieves far better relative improvements over the SMT baselines than previous hybrid systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Convolutional Self-Attention Networks",
        "paper_url": "https://aclanthology.org/N19-1407/",
        "paper_authors": [
            "Baosong Yang",
            "Longyue Wang",
            "Derek F. Wong",
            "Lidia S. Chao",
            "Zhaopeng Tu"
        ],
        "paper_abstract": "Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Rethinking Complex Neural Network Architectures for Document Classification",
        "paper_url": "https://aclanthology.org/N19-1408/",
        "paper_authors": [
            "Ashutosh Adhikari",
            "Achyudh Ram",
            "Raphael Tang",
            "Jimmy Lin"
        ],
        "paper_abstract": "Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pre-trained language model representations for language generation",
        "paper_url": "https://aclanthology.org/N19-1409/",
        "paper_authors": [
            "Sergey Edunov",
            "Alexei Baevski",
            "Michael Auli"
        ],
        "paper_abstract": "Pre-trained language model representations have been successful in a wide range of language understanding tasks. In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization. We find that pre-trained representations are most effective when added to the encoder network which slows inference by only 14%. Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available. Finally, on abstractive summarization we achieve a new state of the art on the full text version of CNN/DailyMail.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pragmatically Informative Text Generation",
        "paper_url": "https://aclanthology.org/N19-1410/",
        "paper_authors": [
            "Sheng Shen",
            "Daniel Fried",
            "Jacob Andreas",
            "Dan Klein"
        ],
        "paper_abstract": "We improve the informativeness of models for conditional text generation using techniques from computational pragmatics. These techniques formulate language production as a game between speakers and listeners, in which a speaker should generate output text that a listener can use to correctly identify the original input that the text describes. While such approaches are widely used in cognitive science and grounded language learning, they have received less attention for more standard language generation tasks. We consider two pragmatic modeling methods for text generation: one where pragmatics is imposed by information preservation, and another where pragmatics is imposed by explicit modeling of distractors. We find that these methods improve the performance of strong existing systems for abstractive summarization and generation from structured meaning representations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation",
        "paper_url": "https://aclanthology.org/N19-1411/",
        "paper_authors": [
            "Hareesh Bahuleyan",
            "Lili Mou",
            "Hao Zhou",
            "Olga Vechtomova"
        ],
        "paper_abstract": "The variational autoencoder (VAE) imposes a probabilistic distribution (typically Gaussian) on the latent space and penalizes the Kullback-Leibler (KL) divergence between the posterior and prior. In NLP, VAEs are extremely difficult to train due to the problem of KL collapsing to zero. One has to implement various heuristics such as KL weight annealing and word dropout in a carefully engineered manner to successfully train a VAE for text. In this paper, we propose to use the Wasserstein autoencoder (WAE) for probabilistic sentence generation, where the encoder could be either stochastic or deterministic. We show theoretically and empirically that, in the original WAE, the stochastically encoded Gaussian distribution tends to become a Dirac-delta function, and we propose a variant of WAE that encourages the stochasticity of the encoder. Experimental results show that the latent space learned by WAE exhibits properties of continuity and smoothness as in VAEs, while simultaneously achieving much higher BLEU scores for sentence reconstruction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Benchmarking Hierarchical Script Knowledge",
        "paper_url": "https://aclanthology.org/N19-1412/",
        "paper_authors": [
            "Yonatan Bisk",
            "Jan Buys",
            "Karl Pichotta",
            "Yejin Choi"
        ],
        "paper_abstract": "Understanding procedural language requires reasoning about both hierarchical and temporal relations between events. For example, \u201cboiling pasta\u201d is a sub-event of \u201cmaking a pasta dish\u201d, typically happens before \u201cdraining pasta,\u201d and requires the use of omitted tools (e.g. a strainer, sink...). While people are able to choose when and how to use abstract versus concrete instructions, the NLP community lacks corpora and tasks for evaluating if our models can do the same. In this paper, we introduce KidsCook, a parallel script corpus, as well as a cloze task which matches video captions with missing procedural details. Experimental results show that state-of-the-art models struggle at this task, which requires inducing functional commonsense knowledge not explicitly stated in text.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A large-scale study of the effects of word frequency and predictability in naturalistic reading",
        "paper_url": "https://aclanthology.org/N19-1413/",
        "paper_authors": [
            "Cory Shain"
        ],
        "paper_abstract": "A number of psycholinguistic studies have factorially manipulated words\u2019 contextual predictabilities and corpus frequencies and shown separable effects of each on measures of human sentence processing, a pattern which has been used to support distinct mechanisms underlying prediction on the one hand and lexical retrieval on the other. This paper examines the generalizability of this finding to more realistic conditions of sentence processing by studying effects of frequency and predictability in three large-scale naturalistic reading corpora. Results show significant effects of word frequency and predictability in isolation but no effect of frequency over and above predictability, and thus do not provide evidence of distinct mechanisms. The non-replication of separable effects in a naturalistic setting raises doubts about the existence of such a distinction in everyday sentence comprehension. Instead, these results are consistent with previous claims that apparent effects of frequency are underlyingly effects of predictability.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Augmenting word2vec with latent Dirichlet allocation within a clinical application",
        "paper_url": "https://aclanthology.org/N19-1414/",
        "paper_authors": [
            "Akshay Budhkar",
            "Frank Rudzicz"
        ],
        "paper_abstract": "This paper presents three hybrid models that directly combine latent Dirichlet allocation and word embedding for distinguishing between speakers with and without Alzheimer\u2019s disease from transcripts of picture descriptions. Two of our models get F-scores over the current state-of-the-art using automatic methods on the DementiaBank dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On the Idiosyncrasies of the Mandarin Chinese Classifier System",
        "paper_url": "https://aclanthology.org/N19-1415/",
        "paper_authors": [
            "Shijia Liu",
            "Hongyuan Mei",
            "Adina Williams",
            "Ryan Cotterell"
        ],
        "paper_abstract": "While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the classifiers modify. Using the empirical distribution of classifiers from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual information (in bits) between the distribution over classifiers and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging",
        "paper_url": "https://aclanthology.org/N19-1416/",
        "paper_authors": [
            "Sara Meftah",
            "Youssef Tamaazousti",
            "Nasredine Semmar",
            "Hassane Essafi",
            "Fatiha Sadat"
        ],
        "paper_abstract": "Fine-tuning neural networks is widely used to transfer valuable knowledge from high-resource to low-resource domains. In a standard fine-tuning scheme, source and target problems are trained using the same architecture. Although capable of adapting to new domains, pre-trained units struggle with learning uncommon target-specific patterns. In this paper, we propose to augment the target-network with normalised, weighted and randomly initialised units that beget a better adaptation while maintaining the valuable source knowledge. Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Show Some Love to Your n-grams: A Bit of Progress and Stronger n-gram Language Modeling Baselines",
        "paper_url": "https://aclanthology.org/N19-1417/",
        "paper_authors": [
            "Ehsan Shareghi",
            "Daniela Gerz",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ],
        "paper_abstract": "In recent years neural language models (LMs) have set the state-of-the-art performance for several benchmarking datasets. While the reasons for their success and their computational demand are well-documented, a comparison between neural models and more recent developments in n-gram models is neglected. In this paper, we examine the recent progress in n-gram literature, running experiments on 50 languages covering all morphological language families. Experimental results illustrate that a simple extension of Modified Kneser-Ney outperforms an lstm language model on 42 languages while a word-level Bayesian n-gram LM (Shareghi et al., 2017) outperforms the character-aware neural model (Kim et al., 2016) on average across all languages, and its extension which explicitly injects linguistic knowledge (Gerz et al., 2018) on 8 languages. Further experiments on larger Europarl datasets for 3 languages indicate that neural architectures are able to outperform computationally much cheaper n-gram models: n-gram training is up to 15,000x quicker. Our experiments illustrate that standalone n-gram models lend themselves as natural choices for resource-lean or morphologically rich languages, while the recent progress has significantly improved their accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Data Augmentation for Context-Sensitive Neural Lemmatization Using Inflection Tables and Raw Text",
        "paper_url": "https://aclanthology.org/N19-1418/",
        "paper_authors": [
            "Toms Bergmanis",
            "Sharon Goldwater"
        ],
        "paper_abstract": "Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Using context can help, both for unseen and ambiguous words. Yet most context-sensitive approaches require full lemma-annotated sentences for training, which may be scarce or unavailable in low-resource languages. In addition (as shown here), in a low-resource setting, a lemmatizer can learn more from n labeled examples of distinct words (types) than from n (contiguous) labeled tokens, since the latter contain far fewer distinct types. To combine the efficiency of type-based learning with the benefits of context, we propose a way to train a context-sensitive lemmatizer with little or no labeled corpus data, using inflection tables from the UniMorph project and raw text examples from Wikipedia that provide sentence contexts for the unambiguous UniMorph examples. Despite these being unambiguous examples, the model successfully generalizes from them, leading to improved results (both overall, and especially on unseen words) in comparison to a baseline that does not use context.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Structural Probe for Finding Syntax in Word Representations",
        "paper_url": "https://aclanthology.org/N19-1419/",
        "paper_authors": [
            "John Hewitt",
            "Christopher D. Manning"
        ],
        "paper_abstract": "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network\u2019s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models\u2019 vector geometry.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CNM: An Interpretable Complex-valued Network for Matching",
        "paper_url": "https://aclanthology.org/N19-1420/",
        "paper_authors": [
            "Qiuchi Li",
            "Benyou Wang",
            "Massimo Melucci"
        ],
        "paper_abstract": "This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
        "paper_url": "https://aclanthology.org/N19-1421/",
        "paper_authors": [
            "Alon Talmor",
            "Jonathan Herzig",
            "Nicholas Lourie",
            "Jonathan Berant"
        ],
        "paper_abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Probing the Need for Visual Context in Multimodal Machine Translation",
        "paper_url": "https://aclanthology.org/N19-1422/",
        "paper_authors": [
            "Ozan Caglayan",
            "Pranava Madhyastha",
            "Lucia Specia",
            "Lo\u00efc Barrault"
        ],
        "paper_abstract": "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "paper_url": "https://aclanthology.org/N19-1423/",
        "paper_authors": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "paper_abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "What\u2019s in a Name? Reducing Bias in Bios without Access to Protected Attributes",
        "paper_url": "https://aclanthology.org/N19-1424/",
        "paper_authors": [
            "Alexey Romanov",
            "Maria De-Arteaga",
            "Hanna Wallach",
            "Jennifer Chayes",
            "Christian Borgs",
            "Alexandra Chouldechova",
            "Sahin Geyik",
            "Krishnaram Kenthapadi",
            "Anna Rumshisky",
            "Adam Kalai"
        ],
        "paper_abstract": "There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual\u2019s true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals\u2019 names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier\u2019s overall true positive rate.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)",
        "paper_url": "https://aclanthology.org/N19-2000/",
        "paper_authors": [
            "Anastassia Loukina",
            "Michelle Morales",
            "Rohit Kumar"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enabling Real-time Neural IME with Incremental Vocabulary Selection",
        "paper_url": "https://aclanthology.org/N19-2001/",
        "paper_authors": [
            "Jiali Yao",
            "Raphael Shu",
            "Xinjian Li",
            "Katsutoshi Ohtsuki",
            "Hideki Nakayama"
        ],
        "paper_abstract": "Input method editor (IME) converts sequential alphabet key inputs to words in a target language. It is an indispensable service for billions of Asian users. Although the neural-based language model is extensively studied and shows promising results in sequence-to-sequence tasks, applying a neural-based language model to IME was not considered feasible due to high latency when converting words on user devices. In this work, we articulate the bottleneck of neural IME decoding to be the heavy softmax computation over a large vocabulary. We propose an approach that incrementally builds a subset vocabulary from the word lattice. Our approach always computes the probability with a selected subset vocabulary. When the selected vocabulary is updated, the stale probabilities in previous steps are fixed by recomputing the missing logits. The experiments on Japanese IME benchmark shows an over 50x speedup for the softmax computations comparing to the baseline, reaching real-time speed even on commodity CPU without losing conversion accuracy. The approach is potentially applicable to other incremental sequence-to-sequence decoding tasks such as real-time continuous speech recognition.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Locale-agnostic Universal Domain Classification Model in Spoken Language Understanding",
        "paper_url": "https://aclanthology.org/N19-2002/",
        "paper_authors": [
            "Jihwan Lee",
            "Ruhi Sarikaya",
            "Young-Bum Kim"
        ],
        "paper_abstract": "In this paper, we introduce an approach for leveraging available data across multiple locales sharing the same language to 1) improve domain classification model accuracy in Spoken Language Understanding and user experience even if new locales do not have sufficient data and 2) reduce the cost of scaling the domain classifier to a large number of locales. We propose a locale-agnostic universal domain classification model based on selective multi-task learning that learns a joint representation of an utterance over locales with different sets of domains and allows locales to share knowledge selectively depending on the domains. The experimental results demonstrate the effectiveness of our approach on domain classification task in the scenario of multiple locales with imbalanced data and disparate domain sets. The proposed approach outperforms other baselines models especially when classifying locale-specific domains and also low-resourced domains.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Practical Semantic Parsing for Spoken Language Understanding",
        "paper_url": "https://aclanthology.org/N19-2003/",
        "paper_authors": [
            "Marco Damonte",
            "Rahul Goel",
            "Tagyoung Chung"
        ],
        "paper_abstract": "Executable semantic parsing is the task of converting natural language utterances into logical forms that can be directly used as queries to get a response. We build a transfer learning framework for executable semantic parsing. We show that the framework is effective for Question Answering (Q&A) as well as for Spoken Language Understanding (SLU). We further investigate the case where a parser on a new domain can be learned by exploiting data on other domains, either via multi-task learning between the target domain and an auxiliary domain or via pre-training on the auxiliary domain and fine-tuning on the target domain. With either flavor of transfer learning, we are able to improve performance on most domains; we experiment with public data sets such as Overnight and NLmaps as well as with commercial SLU data. The experiments carried out on data sets that are different in nature show how executable semantic parsing can unify different areas of NLP such as Q&A and SLU.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring",
        "paper_url": "https://aclanthology.org/N19-2004/",
        "paper_authors": [
            "Zhengyuan Liu",
            "Hazel Lim",
            "Nur Farah Ain Suhaimi",
            "Shao Chuen Tong",
            "Sharon Ong",
            "Angela Ng",
            "Sheldon Lee",
            "Michael R. Macdonald",
            "Savitha Ramasamy",
            "Pavitra Krishnaswamy",
            "Wai Leng Chow",
            "Nancy F. Chen"
        ],
        "paper_abstract": "Data for human-human spoken dialogues for research and development are currently very limited in quantity, variety, and sources; such data are even scarcer in healthcare. In this work, we investigate fast prototyping of a dialogue comprehension system by leveraging on minimal nurse-to-patient conversations. We propose a framework inspired by nurse-initiated clinical symptom monitoring conversations to construct a simulated human-human dialogue dataset, embodying linguistic characteristics of spoken interactions like thinking aloud, self-contradiction, and topic drift. We then adopt an established bidirectional attention pointer network on this simulated dataset, achieving more than 80% F1 score on a held-out test set from real-world nurse-to-patient conversations. The ability to automatically comprehend conversations in the healthcare domain by exploiting only limited data has implications for improving clinical workflows through red flag symptom detection and triaging capabilities. We demonstrate the feasibility for efficient and effective extraction, retrieval and comprehension of symptom checking information discussed in multi-turn human-human spoken conversations.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents",
        "paper_url": "https://aclanthology.org/N19-2005/",
        "paper_authors": [
            "Xiaojing Liu",
            "Feiyu Gao",
            "Qiong Zhang",
            "Huasha Zhao"
        ],
        "paper_abstract": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Diversifying Reply Suggestions Using a Matching-Conditional Variational Autoencoder",
        "paper_url": "https://aclanthology.org/N19-2006/",
        "paper_authors": [
            "Budhaditya Deb",
            "Peter Bailey",
            "Milad Shokouhi"
        ],
        "paper_abstract": "We consider the problem of diversifying automated reply suggestions for a commercial instant-messaging (IM) system (Skype). Our conversation model is a standard matching based information retrieval architecture, which consists of two parallel encoders to project messages and replies into a common feature representation. During inference, we select replies from a fixed response set using nearest neighbors in the feature space. To diversify responses, we formulate the model as a generative latent variable model with Conditional Variational Auto-Encoder (M-CVAE). We propose a constrained-sampling approach to make the variational inference in M-CVAE efficient for our production system. In offline experiments, M-CVAE consistently increased diversity by \u223c30\u221240% without significant impact on relevance. This translated to a \u223c5% gain in click-rate in our online production system.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Goal-Oriented End-to-End Conversational Models with Profile Features in a Real-World Setting",
        "paper_url": "https://aclanthology.org/N19-2007/",
        "paper_authors": [
            "Yichao Lu",
            "Manisha Srivastava",
            "Jared Kramer",
            "Heba Elfardy",
            "Andrea Kahn",
            "Song Wang",
            "Vikas Bhardwaj"
        ],
        "paper_abstract": "End-to-end neural models for goal-oriented conversational systems have become an increasingly active area of research, though results in real-world settings are few. We present real-world results for two issue types in the customer service domain. We train models on historical chat transcripts and test on live contacts using a human-in-the-loop research platform. Additionally, we incorporate customer profile features to assess their impact on model performance. We experiment with two approaches for response generation: (1) sequence-to-sequence generation and (2) template ranking. To test our models, a customer service agent handles live contacts and at each turn we present the top four model responses and allow the agent to select (and optionally edit) one of the suggestions or to type their own. We present results for turn acceptance rate, response coverage, and edit rate based on approximately 600 contacts, as well as qualitative analysis on patterns of turn rejection and edit behavior. Top-4 turn acceptance rate across all models ranges from 63%-80%. Our results suggest that these models are promising for an agent-support application.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Detecting Customer Complaint Escalation with Recurrent Neural Networks and Manually-Engineered Features",
        "paper_url": "https://aclanthology.org/N19-2008/",
        "paper_authors": [
            "Wei Yang",
            "Luchen Tan",
            "Chunwei Lu",
            "Anqi Cui",
            "Han Li",
            "Xi Chen",
            "Kun Xiong",
            "Muzi Wang",
            "Ming Li",
            "Jian Pei",
            "Jimmy Lin"
        ],
        "paper_abstract": "Consumers dissatisfied with the normal dispute resolution process provided by an e-commerce company\u2019s customer service agents have the option of escalating their complaints by filing grievances with a government authority. This paper tackles the challenge of monitoring ongoing text chat dialogues to identify cases where the customer expresses such an intent, providing triage and prioritization for a separate pool of specialized agents specially trained to handle more complex situations. We describe a hybrid model that tackles this challenge by integrating recurrent neural networks with manually-engineered features. Experiments show that both components are complementary and contribute to overall recall, outperforming competitive baselines. A trial online deployment of our model demonstrates its business value in improving customer service.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multi-Modal Generative Adversarial Network for Short Product Title Generation in Mobile E-Commerce",
        "paper_url": "https://aclanthology.org/N19-2009/",
        "paper_authors": [
            "Jianguo Zhang",
            "Pengcheng Zou",
            "Zhao Li",
            "Yao Wan",
            "Xiuming Pan",
            "Yu Gong",
            "Philip S. Yu"
        ],
        "paper_abstract": "Nowadays, more and more customers browse and purchase products in favor of using mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are usually inclined to describe redundant and over-informative product titles to attract attentions from customers, it is important to concisely display short product titles on limited screen of mobile phones. To address this discrepancy, previous studies mainly consider textual information of long product titles and lacks of human-like view during training and evaluation process. In this paper, we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short product title generation in E-Commerce, which innovatively incorporates image information and attribute tags from product, as well as textual information from original long titles. MM-GAN poses short title generation as a reinforcement learning process, where the generated titles are evaluated by the discriminator in a human-like view. Extensive experiments on a large-scale E-Commerce dataset demonstrate that our algorithm outperforms other state-of-the-art methods. Moreover, we deploy our model into a real-world online E-Commerce environment and effectively boost the performance of click through rate and click conversion rate by 1.66% and 1.87%, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Case Study on Neural Headline Generation for Editing Support",
        "paper_url": "https://aclanthology.org/N19-2010/",
        "paper_authors": [
            "Kazuma Murao",
            "Ken Kobayashi",
            "Hayato Kobayashi",
            "Taichi Yatsuka",
            "Takeshi Masuyama",
            "Tatsuru Higurashi",
            "Yoshimune Tabuchi"
        ],
        "paper_abstract": "There have been many studies on neural headline generation models trained with a lot of (article, headline) pairs. However, there are few situations for putting such models into practical use in the real world since news articles typically already have corresponding headlines. In this paper, we describe a practical use case of neural headline generation in a news aggregator, where dozens of professional editors constantly select important news articles and manually create their headlines, which are much shorter than the original headlines. Specifically, we show how to deploy our model to an editing support tool and report the results of comparing the behavior of the editors before and after the release.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Lexicons for Slot Tagging in Spoken Language Understanding",
        "paper_url": "https://aclanthology.org/N19-2011/",
        "paper_authors": [
            "Kyle Williams"
        ],
        "paper_abstract": "We explore the use of lexicons or gazettes in neural models for slot tagging in spoken language understanding. We develop models that encode lexicon information as neural features for use in a Long-short term memory neural network. Experiments are performed on data from 4 domains from an intelligent assistant under conditions that often occur in an industry setting, where there may be: 1) large amounts of training data, 2) limited amounts of training data for new domains, and 3) cross domain training. Results show that the use of neural lexicon information leads to a significant improvement in slot tagging, with improvements in the F-score of up to 12%. Our findings have implications for how lexicons can be used to improve the performance of neural slot tagging models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Active Learning for New Domains in Natural Language Understanding",
        "paper_url": "https://aclanthology.org/N19-2012/",
        "paper_authors": [
            "Stanislav Peshterliev",
            "John Kearney",
            "Abhyuday Jagannatha",
            "Imre Kiss",
            "Spyros Matsoukas"
        ],
        "paper_abstract": "We explore active learning (AL) for improving the accuracy of new domains in a natural language understanding (NLU) system. We propose an algorithm called Majority-CRF that uses an ensemble of classification models to guide the selection of relevant utterances, as well as a sequence labeling model to help prioritize informative examples. Experiments with three domains show that Majority-CRF achieves 6.6%-9% relative error rate reduction compared to random sampling with the same annotation budget, and statistically significant improvements compared to other AL approaches. Additionally, case studies with human-in-the-loop AL on six new domains show 4.6%-9% improvement on an existing NLU system.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Scaling Multi-Domain Dialogue State Tracking via Query Reformulation",
        "paper_url": "https://aclanthology.org/N19-2013/",
        "paper_authors": [
            "Pushpendre Rastogi",
            "Arpit Gupta",
            "Tongfei Chen",
            "Mathias Lambert"
        ],
        "paper_abstract": "We present a novel approach to dialogue state tracking and referring expression resolution tasks. Successful contextual understanding of multi-turn spoken dialogues requires resolving referring expressions across turns and tracking the entities relevant to the conversation across turns. Tracking conversational state is particularly challenging in a multi-domain scenario when there exist multiple spoken language understanding (SLU) sub-systems, and each SLU sub-system operates on its domain-specific meaning representation. While previous approaches have addressed the disparate schema issue by learning candidate transformations of the meaning representation, in this paper, we instead model the reference resolution as a dialogue context-aware user query reformulation task \u2013 the dialog state is serialized to a sequence of natural language tokens representing the conversation. We develop our model for query reformulation using a pointer-generator network and a novel multi-task learning setup. In our experiments, we show a significant improvement in absolute F1 on an internal as well as a, soon to be released, public benchmark respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Are the Tools up to the Task? an Evaluation of Commercial Dialog Tools in Developing Conversational Enterprise-grade Dialog Systems",
        "paper_url": "https://aclanthology.org/N19-2014/",
        "paper_authors": [
            "Marie Meteer",
            "Meghan Hickey",
            "Carmi Rothberg",
            "David Nahamoo",
            "Ellen Eide Kislal"
        ],
        "paper_abstract": "There has been a significant investment in dialog systems (tools and runtime) for building conversational systems by major companies including Google, IBM, Microsoft, and Amazon. The question remains whether these tools are up to the task of building conversational, task-oriented dialog applications at the enterprise level. In our company, we are exploring and comparing several toolsets in an effort to determine their strengths and weaknesses in meeting our goals for dialog system development: accuracy, time to market, ease of replicating and extending applications, and efficiency and ease of use by developers. In this paper, we provide both quantitative and qualitative results in three main areas: natural language understanding, dialog, and text generation. While existing toolsets were all incomplete, we hope this paper will provide a roadmap of where they need to go to meet the goal of building effective dialog systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Development and Deployment of a Large-Scale Dialog-based Intelligent Tutoring System",
        "paper_url": "https://aclanthology.org/N19-2015/",
        "paper_authors": [
            "Shazia Afzal",
            "Tejas Dhamecha",
            "Nirmal Mukhi",
            "Renuka Sindhgatta",
            "Smit Marvaniya",
            "Matthew Ventura",
            "Jessica Yarbro"
        ],
        "paper_abstract": "There are significant challenges involved in the design and implementation of a dialog-based tutoring system (DBT) ranging from domain engineering to natural language classification and eventually instantiating an adaptive, personalized dialog strategy. These issues are magnified when implementing such a system at scale and across domains. In this paper, we describe and reflect on the design, methods, decisions and assessments that led to the successful deployment of our AI driven DBT currently being used by several hundreds of college level students for practice and self-regulated study in diverse subjects like Sociology, Communications, and American Government.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning When Not to Answer: a Ternary Reward Structure for Reinforcement Learning Based Question Answering",
        "paper_url": "https://aclanthology.org/N19-2016/",
        "paper_authors": [
            "Fr\u00e9deric Godin",
            "Anjishnu Kumar",
            "Arpit Mittal"
        ],
        "paper_abstract": "In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications. We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings. More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Extraction of Message Sequence Charts from Software Use-Case Descriptions",
        "paper_url": "https://aclanthology.org/N19-2017/",
        "paper_authors": [
            "Girish Palshikar",
            "Nitin Ramrakhiyani",
            "Sangameshwar Patil",
            "Sachin Pawar",
            "Swapnil Hingmire",
            "Vasudeva Varma",
            "Pushpak Bhattacharyya"
        ],
        "paper_abstract": "Software Requirement Specification documents provide natural language descriptions of the core functional requirements as a set of use-cases. Essentially, each use-case contains a set of actors and sequences of steps describing the interactions among them. Goals of use-case reviews and analyses include their correctness, completeness, detection of ambiguities, prototyping, verification, test case generation and traceability. Message Sequence Chart (MSC) have been proposed as a expressive, rigorous yet intuitive visual representation of use-cases. In this paper, we describe a linguistic knowledge-based approach to extract MSCs from use-cases. Compared to existing techniques, we extract richer constructs of the MSC notation such as timers, conditions and alt-boxes. We apply this tool to extract MSCs from several real-life software use-case descriptions and show that it performs better than the existing techniques. We also discuss the benefits and limitations of the extracted MSCs to meet the above goals.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Knowledge Base Construction from Robust Infobox Extraction",
        "paper_url": "https://aclanthology.org/N19-2018/",
        "paper_authors": [
            "Boya Peng",
            "Yejin Huh",
            "Xiao Ling",
            "Michele Banko"
        ],
        "paper_abstract": "A capable, automatic Question Answering (QA) system can provide more complete and accurate answers using a comprehensive knowledge base (KB). One important approach to constructing a comprehensive knowledge base is to extract information from Wikipedia infobox tables to populate an existing KB. Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DBpedia), three major challenges remain: 1) Deterministic extraction patterns used in DBpedia are vulnerable to template changes; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB. This paper presents a robust approach that tackles all three challenges. We build probabilistic models to predict relations between entity mentions directly from the infobox tables in HTML. The entity mentions are linked to identifiers in an existing KB if possible. The unlinkable ones are also parsed and preserved in the final output. Training data for both the relation extraction and the entity linking models are automatically generated using distant supervision. We demonstrate the empirical effectiveness of the proposed method in both precision and recall compared to a strong IBE baseline, DBpedia, with an absolute improvement of 41.3% in average F1. We also show that our extraction makes the final KB significantly more complete, improving the completeness score of list-value relation types by 61.4%.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A k-Nearest Neighbor Approach towards Multi-level Sequence Labeling",
        "paper_url": "https://aclanthology.org/N19-2019/",
        "paper_authors": [
            "Yue Chen",
            "John Chen"
        ],
        "paper_abstract": "In this paper we present a new method for intent recognition for complex dialog management in low resource situations. Complex dialog management is required because our target domain is real world mixed initiative food ordering between agents and their customers, where individual customer utterances may contain multiple intents and refer to food items with complex structure. For example, a customer might say \u201cCan I get a deluxe burger with large fries and oh put extra mayo on the burger would you?\u201d We approach this task as a multi-level sequence labeling problem, with the constraint of limited real training data. Both traditional methods like HMM, MEMM, or CRF and newer methods like DNN or BiLSTM use only homogeneous feature sets. Newer methods perform better but also require considerably more data. Previous research has done pseudo-data synthesis to obtain the required amounts of training data. We propose to use a k-NN learner with heterogeneous feature set. We used windowed word n-grams, POS tag n-grams and pre-trained word embeddings as features. For the experiments we perform a comparison between using pseudo-data and real world data. We also perform semi-supervised self-training to obtain additional labeled data, in order to better model real world scenarios. Instead of using massive pseudo-data, we show that with only less than 1% of the data size, we can achieve better result than any of the methods above by annotating real world data. We achieve labeled bracketed F-scores of 75.46, 52.84 and 49.66 for the three levels of sequence labeling where each level has a longer word span than its previous level. Overall we achieve 60.71F. In comparison, two previous systems, MEMM and DNN-ELMO, achieved 52.32 and 45.25 respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Train One Get One Free: Partially Supervised Neural Network for Bug Report Duplicate Detection and Clustering",
        "paper_url": "https://aclanthology.org/N19-2020/",
        "paper_authors": [
            "Lahari Poddar",
            "Leonardo Neves",
            "William Brendel",
            "Luis Marujo",
            "Sergey Tulyakov",
            "Pradeep Karuturi"
        ],
        "paper_abstract": "Tracking user reported bugs requires considerable engineering effort in going through many repetitive reports and assigning them to the correct teams. This paper proposes a neural architecture that can jointly (1) detect if two bug reports are duplicates, and (2) aggregate them into latent topics. Leveraging the assumption that learning the topic of a bug is a sub-task for detecting duplicates, we design a loss function that can jointly perform both tasks but needs supervision for only duplicate classification, achieving topic clustering in an unsupervised fashion. We use a two-step attention module that uses self-attention for topic clustering and conditional attention for duplicate detection. We study the characteristics of two types of real world datasets that have been marked for duplicate bugs by engineers and by non-technical annotators. The results demonstrate that our model not only can outperform state-of-the-art methods for duplicate classification on both cases, but can also learn meaningful latent clusters without additional supervision.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Robust Semantic Parsing with Adversarial Learning for Domain Generalization",
        "paper_url": "https://aclanthology.org/N19-2021/",
        "paper_authors": [
            "Gabriel Marzinotto",
            "G\u00e9raldine Damnati",
            "Fr\u00e9d\u00e9ric B\u00e9chet",
            "Beno\u00eet Favre"
        ],
        "paper_abstract": "This paper addresses the issue of generalization for Semantic Parsing in an adversarial framework. Building models that are more robust to inter-document variability is crucial for the integration of Semantic Parsing technologies in real applications. The underlying question throughout this study is whether adversarial learning can be used to train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations. We propose to perform Semantic Parsing with a domain classification adversarial task, covering various use-cases with or without explicit knowledge of the domain. The strategy is first evaluated on a French corpus of encyclopedic documents, annotated with FrameNet, in an information retrieval perspective. This corpus constitutes a new public benchmark, gathering documents from various thematic domains and various sources. We show that adversarial learning yields improved results when using explicit domain classification as the adversarial task. We also propose an unsupervised domain discovery approach that yields equivalent improvements. The latter is also evaluated on a PropBank Semantic Role Labeling task on the CoNLL-2005 benchmark and is shown to increase the model\u2019s generalization capabilities on out-of-domain data.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TOI-CNN: a Solution of Information Extraction on Chinese Insurance Policy",
        "paper_url": "https://aclanthology.org/N19-2022/",
        "paper_authors": [
            "Lin Sun",
            "Kai Zhang",
            "Fule Ji",
            "Zhenhua Yang"
        ],
        "paper_abstract": "Contract analysis can significantly ease the work for humans using AI techniques. This paper shows a problem of Element Tagging on Insurance Policy (ETIP). A novel Text-Of-Interest Convolutional Neural Network (TOI-CNN) is proposed for the ETIP solution. We introduce a TOI pooling layer to replace traditional pooling layer for processing the nested phrasal or clausal elements in insurance policies. The advantage of TOI pooling layer is that the nested elements from one sentence could share computation and context in the forward and backward passes. The computation of backpropagation through TOI pooling is also demonstrated in the paper. We have collected a large Chinese insurance contract dataset and labeled the critical elements of seven categories to test the performance of the proposed method. The results show the promising performance of our method in the ETIP problem.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Cross-lingual Transfer Learning for Japanese Named Entity Recognition",
        "paper_url": "https://aclanthology.org/N19-2023/",
        "paper_authors": [
            "Andrew Johnson",
            "Penny Karanasou",
            "Judith Gaspers",
            "Dietrich Klakow"
        ],
        "paper_abstract": "This work explores cross-lingual transfer learning (TL) for named entity recognition, focusing on bootstrapping Japanese from English. A deep neural network model is adopted and the best combination of weights to transfer is extensively investigated. Moreover, a novel approach is presented that overcomes linguistic differences between this language pair by romanizing a portion of the Japanese input. Experiments are conducted on external datasets, as well as internal large-scale real-world ones. Gains with TL are achieved for all evaluated cases. Finally, the influence on TL of the target dataset size and of the target tagset distribution is further investigated.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Neural Text Normalization with Subword Units",
        "paper_url": "https://aclanthology.org/N19-2024/",
        "paper_authors": [
            "Courtney Mansfield",
            "Ming Sun",
            "Yuzong Liu",
            "Ankur Gandhe",
            "Bj\u00f6rn Hoffmeister"
        ],
        "paper_abstract": "Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Audio De-identification - a New Entity Recognition Task",
        "paper_url": "https://aclanthology.org/N19-2025/",
        "paper_authors": [
            "Ido Cohn",
            "Itay Laish",
            "Genady Beryozkin",
            "Gang Li",
            "Izhak Shafran",
            "Idan Szpektor",
            "Tzvika Hartman",
            "Avinatan Hassidim",
            "Yossi Matias"
        ],
        "paper_abstract": "Named Entity Recognition (NER) has been mostly studied in the context of written text. Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor. In such recordings, audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text. The application of NER in the context of audio de-identification has yet to be fully investigated. To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected. We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment. Finally, we introduce a novel metric for audio de-ID and a new evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline\u2019s results on it.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "In Other News: a Bi-style Text-to-speech Model for Synthesizing Newscaster Voice with Limited Data",
        "paper_url": "https://aclanthology.org/N19-2026/",
        "paper_authors": [
            "Nishant Prateek",
            "Mateusz \u0141ajszczak",
            "Roberto Barra-Chicote",
            "Thomas Drugman",
            "Jaime Lorenzo-Trueba",
            "Thomas Merritt",
            "Srikanth Ronanki",
            "Trevor Wood"
        ],
        "paper_abstract": "Neural text-to-speech synthesis (NTTS) models have shown significant progress in generating high-quality speech, however they require a large quantity of training data. This makes creating models for multiple styles expensive and time-consuming. In this paper different styles of speech are analysed based on prosodic variations, from this a model is proposed to synthesise speech in the style of a newscaster, with just a few hours of supplementary data. We pose the problem of synthesising in a target style using limited data as that of creating a bi-style model that can synthesise both neutral-style and newscaster-style speech via a one-hot vector which factorises the two styles. We also propose conditioning the model on contextual word embeddings, and extensively evaluate it against neutral NTTS, and neutral concatenative-based synthesis. This model closes the gap in perceived style-appropriateness between natural recordings for newscaster-style of speech, and neutral speech synthesis by approximately two-thirds.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generate, Filter, and Rank: Grammaticality Classification for Production-Ready NLG Systems",
        "paper_url": "https://aclanthology.org/N19-2027/",
        "paper_authors": [
            "Ashwini Challa",
            "Kartikeya Upasani",
            "Anusha Balakrishnan",
            "Rajen Subba"
        ],
        "paper_abstract": "Neural approaches to Natural Language Generation (NLG) have been promising for goal-oriented dialogue. One of the challenges of productionizing these approaches, however, is the ability to control response quality, and ensure that generated responses are acceptable. We propose the use of a generate, filter, and rank framework, in which candidate responses are first filtered to eliminate unacceptable responses, and then ranked to select the best response. While acceptability includes grammatical correctness and semantic correctness, we focus only on grammaticality classification in this paper, and show that existing datasets for grammatical error correction don\u2019t correctly capture the distribution of errors that data-driven generators are likely to make. We release a grammatical classification and semantic correctness classification dataset for the weather domain that consists of responses generated by 3 data-driven NLG systems. We then explore two supervised learning approaches (CNNs and GBDTs) for classifying grammaticality. Our experiments show that grammaticality classification is very sensitive to the distribution of errors in the data, and that these distributions vary significantly with both the source of the response as well as the domain. We show that it\u2019s possible to achieve high precision with reasonable recall on our dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Content-based Dwell Time Engagement Prediction Model for News Articles",
        "paper_url": "https://aclanthology.org/N19-2028/",
        "paper_authors": [
            "Heidar Davoudi",
            "Aijun An",
            "Gordon Edall"
        ],
        "paper_abstract": "The article dwell time (i.e., expected time that users spend on an article) is among the most important factors showing the article engagement. It is of great interest to predict the dwell time of an article before its release. This allows digital newspapers to make informed decisions and publish more engaging articles. In this paper, we propose a novel content-based approach based on a deep neural network architecture for predicting article dwell times. The proposed model extracts emotion, event and entity features from an article, learns interactions among them, and combines the interactions with the word-based features of the article to learn a model for predicting the dwell time. The experimental results on a real dataset from a major newspaper show that the proposed model outperforms other state-of-the-art baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop",
        "paper_url": "https://aclanthology.org/N19-3000/",
        "paper_authors": [
            "Sudipta Kar",
            "Farah Nadeem",
            "Laura Burdick",
            "Greg Durrett",
            "Na-Rae Han"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Is It Dish Washer Safe? Automatically Answering \u201cYes/No\u201d Questions Using Customer Reviews",
        "paper_url": "https://aclanthology.org/N19-3001/",
        "paper_authors": [
            "Daria Dzendzik",
            "Carl Vogel",
            "Jennifer Foster"
        ],
        "paper_abstract": "It has become commonplace for people to share their opinions about all kinds of products by posting reviews online. It has also become commonplace for potential customers to do research about the quality and limitations of these products by posting questions online. We test the extent to which reviews are useful in question-answering by combining two Amazon datasets and focusing our attention on yes/no questions. A manual analysis of 400 cases reveals that the reviews directly contain the answer to the question just over a third of the time. Preliminary reading comprehension experiments with this dataset prove inconclusive, with accuracy in the range 50-66%.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Identifying and Reducing Gender Bias in Word-Level Language Models",
        "paper_url": "https://aclanthology.org/N19-3002/",
        "paper_authors": [
            "Shikha Bordia",
            "Samuel R. Bowman"
        ],
        "paper_abstract": "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora\u2014Penn Treebank, WikiText-2, and CNN/Daily Mail\u2014resulting in similar conclusions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Emotion Impacts Speech Recognition Performance",
        "paper_url": "https://aclanthology.org/N19-3003/",
        "paper_authors": [
            "Rushab Munot",
            "Ani Nenkova"
        ],
        "paper_abstract": "It has been established that the performance of speech recognition systems depends on multiple factors including the lexical content, speaker identity and dialect. Here we use three English datasets of acted emotion to demonstrate that emotional content also impacts the performance of commercial systems. On two of the corpora, emotion is a bigger contributor to recognition errors than speaker identity and on two, neutral speech is recognized considerably better than emotional speech. We further evaluate the commercial systems on spontaneous interactions that contain portions of emotional speech. We propose and validate on the acted datasets, a method that allows us to evaluate the overall impact of emotion on recognition even when manual transcripts are not available. Using this method, we show that emotion in natural spontaneous dialogue is a less prominent but still significant factor in recognition accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The Strength of the Weakest Supervision: Topic Classification Using Class Labels",
        "paper_url": "https://aclanthology.org/N19-3004/",
        "paper_authors": [
            "Jiatong Li",
            "Kai Zheng",
            "Hua Xu",
            "Qiaozhu Mei",
            "Yue Wang"
        ],
        "paper_abstract": "When developing topic classifiers for real-world applications, we begin by defining a set of meaningful topic labels. Ideally, an intelligent classifier can understand these labels right away and start classifying documents. Indeed, a human can confidently tell if an article is about science, politics, sports, or none of the above, after knowing just the class labels. We study the problem of training an initial topic classifier using only class labels. We investigate existing techniques for solving this problem and propose a simple but effective approach. Experiments on a variety of topic classification data sets show that learning from class labels can save significant initial labeling effort, essentially providing a \u201dfree\u201d warm start to the topic classifier.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling",
        "paper_url": "https://aclanthology.org/N19-3005/",
        "paper_authors": [
            "Debjit Paul",
            "Mittul Singh",
            "Michael A. Hedderich",
            "Dietrich Klakow"
        ],
        "paper_abstract": "In this paper, we address the problem of effectively self-training neural networks in a low-resource setting. Self-training is frequently used to automatically increase the amount of training data. However, in a low-resource scenario, it is less effective due to unreliable annotations created using self-labeling of unlabeled data. We propose to combine self-training with noise handling on the self-labeled data. Directly estimating noise on the combined clean training set and self-labeled data can lead to corruption of the clean data and hence, performs worse. Thus, we propose the Clean and Noisy Label Neural Network which trains on clean and noisy self-labeled data simultaneously by explicitly modelling clean and noisy labels separately. In our experiments on Chunking and NER, this approach performs more robustly than the baselines. Complementary to this explicit approach, noise can also be handled implicitly with the help of an auxiliary learning task. To such a complementary approach, our method is more beneficial than other baseline methods and together provides the best performance overall.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Opinion Mining with Deep Contextualized Embeddings",
        "paper_url": "https://aclanthology.org/N19-3006/",
        "paper_authors": [
            "Wen-Bin Han",
            "Noriko Kando"
        ],
        "paper_abstract": "Detecting opinion expression is a potential and essential task in opinion mining that can be extended to advanced tasks. In this paper, we considered opinion expression detection as a sequence labeling task and exploited different deep contextualized embedders into the state-of-the-art architecture, composed of bidirectional long short-term memory (BiLSTM) and conditional random field (CRF). Our experimental results show that using different word embeddings can cause contrasting results, and the model can achieve remarkable scores with deep contextualized embeddings. Especially, using BERT embedder can significantly exceed using ELMo embedder.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Bag-of-concepts Model Improves Relation Extraction in a Narrow Knowledge Domain with Limited Data",
        "paper_url": "https://aclanthology.org/N19-3007/",
        "paper_authors": [
            "Jiyu Chen",
            "Karin Verspoor",
            "Zenan Zhai"
        ],
        "paper_abstract": "This paper focuses on a traditional relation extraction task in the context of limited annotated data and a narrow knowledge domain. We explore this task with a clinical corpus consisting of 200 breast cancer follow-up treatment letters in which 16 distinct types of relations are annotated. We experiment with an approach to extracting typed relations called window-bounded co-occurrence (WBC), which uses an adjustable context window around entity mentions of a relevant type, and compare its performance with a more typical intra-sentential co-occurrence baseline. We further introduce a new bag-of-concepts (BoC) approach to feature engineering based on the state-of-the-art word embeddings and word synonyms. We demonstrate the competitiveness of BoC by comparing with methods of higher complexity, and explore its effectiveness on this small dataset.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Generating Text through Adversarial Training Using Skip-Thought Vectors",
        "paper_url": "https://aclanthology.org/N19-3008/",
        "paper_authors": [
            "Afroz Ahamad"
        ],
        "paper_abstract": "GANs have been shown to perform exceedingly well on tasks pertaining to image generation and style transfer. In the field of language modelling, word embeddings such as GLoVe and word2vec are state-of-the-art methods for applying neural network models on textual data. Attempts have been made to utilize GANs with word embeddings for text generation. This study presents an approach to text generation using Skip-Thought sentence embeddings with GANs based on gradient penalty functions and f-measures. The proposed architecture aims to reproduce writing style in the generated text by modelling the way of expression at a sentence level across all the works of an author. Extensive experiments were run in different embedding settings on a variety of tasks including conditional text generation and language generation. The model outperforms baseline text generation networks across several automated evaluation metrics like BLEU-n, METEOR and ROUGE. Further, wide applicability and effectiveness in real life tasks are demonstrated through human judgement scores.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Partially Rule-Based Approach to AMR Generation",
        "paper_url": "https://aclanthology.org/N19-3009/",
        "paper_authors": [
            "Emma Manning"
        ],
        "paper_abstract": "This paper presents a new approach to generating English text from Abstract Meaning Representation (AMR). In contrast to the neural and statistical MT approaches used in other AMR generation systems, this one is largely rule-based, supplemented only by a language model and simple statistical linearization models, allowing for more control over the output. We also address the difficulties of automatically evaluating AMR generation systems and the problems with BLEU for this task. We compare automatic metrics to human evaluations and show that while METEOR and TER arguably reflect human judgments better than BLEU, further research into suitable evaluation metrics is needed.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Computational Investigations of Pragmatic Effects in Natural Language",
        "paper_url": "https://aclanthology.org/N19-3010/",
        "paper_authors": [
            "Jad Kabbara"
        ],
        "paper_abstract": "Semantics and pragmatics are two complimentary and intertwined aspects of meaning in language. The former is concerned with the literal (context-free) meaning of words and sentences, the latter focuses on the intended meaning, one that is context-dependent. While NLP research has focused in the past mostly on semantics, the goal of this thesis is to develop computational models that leverage this pragmatic knowledge in language that is crucial to performing many NLP tasks correctly. In this proposal, we begin by reviewing the current progress in this thesis, namely, on the tasks of definiteness prediction and adverbial presupposition triggering. Then we discuss the proposed research for the remainder of the thesis which builds on this progress towards the goal of building better and more pragmatically-aware natural language generation and understanding systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SEDTWik: Segmentation-based Event Detection from Tweets Using Wikipedia",
        "paper_url": "https://aclanthology.org/N19-3011/",
        "paper_authors": [
            "Keval Morabia",
            "Neti Lalita Bhanu Murthy",
            "Aruna Malapati",
            "Surender Samant"
        ],
        "paper_abstract": "Event Detection has been one of the research areas in Text Mining that has attracted attention during this decade due to the widespread availability of social media data specifically twitter data. Twitter has become a major source for information about real-world events because of the use of hashtags and the small word limit of Twitter that ensures concise presentation of events. Previous works on event detection from tweets are either applicable to detect localized events or breaking news only or miss out on many important events. This paper presents the problems associated with event detection from tweets and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories. The main idea is to split each tweet and hash-tag into segments, extract bursty segments, cluster them, and summarize them. We evaluated our results on the well-known Events2012 corpus and achieved state-of-the-art results. Keywords: Event detection, Twitter, Social Media, Microblogging, Tweet segmentation, Text Mining, Wikipedia, Hashtag.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multimodal Machine Translation with Embedding Prediction",
        "paper_url": "https://aclanthology.org/N19-3012/",
        "paper_authors": [
            "Tosho Hirasawa",
            "Hayahide Yamagishi",
            "Yukio Matsumura",
            "Mamoru Komachi"
        ],
        "paper_abstract": "Multimodal machine translation is an attractive application of neural machine translation (NMT). It helps computers to deeply understand visual objects and their relations with natural languages. However, multimodal NMT systems suffer from a shortage of available training data, resulting in poor performance for translating rare words. In NMT, pretrained word embeddings have been shown to improve NMT of low-resource domains, and a search-based approach is proposed to address the rare word problem. In this study, we effectively combine these two approaches in the context of multimodal NMT and explore how we can take full advantage of pretrained word embeddings to better translate rare words. We report overall performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement of 7.67 F-score for rare word translation.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Deep Learning and Sociophonetics: Automatic Coding of Rhoticity Using Neural Networks",
        "paper_url": "https://aclanthology.org/N19-3013/",
        "paper_authors": [
            "Sarah Gupta",
            "Anthony DiPadova"
        ],
        "paper_abstract": "Automated extraction methods are widely available for vowels, but automated methods for coding rhoticity have lagged far behind. R-fulness versus r-lessness (in words like park, store, etc.) is a classic and frequently cited variable, but it is still commonly coded by human analysts rather than automated methods. Human-coding requires extensive resources and lacks replicability, making it difficult to compare large datasets across research groups. Can reliable automated methods be developed to aid in coding rhoticity? In this study, we use Neural Networks/Deep Learning, training our model on 208 Boston-area speakers.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Data Augmentation by Data Noising for Open-vocabulary Slots in Spoken Language Understanding",
        "paper_url": "https://aclanthology.org/N19-3014/",
        "paper_authors": [
            "Hwa-Yeon Kim",
            "Yoon-Hyung Roh",
            "Young-Kil Kim"
        ],
        "paper_abstract": "One of the main challenges in Spoken Language Understanding (SLU) is dealing with \u2018open-vocabulary\u2019 slots. Recently, SLU models based on neural network were proposed, but it is still difficult to recognize the slots of unknown words or \u2018open-vocabulary\u2019 slots because of the high cost of creating a manually tagged SLU dataset. This paper proposes data noising, which reflects the characteristics of the \u2018open-vocabulary\u2019 slots, for data augmentation. We applied it to an attention based bi-directional recurrent neural network (Liu and Lane, 2016) and experimented with three datasets: Airline Travel Information System (ATIS), Snips, and MIT-Restaurant. We achieved performance improvements of up to 0.57% and 3.25 in intent prediction (accuracy) and slot filling (f1-score), respectively. Our method is advantageous because it does not require additional memory and it can be applied simultaneously with the training process of the model.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Expectation and Locality Effects in the Prediction of Disfluent Fillers and Repairs in English Speech",
        "paper_url": "https://aclanthology.org/N19-3015/",
        "paper_authors": [
            "Samvit Dammalapati",
            "Rajakrishnan Rajkumar",
            "Sumeet Agarwal"
        ],
        "paper_abstract": "This study examines the role of three influential theories of language processing, viz., Surprisal Theory, Uniform Information Density (UID) hypothesis and Dependency Locality Theory (DLT), in predicting disfluencies in speech production. To this end, we incorporate features based on lexical surprisal, word duration and DLT integration and storage costs into logistic regression classifiers aimed to predict disfluencies in the Switchboard corpus of English conversational speech. We find that disfluencies occur in the face of upcoming difficulties and speakers tend to handle this by lessening cognitive load before disfluencies occur. Further, we see that reparandums behave differently from disfluent fillers possibly due to the lessening of the cognitive load also happening in the word choice of the reparandum, i.e., in the disfluency itself. While the UID hypothesis does not seem to play a significant role in disfluency prediction, lexical surprisal and DLT costs do give promising results in explaining language production. Further, we also find that as a means to lessen cognitive load for upcoming difficulties speakers take more time on words preceding disfluencies, making duration a key element in understanding disfluencies.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Gating Mechanisms for Combining Character and Word-level Word Representations: an Empirical Study",
        "paper_url": "https://aclanthology.org/N19-3016/",
        "paper_authors": [
            "Jorge Balazs",
            "Yutaka Matsuo"
        ],
        "paper_abstract": "In this paper we study how different ways of combining character and word-level representations affect the quality of both final word and sentence representations. We provide strong empirical evidence that modeling characters improves the learned representations at the word and sentence levels, and that doing so is particularly useful when representing less frequent words. We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representations that encode semantic similarity, as it performed reasonably well in several word similarity datasets. Finally, our findings suggest that properly capturing semantic similarity at the word level does not consistently yield improved performance in downstream sentence-level tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Pregroup Representation of Word Order Alternation Using Hindi Syntax",
        "paper_url": "https://aclanthology.org/N19-3017/",
        "paper_authors": [
            "Alok Debnath",
            "Manish Shrivastava"
        ],
        "paper_abstract": "Pregroup calculus has been used for the representation of free word order languages (Sanskrit and Hungarian), using a construction called precyclicity. However, restricted word order alternation has not been handled before. This paper aims at introducing and formally expressing three methods of representing word order alternation in the pregroup representation of any language. This paper describes the word order alternation patterns of Hindi, and creates a basic pregroup representation for the language. In doing so, the shortcoming of correct reductions for ungrammatical sentences due to the current apparatus is highlighted, and the aforementioned methods are invoked for a grammatically accurate representation of restricted word order alternation. The replicability of these methods is explained in the representation of adverbs and prepositional phrases in English.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Speak up, Fight Back! Detection of Social Media Disclosures of Sexual Harassment",
        "paper_url": "https://aclanthology.org/N19-3018/",
        "paper_authors": [
            "Arijit Ghosh Chowdhury",
            "Ramit Sawhney",
            "Puneet Mathur",
            "Debanjan Mahata",
            "Rajiv Ratn Shah"
        ],
        "paper_abstract": "The #MeToo movement is an ongoing prevalent phenomenon on social media aiming to demonstrate the frequency and widespread of sexual harassment by providing a platform to speak narrate personal experiences of such harassment. The aggregation and analysis of such disclosures pave the way to development of technology-based prevention of sexual harassment. We contend that the lack of specificity in generic sentence classification models may not be the best way to tackle text subtleties that intrinsically prevail in a classification task as complex as identifying disclosures of sexual harassment. We propose the Disclosure Language Model, a three part ULMFiT architecture, consisting of a Language model, a Medium-Specific (Twitter) model and a Task-Specific classifier to tackle this problem and create a manually annotated real-world dataset to test our technique on this, to show that using a Discourse Language Model often yields better classification performance over (i) Generic deep learning based sentence classification models (ii) existing models that rely on handcrafted stylistic features. An extensive comparison with state-of-the-art generic and specific models along with a detailed error analysis presents the case for our proposed methodology.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SNAP-BATNET: Cascading Author Profiling and Social Network Graphs for Suicide Ideation Detection on Social Media",
        "paper_url": "https://aclanthology.org/N19-3019/",
        "paper_authors": [
            "Rohan Mishra",
            "Pradyumn Prakhar Sinha",
            "Ramit Sawhney",
            "Debanjan Mahata",
            "Puneet Mathur",
            "Rajiv Ratn Shah"
        ],
        "paper_abstract": "Suicide is a leading cause of death among youth and the use of social media to detect suicidal ideation is an active line of research. While it has been established that these users share a common set of properties, the current state-of-the-art approaches utilize only text-based (stylistic and semantic) cues. We contend that the use of information from networks in the form of condensed social graph embeddings and author profiling using features from historical data can be combined with an existing set of features to improve the performance. To that end, we experiment on a manually annotated dataset of tweets created using a three-phase strategy and propose SNAP-BATNET, a deep learning based model to extract text-based features and a novel Feature Stacking approach to combine other community-based information such as historical author profiling and graph embeddings that outperform the current state-of-the-art. We conduct a comprehensive quantitative analysis with baselines, both generic and specific, that presents the case for SNAP-BATNET, along with an error analysis that highlights the limitations and challenges faced paving the way to the future of AI-based suicide ideation detection.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
        "paper_url": "https://aclanthology.org/N19-4000/",
        "paper_authors": [
            "Waleed Ammar",
            "Annie Louis",
            "Nasrin Mostafazadeh"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Abbreviation Explorer - an interactive system for pre-evaluation of Unsupervised Abbreviation Disambiguation",
        "paper_url": "https://aclanthology.org/N19-4001/",
        "paper_authors": [
            "Manuel R. Ciosici",
            "Ira Assent"
        ],
        "paper_abstract": "We present Abbreviation Explorer, a system that supports interactive exploration of abbreviations that are challenging for Unsupervised Abbreviation Disambiguation (UAD). Abbreviation Explorer helps to identify long-forms that are easily confused, and to pinpoint likely causes such as limitations of normalization, language switching, or inconsistent typing. It can also support determining which long-forms would benefit from additional input text for unsupervised abbreviation disambiguation. The system provides options for creating corrective rules that merge redundant long-forms with identical meaning. The identified rules can be easily applied to the already existing vector spaces used by UAD to improve disambiguation performance, while also avoiding the cost of retraining.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ADIDA: Automatic Dialect Identification for Arabic",
        "paper_url": "https://aclanthology.org/N19-4002/",
        "paper_authors": [
            "Ossama Obeid",
            "Mohammad Salameh",
            "Houda Bouamor",
            "Nizar Habash"
        ],
        "paper_abstract": "This demo paper describes ADIDA, a web-based system for automatic dialect identification for Arabic text. The system distinguishes among the dialects of 25 Arab cities (from Rabat to Muscat) in addition to Modern Standard Arabic. The results are presented with either a point map or a heat map visualizing the automatic identification probabilities over a geographical map of the Arab World.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Enabling Search and Collaborative Assembly of Causal Interactions Extracted from Multilingual and Multi-domain Free Text",
        "paper_url": "https://aclanthology.org/N19-4003/",
        "paper_authors": [
            "George C. G. Barbosa",
            "Zechy Wong",
            "Gus Hahn-Powell",
            "Dane Bell",
            "Rebecca Sharp",
            "Marco A. Valenzuela-Esc\u00e1rcega",
            "Mihai Surdeanu"
        ],
        "paper_abstract": "Many of the most pressing current research problems (e.g., public health, food security, or climate change) require multi-disciplinary collaborations. In order to facilitate this process, we propose a system that incorporates multi-domain extractions of causal interactions into a single searchable knowledge graph. Our system enables users to search iteratively over direct and indirect connections in this knowledge graph, and collaboratively build causal models in real time. To enable the aggregation of causal information from multiple languages, we extend an open-domain machine reader to Portuguese. The new Portuguese reader extracts over 600 thousand causal statements from 120 thousand Portuguese publications with a precision of 62%, which demonstrates the value of mining multilingual scientific information.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "INS: An Interactive Chinese News Synthesis System",
        "paper_url": "https://aclanthology.org/N19-4004/",
        "paper_authors": [
            "Hui Liu",
            "Wentao Qin",
            "Xiaojun Wan"
        ],
        "paper_abstract": "Nowadays, we are surrounded by more and more online news articles. Tens or hundreds of news articles need to be read if we wish to explore a hot news event or topic. So it is of vital importance to automatically synthesize a batch of news articles related to the event or topic into a new synthesis article (or overview article) for reader\u2019s convenience. It is so challenging to make news synthesis fully automatic that there is no successful solution by now. In this paper, we put forward a novel Interactive News Synthesis system (i.e. INS), which can help generate news overview articles automatically or by interacting with users. More importantly, INS can serve as a tool for editors to help them finish their jobs. In our experiments, INS performs well on both topic representation and synthesis article generation. A user study also demonstrates the usefulness and users\u2019 satisfaction with the INS tool. A demo video is available at https://youtu.be/7ItteKW3GEk.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning to Respond to Mixed-code Queries using Bilingual Word Embeddings",
        "paper_url": "https://aclanthology.org/N19-4005/",
        "paper_authors": [
            "Chia-Fang Ho",
            "Jason Chang",
            "Jhih-Jie Chen",
            "Chingyu Yang"
        ],
        "paper_abstract": "We present a method for learning bilingual word embeddings in order to support second language (L2) learners in finding recurring phrases and example sentences that match mixed-code queries (e.g., \u201c\u63a5 \u53d7 sentence\u201d) composed of words in both target language and native language (L1). In our approach, mixed-code queries are transformed into target language queries aimed at maximizing the probability of retrieving relevant target language phrases and sentences. The method involves converting a given parallel corpus into mixed-code data, generating word embeddings from mixed-code data, and expanding queries in target languages based on bilingual word embeddings. We present a prototype search engine, x.Linggle, that applies the method to a linguistic search engine for a parallel corpus. Preliminary evaluation on a list of common word-translation shows that the method performs reasonablly well.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Train, Sort, Explain: Learning to Diagnose Translation Models",
        "paper_url": "https://aclanthology.org/N19-4006/",
        "paper_authors": [
            "Robert Schwarzenberg",
            "David Harbecke",
            "Vivien Macketanz",
            "Eleftherios Avramidis",
            "Sebastian M\u00f6ller"
        ],
        "paper_abstract": "Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "compare-mt: A Tool for Holistic Comparison of Language Generation Systems",
        "paper_url": "https://aclanthology.org/N19-4007/",
        "paper_authors": [
            "Graham Neubig",
            "Zi-Yi Dou",
            "Junjie Hu",
            "Paul Michel",
            "Danish Pruthi",
            "Xinyi Wang"
        ],
        "paper_abstract": "In this paper, we describe compare-mt, a tool for holistic analysis and comparison of the results of systems for language generation tasks such as machine translation. The main goal of the tool is to give the user a high-level and coherent view of the salient differences between systems that can then be used to guide further analysis or system improvement. It implements a number of tools to do so, such as analysis of accuracy of generation of particular types of words, bucketed histograms of sentence accuracies or counts based on salient characteristics, and extraction of characteristic n-grams for each system. It also has a number of advanced features such as use of linguistic labels, source side data, or comparison of log likelihoods for probabilistic models, and also aims to be easily extensible by users to new types of analysis. compare-mt is a pure-Python open source package, that has already proven useful to generate analyses that have been used in our published papers. Demo Video: https://youtu.be/NyJEQT7t2CA",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Eidos, INDRA, & Delphi: From Free Text to Executable Causal Models",
        "paper_url": "https://aclanthology.org/N19-4008/",
        "paper_authors": [
            "Rebecca Sharp",
            "Adarsh Pyarelal",
            "Benjamin Gyori",
            "Keith Alcock",
            "Egoitz Laparra",
            "Marco A. Valenzuela-Esc\u00e1rcega",
            "Ajay Nagesh",
            "Vikas Yadav",
            "John Bachman",
            "Zheng Tang",
            "Heather Lent",
            "Fan Luo",
            "Mithun Paul",
            "Steven Bethard",
            "Kobus Barnard",
            "Clayton Morrison",
            "Mihai Surdeanu"
        ],
        "paper_abstract": "Building causal models of complicated phenomena such as food insecurity is currently a slow and labor-intensive manual process. In this paper, we introduce an approach that builds executable probabilistic models from raw, free text. The proposed approach is implemented through three systems: Eidos, INDRA, and Delphi. Eidos is an open-domain machine reading system designed to extract causal relations from natural language. It is rule-based, allowing for rapid domain transfer, customizability, and interpretability. INDRA aggregates multiple sources of causal information and performs assembly to create a coherent knowledge base and assess its reliability. This assembled knowledge serves as the starting point for modeling. Delphi is a modeling framework that assembles quantified causal fragments and their contexts into executable probabilistic models that respect the semantics of the original text, and can be used to support decision making.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
        "paper_url": "https://aclanthology.org/N19-4009/",
        "paper_authors": [
            "Myle Ott",
            "Sergey Edunov",
            "Alexei Baevski",
            "Angela Fan",
            "Sam Gross",
            "Nathan Ng",
            "David Grangier",
            "Michael Auli"
        ],
        "paper_abstract": "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP",
        "paper_url": "https://aclanthology.org/N19-4010/",
        "paper_authors": [
            "Alan Akbik",
            "Tanja Bergmann",
            "Duncan Blythe",
            "Kashif Rasul",
            "Stefan Schweter",
            "Roland Vollgraf"
        ],
        "paper_abstract": "We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to \u201cmix and match\u201d various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a \u201cmodel zoo\u201d of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ChatEval: A Tool for Chatbot Evaluation",
        "paper_url": "https://aclanthology.org/N19-4011/",
        "paper_authors": [
            "Jo\u00e3o Sedoc",
            "Daphne Ippolito",
            "Arun Kirubarajan",
            "Jai Thirani",
            "Lyle Ungar",
            "Chris Callison-Burch"
        ],
        "paper_abstract": "Open-domain dialog systems (i.e. chatbots) are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing tools and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure standardization and transparency. In addition, we introduce open-source baseline models and evaluation datasets. ChatEval can be found at https://chateval.org.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LeafNATS: An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization",
        "paper_url": "https://aclanthology.org/N19-4012/",
        "paper_authors": [
            "Tian Shi",
            "Ping Wang",
            "Chandan K. Reddy"
        ],
        "paper_abstract": "Neural abstractive text summarization (NATS) has received a lot of attention in the past few years from both industry and academia. In this paper, we introduce an open-source toolkit, namely LeafNATS, for training and evaluation of different sequence-to-sequence based models for the NATS task, and for deploying the pre-trained models to real-world applications. The toolkit is modularized and extensible in addition to maintaining competitive performance in the NATS task. A live news blogging system has also been implemented to demonstrate how these models can aid blog/news editors by providing them suggestions of headlines and summaries of their articles.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "End-to-End Open-Domain Question Answering with BERTserini",
        "paper_url": "https://aclanthology.org/N19-4013/",
        "paper_authors": [
            "Wei Yang",
            "Yuqing Xie",
            "Aileen Lin",
            "Xingyu Li",
            "Luchen Tan",
            "Kun Xiong",
            "Ming Li",
            "Jimmy Lin"
        ],
        "paper_abstract": "We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "FAKTA: An Automatic End-to-End Fact Checking System",
        "paper_url": "https://aclanthology.org/N19-4014/",
        "paper_authors": [
            "Moin Nadeem",
            "Wei Fang",
            "Brian Xu",
            "Mitra Mohtarami",
            "James Glass"
        ],
        "paper_abstract": "We present FAKTA which is a unified framework that integrates various components of a fact-checking process: document retrieval from media sources with various types of reliability, stance detection of documents with respect to given claims, evidence extraction, and linguistic analysis. FAKTA predicts the factuality of given claims and provides evidence at the document and sentence level to explain its predictions.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "iComposer: An Automatic Songwriting System for Chinese Popular Music",
        "paper_url": "https://aclanthology.org/N19-4015/",
        "paper_authors": [
            "Hsin-Pei Lee",
            "Jhih-Sheng Fang",
            "Wei-Yun Ma"
        ],
        "paper_abstract": "In this paper, we introduce iComposer, an interactive web-based songwriting system designed to assist human creators by greatly simplifying music production. iComposer automatically creates melodies to accompany any given text. It also enables users to generate a set of lyrics given arbitrary melodies. iComposer is based on three sequence-to-sequence models, which are used to predict melody, rhythm, and lyrics, respectively. Songs generated by iComposer are compared with human-composed and randomly-generated ones in a subjective test, the experimental results of which demonstrate the capability of the proposed system to write pleasing melodies and meaningful lyrics at a level similar to that of humans.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",
        "paper_url": "https://aclanthology.org/N19-4016/",
        "paper_authors": [
            "Seraphina Goldfarb-Tarrant",
            "Haining Feng",
            "Nanyun Peng"
        ],
        "paper_abstract": "Story composition is a challenging problem for machines and even for humans. We present a neural narrative generation system that interacts with humans to generate stories. Our system has different levels of human interaction, which enables us to understand at what stage of story-writing human collaboration is most productive, both to improving story quality and human engagement in the writing process. We compare different varieties of interaction in story-writing, story-planning, and diversity controls under time constraints, and show that increased types of human collaboration at both planning and writing stages results in a 10-50% improvement in story quality as compared to less interactive baselines. We also show an accompanying increase in user engagement and satisfaction with stories as compared to our own less interactive systems and to previous turn-taking approaches to interaction. Finally, we find that humans tasked with collaboratively improving a particular characteristic of a story are in fact able to do so, which has implications for future uses of human-in-the-loop systems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LT Expertfinder: An Evaluation Framework for Expert Finding Methods",
        "paper_url": "https://aclanthology.org/N19-4017/",
        "paper_authors": [
            "Tim Fischer",
            "Steffen Remus",
            "Chris Biemann"
        ],
        "paper_abstract": "Expert finding is the task of ranking persons for a predefined topic or search query. Finding experts for a specified area is an important task and has attracted much attention in the information retrieval community. Most approaches for this task are evaluated in a supervised fashion, which depend on predefined topics of interest as well as gold standard expert rankings. Famous representatives of such datasets are enriched versions of DBLP provided by the ArnetMiner projet or the W3C Corpus of TREC. However, manually ranking experts can be considered highly subjective and detailed rankings are hardly distinguishable. Evaluating these datasets does not necessarily guarantee a good or bad performance of the system. Particularly for dynamic systems, where topics are not predefined but formulated as a search query, we believe a more informative approach is to perform user studies for directly comparing different methods in the same view. In order to accomplish this in a user-friendly way, we present the LT Expert Finder web-application, which is equipped with various query-based expert finding methods that can be easily extended, a detailed expert profile view, detailed evidence in form of relevant documents and statistics, and an evaluation component that allows the qualitative comparison between different rankings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SkillBot: Towards Automatic Skill Development via User Demonstration",
        "paper_url": "https://aclanthology.org/N19-4018/",
        "paper_authors": [
            "Yilin Shen",
            "Avik Ray",
            "Hongxia Jin",
            "Sandeep Nama"
        ],
        "paper_abstract": "We present SkillBot that takes the first step to enable end users to teach new skills in personal assistants (PA). Unlike existing PA products that need software developers to build new skills via IDE tools, an end user can use SkillBot to build new skills just by naturally demonstrating the task on device screen. SkillBot automatically develops a natural language understanding (NLU) engine and implements the action without the need of coding. On both benchmark and in-house datasets, we validate the competitive performance of SkillBot automatically built NLU. We also observe that it only takes a few minutes for an end user to build a new skill using SkillBot.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Multilingual Entity, Relation, Event and Human Value Extraction",
        "paper_url": "https://aclanthology.org/N19-4019/",
        "paper_authors": [
            "Manling Li",
            "Ying Lin",
            "Joseph Hoover",
            "Spencer Whitehead",
            "Clare Voss",
            "Morteza Dehghani",
            "Heng Ji"
        ],
        "paper_abstract": "This paper demonstrates a state-of-the-art end-to-end multilingual (English, Russian, and Ukrainian) knowledge extraction system that can perform entity discovery and linking, relation extraction, event extraction, and coreference. It extracts and aggregates knowledge elements across multiple languages and documents as well as provides visualizations of the results along three dimensions: temporal (as displayed in an event timeline), spatial (as displayed in an event heatmap), and relational (as displayed in entity-relation networks). For our system to further support users\u2019 analyses of causal sequences of events in complex situations, we also integrate a wide range of human moral value measures, independently derived from region-based survey, into the event heatmap. This system is publicly available as a docker container and a live demo.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Litigation Analytics: Extracting and querying motions and orders from US federal courts",
        "paper_url": "https://aclanthology.org/N19-4020/",
        "paper_authors": [
            "Thomas Vacek",
            "Dezhao Song",
            "Hugo Molina-Salgado",
            "Ronald Teo",
            "Conner Cowling",
            "Frank Schilder"
        ],
        "paper_abstract": "Legal litigation planning can benefit from statistics collected from past decisions made by judges. Information on the typical duration for a submitted motion, for example, can give valuable clues for developing a successful strategy. Such information is encoded in semi-structured documents called dockets. In order to extract and aggregate this information, we deployed various information extraction and machine learning techniques. The aggregated data can be queried in real time within the Westlaw Edge search engine. In addition to a keyword search for judges, lawyers, law firms, parties and courts, we also implemented a question answering interface that offers targeted questions in order to get to the respective answers quicker.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Community lexical access for an endangered polysynthetic language: An electronic dictionary for St. Lawrence Island Yupik",
        "paper_url": "https://aclanthology.org/N19-4021/",
        "paper_authors": [
            "Benjamin Hunt",
            "Emily Chen",
            "Sylvia L.R. Schreiner",
            "Lane Schwartz"
        ],
        "paper_abstract": "In this paper, we introduce a morphologically-aware electronic dictionary for St. Lawrence Island Yupik, an endangered language of the Bering Strait region. Implemented using HTML, Javascript, and CSS, the dictionary is set in an uncluttered interface and permits users to search in Yupik or in English for Yupik root words and Yupik derivational suffixes. For each matching result, our electronic dictionary presents the user with the corresponding entry from the Badten (2008) Yupik-English paper dictionary. Because Yupik is a polysynthetic language, handling of multimorphemic word forms is critical. If a user searches for an inflected Yupik word form, we perform a morphological analysis and return entries for the root word and for any derivational suffixes present in the word. This electronic dictionary should serve not only as a valuable resource for all students and speakers of Yupik, but also for field linguists working towards documentation and conservation of the language.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Visualizing Inferred Morphotactic Systems",
        "paper_url": "https://aclanthology.org/N19-4022/",
        "paper_authors": [
            "Haley Lepp",
            "Olga Zamaraeva",
            "Emily M. Bender"
        ],
        "paper_abstract": "We present a web-based system that facilitates the exploration of complex morphological patterns found in morphologically very rich languages. The need for better understanding of such patterns is urgent for linguistics and important for cross-linguistically applicable natural language processing. In this paper we give an overview of the system architecture and describe a sample case study on Abui [abz], a Trans-New Guinea language spoken in Indonesia.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Research Platform for Multi-Robot Dialogue with Humans",
        "paper_url": "https://aclanthology.org/N19-4023/",
        "paper_authors": [
            "Matthew Marge",
            "Stephen Nogar",
            "Cory J. Hayes",
            "Stephanie M. Lukin",
            "Jesse Bloecker",
            "Eric Holder",
            "Clare Voss"
        ],
        "paper_abstract": "This paper presents a research platform that supports spoken dialogue interaction with multiple robots. The demonstration showcases our crafted MultiBot testing scenario in which users can verbally issue search, navigate, and follow instructions to two robotic teammates: a simulated ground robot and an aerial robot. This flexible language and robotic platform takes advantage of existing tools for speech recognition and dialogue management that are compatible with new domains, and implements an inter-agent communication protocol (tactical behavior specification), where verbal instructions are encoded for tasks assigned to the appropriate robot.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Chat-crowd: A Dialog-based Platform for Visual Layout Composition",
        "paper_url": "https://aclanthology.org/N19-4024/",
        "paper_authors": [
            "Paola Cascante-Bonilla",
            "Xuwang Yin",
            "Vicente Ordonez",
            "Song Feng"
        ],
        "paper_abstract": "In this paper we introduce Chat-crowd, an interactive environment for visual layout composition via conversational interactions. Chat-crowd supports multiple agents with two conversational roles: agents who play the role of a designer are in charge of placing objects in an editable canvas according to instructions or commands issued by agents with a director role. The system can be integrated with crowdsourcing platforms for both synchronous and asynchronous data collection and is equipped with comprehensive quality controls on the performance of both types of agents. We expect that this system will be useful to build multimodal goal-oriented dialog tasks that require spatial and geometric reasoning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials",
        "paper_url": "https://aclanthology.org/N19-5000/",
        "paper_authors": [
            "Anoop Sarkar",
            "Michael Strube"
        ],
        "paper_abstract": "",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Deep Adversarial Learning for NLP",
        "paper_url": "https://aclanthology.org/N19-5001/",
        "paper_authors": [
            "William Yang Wang",
            "Sameer Singh",
            "Jiwei Li"
        ],
        "paper_abstract": "Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. Adversarial learning is also a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming at providing some practical perspectives on the future of adversarial learning for solving real-world NLP problems.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Deep Learning for Natural Language Inference",
        "paper_url": "https://aclanthology.org/N19-5002/",
        "paper_authors": [
            "Samuel Bowman",
            "Xiaodan Zhu"
        ],
        "paper_abstract": "This tutorial discusses cutting-edge research on NLI, including recent advance on dataset development, cutting-edge deep learning models, and highlights from recent research on using NLI to understand capabilities and limits of deep learning models for language understanding and reasoning.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Measuring and Modeling Language Change",
        "paper_url": "https://aclanthology.org/N19-5003/",
        "paper_authors": [
            "Jacob Eisenstein"
        ],
        "paper_abstract": "This tutorial is designed to help researchers answer the following sorts of questions: - Are people happier on the weekend? - What was 1861\u2019s word of the year? - Are Democrats and Republicans more different than ever? - When did \u201cgay\u201d stop meaning \u201chappy\u201d? - Are gender stereotypes getting weaker, stronger, or just different? - Who is a linguistic leader? - How can we get internet users to be more polite and objective? Such questions are fundamental to the social sciences and humanities, and scholars in these disciplines are increasingly turning to computational techniques for answers. Meanwhile, the ACL community is increasingly engaged with data that varies across time, and with the social insights that can be offered by analyzing temporal patterns and trends. The purpose of this tutorial is to facilitate this convergence in two main ways: 1. By synthesizing recent computational techniques for handling and modeling temporal data, such as dynamic word embeddings, the tutorial will provide a starting point for future computational research. It will also identify useful tools for social scientists and digital humanities scholars. 2. The tutorial will provide an overview of techniques and datasets from the quantitative social sciences and the digital humanities, which are not well-known in the computational linguistics community. These techniques include vector autoregressive models, multiple comparisons corrections for hypothesis testing, and causal inference. Datasets include historical newspaper archives and corpora of contemporary political speech.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Transfer Learning in Natural Language Processing",
        "paper_url": "https://aclanthology.org/N19-5004/",
        "paper_authors": [
            "Sebastian Ruder",
            "Matthew E. Peters",
            "Swabha Swayamdipta",
            "Thomas Wolf"
        ],
        "paper_abstract": "The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Language Learning and Processing in People and Machines",
        "paper_url": "https://aclanthology.org/N19-5005/",
        "paper_authors": [
            "Aida Nematzadeh",
            "Richard Futrell",
            "Roger Levy"
        ],
        "paper_abstract": "The goal of this tutorial is to bring the fields of computational linguistics and computational cognitive science closer: we will introduce different stages of language acquisition and their parallel problems in NLP. As an example, one of the early challenges children face is mapping the meaning of word labels (such as \u201ccat\u201d) to their referents (the furry animal in the living room). Word learning is similar to the word alignment problem in machine translation. We explain the current computational models of language acquisition, their limitations, and how the insights from these models can be incorporated into NLP applications. Moreover, we discuss how we can take advantage of the cognitive science of language in computational linguistics: for example, by designing cognitively-motivated evaluations task or buildings language-learning inductive biases into our models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Applications of Natural Language Processing in Clinical Research and Practice",
        "paper_url": "https://aclanthology.org/N19-5006/",
        "paper_authors": [
            "Yanshan Wang",
            "Ahmad Tafti",
            "Sunghwan Sohn",
            "Rui Zhang"
        ],
        "paper_abstract": "Rapid growth in adoption of electronic health records (EHRs) has led to an unprecedented expansion in the availability of large longitudinal datasets. Large initiatives such as the Electronic Medical Records and Genomics (eMERGE) Network, the Patient-Centered Outcomes Research Network (PCORNet), and the Observational Health Data Science and Informatics (OHDSI) consortium, have been established and have reported successful applications of secondary use of EHRs in clinical research and practice. In these applications, natural language processing (NLP) technologies have played a crucial role as much of detailed patient information in EHRs is embedded in narrative clinical documents. Meanwhile, a number of clinical NLP systems, such as MedLEE, MetaMap/MetaMap Lite, cTAKES, and MedTagger have been developed and utilized to extract useful information from diverse types of clinical text, such as clinical notes, radiology reports, and pathology reports. Success stories in applying these tools have been reported widely. Despite the demonstrated success of NLP in the clinical domain, methodologies and tools developed for the clinical NLP are still underknown and underutilized by students and experts in the general NLP domain, mainly due to the limited exposure to EHR data. Through this tutorial, we would like to introduce NLP methodologies and tools developed in the clinical domain, and showcase the real-world NLP applications in clinical research and practice at Mayo Clinic (the No. 1 national hospital ranked by the U.S. News & World Report) and the University of Minnesota (the No. 41 best global universities ranked by the U.S. News & World Report). We will review NLP techniques in solving clinical problems and facilitating clinical research, the state-of-the art clinical NLP tools, and share collaboration experience with clinicians, as well as publicly available EHR data and medical resources, and finally conclude the tutorial with vast opportunities and challenges of clinical NLP. The tutorial will provide an overview of clinical backgrounds, and does not presume knowledge in medicine or health care. The goal of this tutorial is to encourage NLP researchers in the general domain (as opposed to the specialized clinical domain) to contribute to this burgeoning area. In this tutorial, we will first present an overview of clinical NLP. We will then dive into two subareas of clinical NLP in clinical research, including big data infrastructure for large-scale clinical NLP and advances of NLP in clinical research, and two subareas in clinical practice, including clinical information extraction and patient cohort retrieval using EHRs. Around 70% of the tutorial will review clinical problems, cutting-edge methodologies, and real-world clinical NLP tools while another 30% introduce use cases at Mayo Clinic and the University of Minnesota. Finally, we will conclude the tutorial with challenges and opportunities in this rapidly developing domain.",
        "paper_code": "#",
        "paper_cite": -1
    }
]