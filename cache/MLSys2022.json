[
    {
        "paper_name": "Matchmaker: Data Drift Mitigation in Machine Learning for Large-Scale Systems",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/069a002768bcb31509d4901961f23b3c-Abstract.html",
        "paper_authors": [
            "Ankur Mallick",
            "Kevin Hsieh",
            "Behnaz Arzani",
            "Gauri Joshi"
        ],
        "paper_abstract": "Today's data centers rely more heavily on machine learning (ML) in their deployed systems. However, these systems are vulnerable to the data drift problem, that is, a mismatch between training and test data, which can lead to significant performance degradation and system inefficiencies. In this paper, we demonstrate the impact of data drift in production by studying two real-world deployments in a leading cloud provider. Our study shows that, despite frequent model retraining, these deployed models experience major accuracy drops (up to 40%) and high accuracy variation, which lead to drastic increase in operational costs. None of the current solutions to the data drift problem are designed for large-scale deployments, which need to address real-world issues such as scale, ground truth latency, and mixed types of data drift. We propose Matchmaker, the first scalable, adaptive, and flexible solution to the data drift problem in large-scale production systems. Matchmaker finds the most similar training data batch and uses the corresponding ML model for inference on each test point. As part of Matchmaker, we introduce a novel similarity metric to address multiple types of data drifts while only incurring limited overhead. Experiments on our two real-world ML deployments show matchmaker significantly improve model accuracy (upto 14\\% and 2\\%), which saves 18\\% and 1\\% in the operational costs. At the same time, Matchmaker provides 8x- and 4x- faster predictions than a state-of-the-art ML data drift solution, AUE.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Hydrozoa: Dynamic Hybrid-Parallel DNN Training on Serverless Containers",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/0a5209deb79dc584d8ddb41a792d8549-Abstract.html",
        "paper_authors": [
            "Runsheng Guo",
            "Victor Guo",
            "Antonio Kim",
            "Josh Hildred",
            "Khuzaima Daudjee"
        ],
        "paper_abstract": "Deep Neural Networks (DNNs) are often trained in parallel on a cluster of virtual machines (VMs) so as to reduce training time. However, this requires explicit cluster management, which is cumbersome and often results in costly overprovisioning of resources. Training DNNs on serverless compute is an attractive alternative that is receiving growing interest. In a serverless environment, users do not need to handle cluster management and can scale compute resources at a fine-grained level while paying for resources only when actively used. Despite these potential benefits, existing serverless systems for DNN training are ineffective because they are limited to CPU-based training and bottlenecked by expensive distributed communication. We present Hydrozoa, a system that trains DNNs on serverless containers with a hybrid-parallel architecture that flexibly combines data- and model-parallelism. Hydrozoa supports GPU-based training and leverages hybrid-parallelism and serverless resource scaling to achieve up to 155.5x and 5.4x higher throughput-per-dollar compared to existing serverless and VM-based training systems. Hydrozoa also allows users to implement dynamic worker-scaling policies during training. We show that dynamic worker scaling improves statistical training efficiency and reduces training costs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SRIFTY: Swift and Thrifty Distributed Neural Network Training on the Cloud",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/0cafb7890f6a7d4de65507d5bb7e0187-Abstract.html",
        "paper_authors": [
            "Liang Luo",
            "Peter West",
            "Pratyush Patel",
            "Arvind Krishnamurthy",
            "Luis Ceze"
        ],
        "paper_abstract": "Finding the best VM configuration is key to achieve lower cost and higher throughput, two primary concerns in cloud-based distributed neural network (NN) training today. Optimal VM selection that meets user constraints requires efficiently navigating a large search space while controlling for the performance variance associated with sharing cloud instances and networks.In this work, we characterize this variance in the context of distributed NN training and present results of a comprehensive throughput and cost-efficiency study we conducted across a wide array of instances to prune for the optimal VM search space. Using insights from these studies, we built Srifty, a system that combines runtime profiling with learned performance models to accurately predict training performance and find the best VM choice that satisfies user constraints, potentially leveraging both heterogeneous setups and spot instances. We integrated Srifty with PyTorch and evaluated it on Amazon EC2. We conducted a large-scale generalization study of Srifty across more than 2K training setups on EC2. Our results show that Srifty achieves an iteration latency prediction error of 8%, and its VM instance recommendations offer significant throughput gain and cost reduction while satisfying user constraints compared to existing solutions in complex, real-world scenarios.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TyXe: Pyro-based Bayesian neural nets for Pytorch",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/14bc46029b7ac590f56a203e0a3ef586-Abstract.html",
        "paper_authors": [
            "Hippolyt Ritter",
            "Theofanis Karaletsos"
        ],
        "paper_abstract": "We introduce TyXe, a Bayesian neural network library built on top of Pytorch and Pyro. Our leading design principle is to cleanly separate architecture, prior, inference and likelihood specification, allowing for a flexible workflow where users can quickly iterate over combinations of these components. In contrast to existing packages TyXe does not implement any layer classes, and instead relies on architectures defined in generic Pytorch code. TyXe then provides modular choices for canonical priors, variational guides, inference techniques, and layer selections for a Bayesian treatment of the specified architecture. Sampling tricks for variance reduction, such as local reparameterization or flipout, are implemented as effect handlers, which can be applied independently of other specifications. We showcase the ease of use of TyXe to explore Bayesian versions of popular models from various libraries: toy regression with a pure Pytorch neural network; large-scale image classification with torchvision ResNets; graph neural networks based on DGL; and Neural Radiance Fields built on top of Pytorch3D. Finally, we provide convenient abstractions for variational continual learning. In all cases the change from a deterministic to a Bayesian neural network comes with minimal modifications to existing code, offering a broad range of researchers and practitioners alike practical access to uncertainty estimation techniques. The library is available at https://github.com/TyXe-BDL/TyXe.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Tale of Two Models: Constructing Evasive Attacks on Edge Models",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/1ccc3bfa05cb37b917068778f3c4523a-Abstract.html",
        "paper_authors": [
            "Wei Hao",
            "Aahil  Awatramani",
            "Jiayang Hu",
            "Chengzhi Mao",
            "Pin-Chun  Chen",
            "Eyal Cidon",
            "Asaf Cidon",
            "Junfeng Yang"
        ],
        "paper_abstract": "Full-precision deep learning models are typically too large or costly to deploy on edge devices. To accommodate to the limited hardware resources, models are adapted to the edge using various edge-adaptation techniques, such as quantization and pruning.While such techniques may have a negligible impact on top-line accuracy, the adapted models exhibit subtle differences in output compared to the original model from which they are derived.In this paper, we introduce a new evasive attack, DIVA, that exploits these differences in edge adaptation, by adding adversarial noise to input data that maximizes the output difference between the original and adapted model. Such an attack is particularly dangerous, because the malicious input will trick the adapted model running on the edge, but will be virtually undetectable by the original model, which typically serves as the authoritative model version, used for validation, debugging and retraining.We compare DIVA to a state-of-the-art attack, PGD, and show that DIVA is only 1.7--3.6% worse on attacking the adapted model but 1.9--4.2 times more likely not to be detected by the the original model under a whitebox and semi-blackbox setting, compared to PGD.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Sequential Aggregation and Rematerialization: Distributed Full-batch Training of Graph Neural Networks on Large Graphs",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/1d781258d409a6efc66cd1aa14a1681c-Abstract.html",
        "paper_authors": [
            "Hesham Mostafa"
        ],
        "paper_abstract": "We present the Sequential Aggregation and Rematerialization (SAR) scheme for distributed full-batch training of Graph Neural Networks (GNNs) on large graphs. Large-scale training of GNNs has recently been dominated by sampling-based methods and methods based on non-learnable message passing.  SAR on the other hand is a distributed technique that can train any GNN type directly on an entire large graph. The key innovation in SAR is the distributed sequential rematerialization scheme which sequentially re-constructs then frees pieces of the prohibitively large GNN computational graph during the backward pass. This results in excellent memory scaling behavior where the memory consumption per worker goes down linearly with the number of workers, even for densely connected graphs. Using SAR, we report the largest applications of full-batch GNN training to-date, and demonstrate large memory savings as the number of workers increases. We also present a general technique based on kernel fusion and attention-matrix rematerialization to optimize both the runtime and memory efficiency of attention-based models. We show that, coupled with SAR, our optimized attention kernels lead to significant speedups and memory savings in attention-based GNNs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Random Offset Block Embedding (ROBE) for compressed embedding tables in deep learning recommendation systems",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/1eb34d662b67a14e3511d0dfd78669be-Abstract.html",
        "paper_authors": [
            "Aditya Desai",
            "Li Chou",
            "Anshumali Shrivastava"
        ],
        "paper_abstract": "Deep learning for recommendation data is one of the most pervasive and challenging AI workload in recent times. State-of-the-art recommendation models are one of the largest models matching the likes of GPT-3 and Switch Transformer. Challenges in deep learning recommendation models (DLRM) stem from learning dense embeddings for each of the categorical tokens. These embedding tables in industrial scale models can be as large as hundreds of terabytes. Such large models lead to a plethora of engineering challenges, not to mention prohibitive communication overheads, and slower training and inference times. Of these, slower inference time directly impacts user experience. Model compression for DLRM is gaining traction and the community has recently shown impressive compression results.  In this paper, we present Random Offset Block Embedding Array (ROBE)  as a low memory alternative to embedding tables which provide orders of magnitude reduction in memory usage while maintaining accuracy and boosting execution speed. ROBE is a simple fundamental approach in improving both cache performance and the variance of randomized hashing, which could be of independent interest in itself. We demonstrate that we can successfully train DLRM models with same accuracy while using $1000 \\times$ less memory. A $1000\\times$ compressed model directly results in faster inference without any engineering effort. In particular, we show that we can train DLRM model using ROBE array of size 100MB on a single GPU to achieve AUC of 0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model of 100GB while achieving about $3.1\\times$ (209\\%) improvement in inference throughput.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bolt: Bridging the Gap between Auto-tuners and Hardware-native Performance",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/1f8053a67ec8e0b57455713cefdd8218-Abstract.html",
        "paper_authors": [
            "Jiarong Xing",
            "Leyuan Wang",
            "Shang Zhang",
            "Jack Chen",
            "Ang Chen",
            "Yibo Zhu"
        ],
        "paper_abstract": "Today\u2019s auto-tuners (e.g., AutoTVM, Ansor) generate efficient tensor programs by navigating a large search space to identify effective implementations, but they do so with opaque hardware details. Thus, their performance could fall behind that of hardware-native libraries (e.g., cuBLAS, cuDNN), which are hand-optimized by device vendors to extract high performance. On the other hand, these vendor libraries have a fixed set of supported functions and lack the customization and automation support afforded by auto-tuners. Bolt bridges this gap and achieves the best of both worlds by using hardware-native templated search, which is enabled by the recent trend that vendor libraries (e.g., CUTLASS) are increasingly modularized and reconfigurable. Bolt provides new opportunities to rethink end-to-end tensor optimizations at the graph, operator, and model levels. We demonstrate this concept by prototyping in TVM on NVIDIA GPUs\u2014both in large deployment in our production environment. Our experiments show that Bolt can improve the inference speed of common convolutional neural networks by 2.5x on average over the state of the art, and it auto-tunes these models within 20 minutes.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "NURD: Negative-Unlabeled Learning for Online Datacenter Straggler Prediction",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/21d45762b38dbbd05c0ddbd6601d8f6c-Abstract.html",
        "paper_authors": [
            "Yi Ding",
            "Avinash Rao",
            "Hyebin Song",
            "Rebecca Willett",
            "Henry (Hank) Hoffmann"
        ],
        "paper_abstract": "Datacenters execute large computational jobs, which are composed of smaller tasks. A job completes when all its tasks finish, so stragglers---rare, yet extremely slow tasks---are a major impediment to datacenter performance. Accurately predicting stragglers would enable proactive intervention, allowing datacenter operators to mitigate stragglers before they delay a job. While much prior work applies machine learning to predict computer system performance, these approaches rely on complete labels---i.e., sufficient examples of all possible behaviors, including straggling and non-straggling---or strong assumptions about the underlying latency distributions---e.g., whether Gaussian or not. Within a running job, however, none of this information is available until stragglers have revealed themselves when they have already delayed the job. To predict stragglers accurately and early without labeled positive examples or assumptions on latency distributions, this paper presents NURD, a novel Negative-Unlabeled learning approach with Reweighting and Distribution-compensation that only trains on negative and unlabeled streaming data. The key idea is to train a predictor using finished tasks of non-stragglers to predict latency for unlabeled running tasks, and then reweight each unlabeled task's prediction based on a weighting function of its feature space. We evaluate NURD on two production traces from Google and Alibaba, and find that compared to the best baseline approach, NURD produces 2--11 percentage point increases in the F1 score in terms of prediction accuracy, and 4.7--8.8 percentage point improvements in job completion time.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary Data",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/294f82c43d69f66c04440cbb2740e52d-Abstract.html",
        "paper_authors": [
            "Wasu Piriyakulkij",
            "Cristina Menghini",
            "Ross Briden",
            "Nihal Vivekanand Nayak",
            "Jeffrey Zhu",
            "Elaheh Raisi",
            "Stephen Bach"
        ],
        "paper_abstract": "Machine learning practitioners often have access to a spectrum of data: labeled data for the target task (which is often limited), unlabeled data, and auxiliary data, the many available labeled datasets for other tasks. We describe TAGLETS, a system built to study techniques for automatically exploiting all three types of data and creating high-quality, servable classifiers. The key components of TAGLETS are: (1) auxiliary data organized according to a knowledge graph, (2) modules encapsulating different methods for exploiting auxiliary and unlabeled data, and (3) a distillation stage in which the ensembled modules are combined into a servable model. We compare TAGLETS with state-of-the-art transfer learning and semi-supervised learning methods on four image classification tasks. Our study covers a range of settings, varying the amount of labeled data and the semantic relatedness of the auxiliary data to the target task. We find that the intelligent incorporation of auxiliary and unlabeled data into multiple learning techniques enables TAGLETS to match---and most often significantly surpass---these alternatives. TAGLETS is available as an open-source system at github.com/anonymous.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Collapsible Linear Blocks for Super-Efficient Super Resolution",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/3134f61af2136e249b0d8f190cbdc508-Abstract.html",
        "paper_authors": [
            "Kartikeya Bhardwaj",
            "Milos Milosavljevic",
            "Liam O'Neil",
            "Dibakar Gope",
            "Ramon Matas",
            "Alex Chalfin",
            "Naveen Suda",
            "Lingchuan Meng",
            "Danny Loh"
        ],
        "paper_abstract": "With the advent of smart devices that support 4K and 8K resolution, Single Image Super Resolution (SISR) has become an important computer vision problem. However, most super resolution deep networks are computationally very expensive. In this paper, we propose Super-Efficient Super Resolution (SESR) networks that establish a new state-of-the-art for efficient super resolution. Our approach is based on linear overparameterization of CNNs and creates an efficient model architecture for SISR. With theoretical analysis, we uncover the limitations of existing overparameterization methods and show how the proposed method alleviates them. Detailed experiments across six benchmark datasets demonstrate that SESR achieves similar or better image quality than state-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate (MAC) operations. As a result, SESR can be used on constrained hardware to perform x2 (1080p to 4K) and x4 (1080p to 8K) SISR. Towards this, we estimate hardware performance numbers for a commercial Arm mobile-Neural Processing Unit (NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the challenges faced by super resolution on AI accelerators and demonstrate that SESR is significantly faster (e.g., 6x-8x higher FPS) than existing models on mobile-NPU. Finally, SESR outperforms prior models by 1.5x-2x in latency on Arm CPU and GPU when deployed on a real mobile device. The code for this work is available at https://github.com/ARM-software/sesr.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Pathways: Asynchronous Distributed Dataflow for ML",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/37385144cac01dff38247ab11c119e3c-Abstract.html",
        "paper_authors": [
            "Paul Barham",
            "Aakanksha Chowdhery",
            "Jeff Dean",
            "Sanjay Ghemawat",
            "Steven Hand",
            "Daniel Hurt",
            "Michael Isard",
            "Hyeontaek Lim",
            "Ruoming Pang",
            "Sudip Roy",
            "Brennan Saeta",
            "Parker Schuh",
            "Ryan Sepassi",
            "Laurent  Shafey",
            "Chandu Thekkath",
            "Yonghui Wu"
        ],
        "paper_abstract": "We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity (~100% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Randomness in Neural Network Training: Characterizing the Impact of Tooling",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/427e0e886ebf87538afdf0badb805b7f-Abstract.html",
        "paper_authors": [
            "Donglin Zhuang",
            "Xingyao Zhang",
            "Shuaiwen Song",
            "Sara Hooker"
        ],
        "paper_abstract": "The quest for determinism in machine learning has disproportionately focused on characterizing the impact of noise introduced by algorithmic design choices. In this work, we address a less well understood and studied question: how does our choice of tooling introduce randomness to deep neural network training. We conduct large scale experiments across different types of hardware, accelerators, state-of-the-art networks, and open-source datasets, to characterize how tooling choices contribute to the level of non-determinism in a system, the impact of said non-determinism, and the cost of eliminating different sources of noise. Our findings suggest that the impact of non-determinism is nuanced. While top-line metrics such as top-1 accuracy are not noticeably impacted, model performance on certain parts of the data distribution is far more sensitive to the introduction of randomness. Our results suggest that deterministic tooling is critical for AI safety. However, we also find that the cost of ensuring determinism varies dramatically between neural network architectures and hardware types,  e.g., with overhead up to \\textit{746\\%} on a spectrum of widely used GPU accelerator architectures, relative to non-deterministic training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Sustainable AI: Environmental Implications, Challenges and Opportunities",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/462211f67c7d858f663355eff93b745e-Abstract.html",
        "paper_authors": [
            "Carole-Jean Wu",
            "Ramya Raghavendra",
            "Udit Gupta",
            "Bilge Acun",
            "Newsha Ardalani",
            "Kiwan Maeng",
            "Gloria Chang",
            "Fiona Aga",
            "Jinshi Huang",
            "Charles Bai",
            "Michael Gschwind",
            "Anurag Gupta",
            "Myle Ott",
            "Anastasia Melnikov",
            "Salvatore Candido",
            "David Brooks",
            "Geeta Chauhan",
            "Benjamin Lee",
            "Hsien-Hsin Lee",
            "Bugra Akyildiz",
            "Maximilian Balandat",
            "Joe Spisak",
            "Ravi Jain",
            "Mike Rabbat",
            "Kim Hazelwood"
        ],
        "paper_abstract": "This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "URSABench: A System for Comprehensive Benchmarking of Bayesian Deep Neural Network Models and Inference methods",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/4a420924d20bc025ebb37849169e6ebd-Abstract.html",
        "paper_authors": [
            "Meet  Vadera",
            "Jinyang Li",
            "Adam Cobb",
            "Brian Jalaian",
            "Tarek Abdelzaher",
            "Benjamin Marlin"
        ],
        "paper_abstract": "While deep learning methods continue to improve in predictive accuracy on a wide range of application domains, significant issues remain with other aspects of their performance, including their ability to quantify uncertainty and their robustness. Recent advances in approximate Bayesian inference hold significant promise for addressing these concerns, but the computational scalability of these methods can be problematic when applied to large-scale models. In this paper, we present URSABench (the Uncertainty, Robustness, Scalability, and Accuracy Benchmark), an open-source suite of models, inference methods, tasks and benchmarking tools. URSABench supports comprehensive assessment of Bayesian deep learning models and approximate Bayesian inference methods, with a focus on classification tasks performed both on server and edge GPUs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Towards the Co-design of Neural Networks and Accelerators",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/4c430a4d0a7de11e85fa5b076e7f1895-Abstract.html",
        "paper_authors": [
            "Yanqi Zhou",
            "Xuanyi Dong",
            "Tianjian Meng",
            "Mingxing Tan",
            "Berkin Akin",
            "Daiyi Peng",
            "Amir Yazdanbakhsh",
            "Da Huang",
            "Ravi Narayanaswami",
            "James Laudon"
        ],
        "paper_abstract": "Better neural architectures and new hardware accelerators are two driving forces for the progress in deep learning. Previous works typically focus on one aspect: they either design new neural architectures for fixed hardware like GPUs or customize hardware (often on FPGAs) for a fixed set of neural models like ResNets or Transformers. In this work, we aim to jointly optimize neural architecture and hardware configurations for Google's Edge TPUs. Through extensive studies, we observe that: 1) the neural architecture search space has to be customized to fully leverage the targeted hardware, 2) neural architecture and hardware accelerator should be jointly searched to achieve the best of both worlds, and 3) conventional metrics such as FLOPs and parameter size often do not well represent model efficiency in real accelerators. Our experiments show that our joint search approach, named NaaS, consistently outperforms previous state-of-the-art results, such as EfficientNet, on both image classification and segmentation tasks. Furthermore, our approach reduces energy consumption by up to 2x under the same accuracy on Edge TPUs.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "QuadraLib: A Performant Quadratic Neural Network Library for Architecture Optimization and Design Exploration",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/6270a15843a2e06a95d3e3ad8b489e4b-Abstract.html",
        "paper_authors": [
            "Zirui Xu",
            "Fuxun Yu",
            "Jinjun Xiong",
            "Xiang Chen"
        ],
        "paper_abstract": "The significant success of Deep Neural Networks (DNNs) is highly promoted by the multiple sophisticated DNN libraries. On the contrary, although some work have proved that Quadratic Deep Neuron Networks (QDNNs) show better non-linearity and learning capability than the traditional first-order DNNs, their neuron design suffers certain drawbacks from theoretical performance to practical deployment. In this paper, we first proposed a new QDNN neuron architecture design, and further developed QuadraLib, a QDNN library to provide architecture optimization and design exploration for QDNNs. Extensive experiments show that our design has better performance regarding prediction accuracy and computation consumption on multiple learning tasks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Revelio: ML-Generated Debugging Queries for Finding Root Causes in Distributed Systems",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/650e2245aa3513ed517f4cf1b3d58e06-Abstract.html",
        "paper_authors": [
            "Pradeep Dogga",
            "Karthik Narasimhan",
            "Anirudh Sivaraman",
            "Shiv Saini",
            "George Varghese",
            "Ravi Netravali"
        ],
        "paper_abstract": "A major difficulty in debugging distributed systems lies in manually determining which of the many available debugging tools to use and how to query that tool\u2019s logs. Our own study of a production debugging workflow confirms the magnitude of this burden. This paper explores whether a deep neural network trained on past bug reports and debugging logs can assist developers in distributed systems debugging. We present Revelio, a debugging assistant which takes user reports and system logs as input, and outputs debugging queries that developers can use to find a bug\u2019s root cause. The key challenges lie in (1) combining inputs of different types (e.g., natural language reports and quantitative logs) and (2) generalizing to unseen faults. Revelio addresses these by employ-ing deep neural networks to uniformly embed diverse input sources and potential queries into a high-dimensional vector space. In addition, it exploits observations from production systems to factorize query generation into two computationally and statistically simpler learning tasks. To evaluate Revelio, we built a testbed with multiple distributed applications and debugging tools. By injecting faults and training on logs and reports from 800 Mechanical Turkers, we show that Revelio includes the most helpful query in its predicted list of top-3 relevant queries 96% of the time. Our developer study confirms the utility of Revelio.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node Sampling",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/676638b91bc90529e09b22e58abb01d6-Abstract.html",
        "paper_authors": [
            "Cheng Wan",
            "Youjie Li",
            "Ang Li",
            "Nam Sung Kim",
            "Yingyan Lin"
        ],
        "paper_abstract": "Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art method for graph-based learning tasks. However, training GCNs at scale is still challenging, hindering both the exploration of more sophisticated GCN architectures and their applications to real-world large graphs. While it might be natural to consider graph partition and distributed training for tackling this challenge, this direction has only been slightly scratched the surface in the previous works due to the limitations of existing designs. In this work, we first analyze why distributed GCN training is ineffective and identify the underlying cause to be the excessive number of boundary nodes of each partitioned subgraph, which easily explodes the memory and communication costs for GCN training. Furthermore, we propose a simple yet effective method dubbed BNS-GCN that adopts random Boundary-Node-Sampling to enable efficient and scalable distributed GCN training. Experiments and ablation studies consistently validate the effectiveness of BNS-GCN, e.g., boosting the throughput by up to 16.2\u00d7 and reducing the memory usage by up to 58%, while maintaining a full-graph accuracy. Furthermore, both theoretical and empirical analysis show that BNS-GCN enjoys a better convergence than existing sampling-based methods. We believe that our BNS-GCN has opened up a new paradigm for enabling GCN training at scale. The code is available at https://github.com/RICE-EIC/BNS-GCN.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "LightSecAgg: a Lightweight and Versatile Design for Secure Aggregation in Federated Learning",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/6c44dc73014d66ba49b28d483a8f8b0d-Abstract.html",
        "paper_authors": [
            "Jinhyun So",
            "Chaoyang He",
            "Chien-Sheng Yang",
            "Songze Li",
            "Qian Yu",
            "Ramy E. Ali",
            "Basak Guler",
            "Salman Avestimehr"
        ],
        "paper_abstract": "Secure model aggregation is a key component of federated learning (FL) that aims at protecting the privacy of each user\u2019s individual model while allowing for their global aggregation. It can be applied to any aggregation-based FL approach for training a global or personalized model. Model aggregation needs to also be resilient against likely user dropouts in FL systems, making its design substantially more complex. State-of-the-art secure aggregation protocols rely on secret sharing of the random-seeds used for mask generations at the users to enable the reconstruction and cancellation of those belonging to the dropped users. The complexity of such approaches, however, grows substantially with the number of dropped users. We propose a new approach, named LightSecAgg, to overcome this bottleneck by changing the design from random-seed reconstruction of the dropped users'' toone-shot aggregate-mask reconstruction of the active users via mask encoding/decoding''. We show that LightSecAgg achieves the same privacy and dropout-resiliency guarantees as the state-of-the-art protocols while significantly reducing the overhead for resiliency against dropped users. We also demonstrate that, unlike existing schemes, LightSecAgg can be applied to secure aggregation in the asynchronous FL setting. Furthermore, we provide a modular system design and optimized on-device parallelization for scalable implementation, by enabling computational overlapping between model training and on-device encoding, as well as improving the speed of concurrent receiving and sending of chunked masks. We evaluate LightSecAgg via extensive experiments for training diverse models (logistic regression, shallow CNNs, MobileNetV3, and EfficientNet-B0) on various  datasets (MNIST, FEMNIST, CIFAR-10, GLD-23K) in a realistic FL system with large number of users and demonstrate that LightSecAgg significantly reduces the total training time.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Learning Compressed Embeddings for On-Device Inference",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/72988287eb4acead9fe584bff6c488c5-Abstract.html",
        "paper_authors": [
            "Niketan Pansare",
            "Jay Katukuri",
            "Aditya Arora",
            "Frank Cipollone",
            "Riyaaz Shaik",
            "Noyan Tokgozoglu",
            "Chandru Venkataraman"
        ],
        "paper_abstract": "In deep learning, embeddings are widely used to represent categorical entities such as words, apps, and movies. An embedding layer maps each entity to a unique vector, causing the layer\u2019s memory requirement to be proportional to the number of entities. In the recommendation domain, a given category can have hundreds of thousands of entities, and its embedding layer can take gigabytes of memory. The scale of these networks makes them difficult to deploy in resource constrained environments, such as smartphones. In this paper, we propose a novel approach for reducing the size of an embedding table while still mapping each entity to its own unique embedding. Rather than maintaining the full embedding table, we construct each entity\u2019s embedding \u201con the fly\u201d using two separate embedding tables. The first table employs hashing to force multiple entities to share an embedding. The second table contains one trainable weight per entity, allowing the model to distinguish between entities sharing the same embedding. Since these two tables are trained jointly, the network is able to learn a unique embedding per entity, helping it maintain a discriminative capability similar to a model with an uncompressed embedding table. We call this approach MEmCom (Multi-Embedding Compression). We compare with state-of-the-art model compression techniques for multiple problem classes including classification and ranking using datasets from various domains. On four popular recommender system datasets, MEmCom had a 4% relative loss in nDCG while compressing the input embedding sizes of our recommendation models by 16x, 4x, 12x, and 40x. MEmCom outperforms the state-of-the-art model compression techniques, which achieved 16%, 6%, 10%, and 8% relative loss in nDCG at the respective compression ratios. Additionally, MEmCom is able to compress the RankNet ranking model by 32x on a dataset with millions of users\u2019 interactions with games while incurring only a 1% relative loss in nDCG.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Gyro Dropout: Maximizing Ensemble Effect in Neural Network Training",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/72aa1632b83c93a2f680dbb5235f1a83-Abstract.html",
        "paper_authors": [
            "Junyeol Lee",
            "Hyeongju Kim",
            "Hyungjun Oh",
            "Jaemin Kim",
            "Hongseok Jeung",
            "Yung-Kyun Noh",
            "Jiwon Seo"
        ],
        "paper_abstract": "This paper proposes gyro dropout, a variant of dropout that improves the efficiency of training neural net-works. Instead of randomly dropping out neurons in every training iteration, gyro dropout pre-selects and trains a fixed number of subnetworks. Because each subnetwork is more stably trained, they are more diversified and thus their ensemble achieves good generalization. We further propose block-wise gyro dropout, or simply block-wise dropout, which is a GPU-friendly variant of gyro dropout. Block-wise dropout partitions hidden neurons into a number of groups that should be dropped out together throughout learning; this makes it efficient to prune the corresponding warp executions on GPUs. We evaluate the two dropout methods with seven neural networks and ten public datasets. In our evaluation, gyro dropout improves the accuracy of trained models by up to 1.93%; gyro dropout consistently achieves higher accuracy than conventional dropout in all experiments. Moreover, block-wise dropout speeds up the training of neural networks by up to 29.8% with little to no accuracy loss. Ourimplementation of gyro dropout is publicly available at https://github.com/mlsys-seo/gyro-dropout.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "A Transferable Approach for Partitioning Machine Learning Models on Multi-Chip-Modules",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/74e22712c9b50a9b43b2ae54e225888e-Abstract.html",
        "paper_authors": [
            "Xinfeng Xie",
            "Prakash Prabhu",
            "Ulysse  Beaugnon",
            "Phitchaya Phothilimthana",
            "Sudip Roy",
            "Azalia Mirhoseini",
            "Eugene Brevdo",
            "James Laudon",
            "Yanqi Zhou"
        ],
        "paper_abstract": "Multi-Chip-Modules (MCMs) reduce the design and fabrication cost of machine learning (ML) accelerators while delivering performance and energy efficiency on par with a monolithic large chip. However, ML compilers targeting  MCMs need to solve complex optimization problems optimally and efficiently to achieve this high performance. One such problem is the multi-chip partitioning problem where compilers determine the optimal partitioning and placement of operations in tensor computation graphs on chiplets in MCMs. Partitioning ML graphs for MCMs is particularly hard as the search space grows exponentially with the number of chiplets available and the number of nodes in the neural network. Furthermore, the constraints imposed by the underlying hardware produce a search space where valid solutions are extremely sparse. In this paper, we present a strategy using a deep reinforcement learning (RL) framework to emit a possibly invalid candidate partition that is then corrected by a constraint solver. Using the constraint solver ensures that RL encounters valid solutions in the sparse space frequently enough to converge with fewer samples as compared to non-learned strategies. The graphical neural network and sequential attention mechanism in our RL framework enable the generalization across different ML graphs. Our evaluation of a production-scale model, BERT, on real hardware reveals that the partitioning generated using RL policy achieves 6.11% and 5.85% higher throughput than random search and simulated annealing. In addition, fine-tuning the pre-trained RL policy reduces the search time from 3 hours to only 9 minutes, while achieving the same throughput as training RL policy from scratch.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "On the Utility of Gradient Compression in Distributed Training Systems",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/773862fcc2e29f650d68960ba5bd1101-Abstract.html",
        "paper_authors": [
            "Saurabh Agarwal",
            "Hongyi Wang",
            "Shivaram Venkataraman",
            "Dimitris Papailiopoulos"
        ],
        "paper_abstract": "A rich body of prior work has highlighted the existence of communication bottlenecks in synchronous data-parallel training. To alleviate these bottlenecks, a long line of recent research proposes gradient and model compression methods.  In this work, we evaluate the efficacy of gradient compression methods and compare their scalability with optimized implementations of synchronous data-parallel SGD across more than 200 realistic distributed setups. Surprisingly, we observe that only in 6 cases out of more than 200, gradient compression methods provide speedup over optimized synchronous data-parallel training in the typical data-center setting. We conduct an extensive investigation to identify the root causes of this phenomenon, and offer a performance model that can be used to identify the benefits of gradient compression for a variety of system setups. Based on our analysis, we propose a list of desirable properties that gradient compression methods should satisfy, in order for them to provide meaningful utility.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "REX: Revisiting Budgeted Training with an Improved Schedule",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/77cdf4ffbd2afd02541e02533ec56820-Abstract.html",
        "paper_authors": [
            "John Chen",
            "Cameron Wolfe",
            "Tasos Kyrillidis"
        ],
        "paper_abstract": "Deep learning practitioners often operate on a computational and monetary budget. Thus, it is critical to design optimization algorithms that perform well under any budget. The linear learning rate schedule is considered the best budget-aware schedule, as it outperforms most other schedules in the low budget regime. On the other hand, learning rate schedules --such as the \\texttt{30-60-90} step schedule-- are known to achieve high performance when the model can be trained for many epochs. Yet, it is often not known a priori whether one's budget will be large or small; thus, the optimal choice of learning rate schedule is made on a case-by-case basis. In this paper, we frame the learning rate schedule selection problem as a combination of $i)$ selecting a profile (i.e., the continuous function that models the learning rate schedule), and $ii)$ choosing a sampling rate (i.e., how frequently the learning rate is updated/sampled from this profile). We propose a novel profile and sampling rate combination called the Reflected Exponential (REX) schedule, which we evaluate across seven different experimental settings with both SGD and Adam optimizers. REX outperforms the linear schedule in the low budget regime, while matching or exceeding the performance of several state-of-the-art learning rate schedules (linear, step, exponential, cosine, step decay on plateau, and OneCycle) in both high and low budget regimes. Furthermore, REX requires no added computation, storage, or hyperparameters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/7c47b303273905755d3e513ab43ef94f-Abstract.html",
        "paper_authors": [
            "Andrew Or",
            "Haoyu Zhang",
            "Michael None Freedman"
        ],
        "paper_abstract": "We propose VirtualFlow, a system leveraging a novel abstraction called virtual node processing to decouple the model from the hardware. In each step of training or inference, the batch of input data is split across virtual nodes instead of hardware accelerators (e.g., GPUs and TPUs). Mapping multiple virtual nodes to each accelerator and processing them sequentially effectively time slices the batch, thereby allowing users to reduce the memory requirements of their workloads and mimic large batch sizes on small clusters. Using this technique, VirtualFlow enables many new use cases, such as reproducing training results across different hardware, resource elasticity, and heterogeneous training. In our evaluation, our implementation of VirtualFlow for TensorFlow achieved strong convergence guarantees across different hardware with out-of-the-box hyperparameters, up to 48% lower job completion times with resource elasticity, and up to 42% higher throughput with heterogeneous training.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "torch.fx: Practical Program Capture and Transformation for Deep Learning in Python",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/7c98f9c7ab2df90911da23f9ce72ed6e-Abstract.html",
        "paper_authors": [
            "James Reed",
            "Zachary DeVito",
            "Horace He",
            "Ansley Ussery",
            "Jason Ansel"
        ],
        "paper_abstract": "Modern deep learning frameworks provide imperative, eager execution programming interfaces embedded in Python to provide a productive development experience. However, deep learning practitioners sometimes need to capture and transform program structure for performance optimization, visualization, analysis, and hardware integration. We study the different designs for program capture and transformation used in deep learning. By designing for typical deep learning use cases rather than long tail ones, it is possible to create a simpler framework for program capture and transformation. We apply this principle in torch.fx, a program capture and transformation library for PyTorch written entirely in Python and optimized for high developer productivity by ML practitioners. We present case studies showing how torch.fx enables workflows previously inaccessible in the PyTorch ecosystem.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "AccMPEG: Optimizing Video Encoding for Accurate Video Analytics",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/853f7b3615411c82a2ae439ab8c4c96e-Abstract.html",
        "paper_authors": [
            "Kuntai Du",
            "Qizheng Zhang",
            "Anton  Arapin",
            "Haodong Wang",
            "Zhengxu Xia",
            "Junchen Jiang"
        ],
        "paper_abstract": "With more videos being recorded by edge sensors (cameras) and analyzed by computer-vision deep neural nets (DNNs), a new breed of video streaming systems has emerged, with the goal to compress and stream videos to remote servers in real time while preserving enough information to allow highly accurate inference by the server-side DNNs. An ideal design of the video streaming system should simultaneously meet three key requirements: (1) low latency of encoding and streaming, (2) high accuracy of server-side DNNs, and (3) low compute overheads on the camera. Unfortunately, despite many recent efforts, such video streaming system has hitherto been elusive, especially when serving advanced vision tasks such as object detection or semantic segmentation.This paper presents AccMPEG, a new video encoding and streaming system that meets the three objectives. The key is to learn how much the encoding quality at each (16x16) macroblock can influence the server-side DNN accuracy, which we call accuracy gradients. Our insight is that these macroblock-level accuracy gradients can be inferred with sufficient precision by feeding the video frames through a cheap model. AccMPEG provides a suite of techniques that, given a new server-side DNN, can quickly create a cheap model to infer the accuracy gradients on any new frame in near realtime. Our extensive evaluation of AccMPEG on two types of edge devices (one Intel Xeon Silver 4100 CPU or NVIDIA Jetson Nano) and three vision tasks (six recent pre-trained DNNs) shows that compared to the state-of-the-art baselines, AccMPEG (with the same camera-side compute resources) can reduce the end-to-end inference delay by 10-43% without hurting accuracy.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "QuClassi: A Hybrid Deep Neural Network Architecture based on Quantum State Fidelity",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/928f1160e52192e3e0017fb63ab65391-Abstract.html",
        "paper_authors": [
            "Samuel A. Stein",
            "Betis Baheri",
            "Daniel Chen",
            "Ying Mao",
            "Qiang Guan",
            "Ang Li",
            "Shuai Xu",
            "Caiwen Ding"
        ],
        "paper_abstract": "In the past decade, remarkable progress has been achieved in deep learning related systems and applications. In the post Moore\u2019s Law era, however, the limit of semiconductor fabrication technology along with the increasing data size has slowed down the development of learning algorithms. In parallel, the rapid development of quantum computing has pushed it into a new era. Google illustrated quantum supremacy by completing a specific task (random sampling problem), in 200 seconds, which continues to be impracticable for the largest classical computers. Due to the exponential potential of quantum computing, quantum based learning is an area of interest, in hopes that certain systems might offer a quantum speedup. In this work, we propose a novel architecture QuClassi, a quantum neural network for both binary and multi-class classification. Powered by a quantum differentiation function along with a hybrid quantum-classic design, QuClassi encodes the data with a reduced number of qubits and generates the quantum circuit, pushing it to the quantum platform for the best states, iteratively. We conduct intensive experiments on both quantum simulators, IBM-Q\u2019s quantum platform as well as evaluate performance on IonQ. The evaluation results demonstrate that QuClassi is able to outperform the state-of-the-art quantum-based solutions, Tensorflow-Quantum and QuantumFlow by up to 53.75% and 203.00% for binary and multi-class classifications. When comparing to traditional deep neural networks, QuClassi achieves a comparable performance with 97.37% fewer parameters.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Graphiler: Optimizing Graph Neural Networks with Message Passing Data Flow Graph",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/a1126573153ad7e9f44ba80e99316482-Abstract.html",
        "paper_authors": [
            "Zhiqiang Xie",
            "Minjie Wang",
            "Zihao Ye",
            "Zheng Zhang",
            "Rui Fan"
        ],
        "paper_abstract": "Graph neural networks (GNNs) are a new class of powerful machine learning models, but easy programming and efficient computing is often at odds. Current GNN frameworks are based on a message passing paradigm, and allow the concise expression of GNN models using built-in primitives and user defined functions (UDFs). While built-in primitives offer high performance, they are limited in expressiveness; UDFs are flexible, but often have low performance and use excessive memory. In this paper, we propose Graphiler, a compiler stack for GNNs which achieves high performance while offering the flexibility of the UDF programming interface. At the core of Graphiler is a novel abstraction called Message Passing Data Flow Graph (MP-DFG), which enables optimizations that substantially reduce computational redundancy and memory footprint, and optimizes both homogeneous and heterogeneous GNNs under a unified framework. Experiments show Graphiler can accelerate UDF GNNs by up to two orders of magnitude, and achieve performance close to or superior to expert implementations, and do so with substantial memory savings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "MLPerf Mobile Inference Benchmark: An Industry-Standard Open-Source Machine Learning Benchmark for On-Device AI",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/a2b2702ea7e682c5ea2c20e8f71efb0c-Abstract.html",
        "paper_authors": [
            "Vijay Janapa Reddi",
            "David Kanter",
            "Peter Mattson",
            "Jared Duke",
            "Thai Nguyen",
            "Ramesh Chukka",
            "Ken Shiring",
            "Koan-Sin Tan",
            "Mark Charlebois",
            "William Chou",
            "Mostafa El-Khamy",
            "Jungwook Hong",
            "Tom St John",
            "Cindy Trinh",
            "Michael Buch",
            "Mark Mazumder",
            "Relja Markovic",
            "Thomas Atta",
            "Fatih Cakir",
            "Masoud Charkhabi",
            "Xiaodong Chen",
            "Cheng-Ming Chiang",
            "Dave Dexter",
            "Terry Heo",
            "Guenther Schmuelling",
            "Maryam Shabani",
            "Dylan Zika"
        ],
        "paper_abstract": "This paper presents the first industry-standard open-source machine learning (ML) benchmark to allow performance and accuracy evaluation of mobile devices with different AI chips and software stacks. The benchmark draws from the expertise of leading mobile-SoC vendors, ML-framework providers, and model producers. It comprises a suite of models that operate with standard data sets, quality metrics and run rules. We describe the design and implementation of this domain-specific ML benchmark. The current benchmark version comes as a mobile app for different computer vision and natural language processing tasks. The benchmark also supports non-smartphone devices, such as laptops and mobile PCs. Benchmark results from the first two rounds reveal the overwhelming complexity of the underlying mobile ML system stack, emphasizing the need for transparency in mobile ML performance analysis. The results also show that the strides being made all through the ML stack improve performance. Within six months, offline throughput improved by 3x, while latency reduced by as much as 12x. ML is an evolving field with changing use cases, models, data sets and quality targets. MLPerf Mobile will evolve and serve as an open-source community framework to guide research and innovation for mobile AI.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "PAPAYA: Practical, Private, and Scalable Federated Learning",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/a8bc4cb14a20f20d1f96188bd61eec87-Abstract.html",
        "paper_authors": [
            "Dzmitry Huba",
            "John Nguyen",
            "Kshitiz Malik",
            "Ruiyu Zhu",
            "Mike Rabbat",
            "Ashkan Yousefpour",
            "Carole-Jean Wu",
            "Hongyuan Zhan",
            "Pavel Ustinov",
            "Harish Srinivas",
            "Kaikai Wang",
            "Anthony Shoumikhin",
            "Jesik Min",
            "Mani Malek"
        ],
        "paper_abstract": "Cross-device Federated Learning (FL) is a distributed learning paradigm with several challenges that differentiate it from traditional distributed learning: variability in the system characteristics on each device, and millions of clients coordinating with a central server being primary ones. Most FL systems described in the literature are synchronous in nature --- they perform a synchronized aggregation of model updates from individual clients. Scaling synchronous FL is challenging since increasing the number of clients training in parallel leads to diminishing returns in training speed, analogous to large-batch training. Moreover, synchronous FL can be slow due to stragglers. In this work, we describe the design of a production asynchronous FL system to tackle the aforementioned issues, sketch some of the system design challenges and their solutions, and touch upon principles that emerged from building the production system for millions of clients. Empirically, we demonstrate that asynchronous FL is significantly faster than synchronous FL when training across millions of devices. In particular, in high concurrency settings, asynchronous FL converges 5$\\times$ faster while being nearly 8$\\times$ more resource efficient than synchronous FL.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "GPU Semiring Primitives for Sparse Neighborhood Methods",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/acd593d2db87a799a8d3da5a860c028e-Abstract.html",
        "paper_authors": [
            "Corey J. Nolet",
            "Divye Gala",
            "Edward Raff",
            "Joe Eaton",
            "Brad Rees",
            "John Zedlewski",
            "Tim Oates"
        ],
        "paper_abstract": "High-performance primitives for mathematical operations on sparse vectors must deal with the challenges of skewed degree distributions and limits on memory consumption that are typically not issues in dense operations. We demonstrate that a sparse semiring primitive can be flexible enough to support a wide range of critical distance measures while maintaining performance and memory efficiency on the GPU. We further show that this primitive is a foundational component for enabling many neighborhood-based information retrieval and machine learning algorithms to accept sparse input. To our knowledge, this is the first work aiming to unify the computation of several critical distance measures on the GPU under a single flexible design paradigm and we hope that it provides a good baseline for future research in this area. Our implementation is fully open source and publicly available as part of the RAFT library of GPU-accelerated machine learning primitives (https://github.com/rapidsai/raft).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Accelerating Training and Inference of Graph Neural Networks with Fast Sampling and Pipelining",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/afacc5db3e0e85b446e6c7727cd7dca5-Abstract.html",
        "paper_authors": [
            "Tim Kaler",
            "Nickolas Stathas",
            "Anne Ouyang",
            "Alexandros-Stavros Iliopoulos",
            "Tao Schardl",
            "Charles E. Leiserson",
            "Jie Chen"
        ],
        "paper_abstract": "Improving the training and inference performance of graph neural networks (GNNs) is faced with a challenge uncommon in general neural networks: creating mini-batches requires a lot of computation and data movement due to the exponential growth of multi-hop graph neighborhoods along network layers. Such a unique challenge gives rise to a diverse set of system design choices. We argue in favor of performing mini-batch training with neighborhood sampling in a distributed multi-GPU environment, under which we identify major performance bottlenecks hitherto under-explored by developers: mini-batch preparation and transfer. We present a sequence of improvements to mitigate these bottlenecks, including a performance-engineered neighborhood sampler, a shared-memory parallelization strategy, and the pipelining of batch transfer with GPU computation. We also conduct an empirical analysis that supports the use of sampling for inference, showing that test accuracies are not materially compromised. Such an observation unifies training and inference, simplifying model implementation. We report comprehensive experimental results with several benchmark data sets and GNN architectures, including a demonstration that, for the ogbn-papers100M data set, our system SALIENT achieves a speedup of 3x over a standard PyTorch-Geometric implementation with a single GPU and a further 8x parallel speedup with 16 GPUs. Therein, training a 3-layer GraphSAGE model with sampling fanout (15, 10, 5) takes 2.0 seconds per epoch and inference with fanout (20, 20, 20) takes 2.4 seconds, attaining test accuracy 64.58%.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "The CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal Padding",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/afe8a4577080504b8bec07bbe4b2b9cc-Abstract.html",
        "paper_authors": [
            "Pratik Fegade",
            "Tianqi Chen",
            "Phillip Gibbons",
            "Todd Mowry"
        ],
        "paper_abstract": "There is often variation in the shape and size of input data used for deep learning. In many cases, such data can be represented using tensors with non-uniform shapes, or ragged tensors. Due to limited and non-portable support for efficient execution on ragged tensors, current deep learning frameworks generally use techniques such as padding and masking to make the data shapes uniform and then offload the computations to optimized kernels for dense tensor algebra. Such techniques can, however, lead to a lot of wasted computation and therefore, a loss in performance. This paper presents CoRa, a tensor compiler that allows users to easily generate efficient code for ragged tensor operators targeting a wide range of CPUs and GPUs. Evaluating CoRa on a variety of operators on ragged tensors as well as on an encoder layer of the transformer model, we find that CoRa (i) performs competitively with hand-optimized implementations of the operators and the transformer encoder and (ii) achieves a 1.6 geomean speedup over PyTorch for the encoder on an Nvidia GPU and a 1.37 geomean speedup over TensorFlow for the multi-head attention module used in transformers on a 64-core ARM CPU.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "HALOS: Hashing Large Output Space for Cheap Inference",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/b059dd6da6b9a86180fbc32a799766cc-Abstract.html",
        "paper_authors": [
            "Zichang Liu",
            "Zhaozhuo Xu",
            "Alan  Ji",
            "Junyan  Zhang",
            "Jonathan  Li",
            "Beidi Chen",
            "Anshumali Shrivastava"
        ],
        "paper_abstract": "Efficient inference in large output space is an essential yet challenging task in large scale machine learning. Previous approaches reduce this problem to Approximate Maximum Inner Product Search (AMIPS), which is based on the observation that the prediction of a given model corresponds to the logit with the largest value. However, models are not perfect in accuracy, and the successful retrievals of the largest logit may not lead to the correct predictions. We argue that approximate MIPS approaches are sub-optimal because they are tailored for retrieving largest inner products class instead of retrieving the correct class. Moreover, the logits generated from neural networks with large output space lead to extra challenges for the AMIPS method to achieve a high recall rate within the computation budget of efficient inference. In this paper, we propose HALOS, which reduces inference into sub-linear computation by selectively activating a small set of output layer neurons that are likely to correspond to the correct classes rather than to yield the largest logit. Our extensive evaluations show that HALOS matches or even outperforms the accuracy of given models with 21x speed up and 87\\% energy reduction.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "dPRO: A Generic Performance Diagnosis and Optimization Toolkit for Expediting Distributed DNN Training",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/b422680f3db0986ddd7f8f126baaf0fa-Abstract.html",
        "paper_authors": [
            "Hanpeng Hu",
            "Chenyu Jiang",
            "Yuchen Zhong",
            "Yanghua Peng",
            "Chuan Wu",
            "Yibo Zhu",
            "Haibin Lin",
            "Chuanxiong Guo"
        ],
        "paper_abstract": "Distributed training using multiple devices (e.g., GPUs) has been widely adopted for learning DNN models over large datasets. However, the performance of large-scale distributed training tends to be far from linear speed-up in practice. Given the complexity of distributed systems, it is challenging to identify the root cause(s) of inefficiency and exercise effective performance optimizations when unexpected low training speed occurs. To date, there exists no software tool which diagnoses performance issues and helps expedite distributed DNN training, while the training can be run using different deep learning frameworks. This paper proposes dPRO, a toolkit that includes: (1) an efficient profiler that collects runtime traces of distributed DNN training across multiple frameworks, especially fine-grained communication traces, and constructs global data flow graphs including detailed communication operations for accurate replay; (2) an optimizer that effectively identifies performance bottlenecks and explores optimization strategies (from computation, communication, and memory aspects) for training acceleration. We implement dPRO on multiple deep learning frameworks (TensorFlow, MXNet) and representative communication schemes (AllReduce and Parameter Server). Extensive experiments show that dPRO predicts the performance of distributed training in various settings with < 5% errors in most cases and finds optimization strategies with up to 3.48x speed-up over the baselines.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/b559156047e50cf316207249d0b5a6c5-Abstract.html",
        "paper_authors": [
            "Hengrui Zhang",
            "Zhongming Yu",
            "Guohao Dai",
            "Guyue Huang",
            "Yufei Ding",
            "Yuan Xie",
            "Yu Wang"
        ],
        "paper_abstract": "Graph Neural Networks (GNNs) have been widely used in various domains, and GNNs with sophisticated computational graph lead to higher latency and larger memory consumption. Optimizing the GNN computational graph suffers from: (1) Redundant neural operator computation. The same data are propagated through the graph structure to perform the same neural operation multiple times in GNNs, leading to redundant computation which accounts for 92.4% of total operators. (2) Inconsistent thread mapping. Efficient thread mapping schemes for vertex-centric and edge-centric operators are different. This inconsistency prohibits operator fusion to reduce memory IO. (3) Excessive intermediate data. For GNN training which is usually performed concurrently with inference, intermediate data must be stored for the backward pass, consuming 91.9% of the total memory requirement.To tackle these challenges, we propose following designs to optimize the GNN computational graph from a novel coordinated computation, IO, and memory perspective: (1) Propagation-postponed operator reorganization. We reorganize operators to perform neural operations before the propagation, thus the redundant computation is eliminated. (2) Unified thread mapping for fusion. We propose a unified thread mapping scheme for both vertex- and edge-centric operators to enable fusion and reduce IO. (3) Intermediate data recomputation. Intermediate data are recomputed during the backward pass to reduce the total memory consumption. Extensive experimental results on three typical GNN models show that, we achieve up to 2.75x end-to-end speedup, 6.89x less memory IO, and 7.73x less memory consumption over state-of-the-art frameworks.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Bit-serial Weight Pools: Compression and Arbitrary Precision Execution of Neural Networks on Resource Constrained Processors",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/b6856f264cd38f1d8f01d7769be62444-Abstract.html",
        "paper_authors": [
            "Shurui Li",
            "Puneet Gupta"
        ],
        "paper_abstract": "Applications of neural networks on edge systems have proliferated in recent years but the ever increasing model size makes neural networks not able to deploy on resource-constrained microcontrollers efficiently. We propose bit-serial weight pools, an end-to-end framework that includes network compression and acceleration of arbitrary sub-byte precision. The framework can achieve up to 8x compression compared to 8-bit networks by sharing a pool of weights across the entire network. We further propose a bit-serial lookup based software implementation that allows runtime-bitwidth tradeoff and is able to achieve more than 2.8x speedup and 7.5x storage compression compared to 8-bit networks, with less than 1% accuracy drop.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Efficient Strong Scaling Through Burst Parallel Training",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/b99e69074b2fa1d8c8fe0d5b60e19397-Abstract.html",
        "paper_authors": [
            "Seo Jin Park",
            "Joshua Fried",
            "Sunghyun Kim",
            "Mohammad Alizadeh",
            "Adam Belay"
        ],
        "paper_abstract": "As emerging deep neural network (DNN) models continue to grow in size, using large GPU clusters to train DNNs is becoming an essential requirement to achieving acceptable training times. In this paper, we consider the case where future increases in cluster size will cause the global batch size that can be used to train models to reach a fundamental limit: beyond a certain point, larger global batch sizes cause sample efficiency to degrade, increasing overall time to accuracy. As a result, to achieve further improvements in training performance, we must instead consider \"strong scaling\" strategies that hold the global batch size constant and allocate smaller batches to each GPU. Unfortunately, this makes it significantly more difficult to use cluster resources efficiently. We present DeepPool, a system that addresses this efficiency challenge through two key ideas. First, burst parallelism allocates large numbers of GPUs to foreground jobs in bursts to exploit the unevenness in parallelism across layers. Second, GPU multiplexing prioritizes throughput for foreground training jobs, while packing in background training jobs to reclaim underutilized GPU resources, thereby improving cluster-wide utilization. Together, these two ideas enable DeepPool to deliver a 1.2 - 2.3x improvement in total cluster throughput over standard data parallelism with a single task when the cluster scale is large.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "SLA-Driven ML INFERENCE FRAMEWORK FOR CLOUDS WITH HETEROGENEOUS ACCELERATORS",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/bcf9bef61a534d0ce4a3c55f09dfcc29-Abstract.html",
        "paper_authors": [
            "Junguk Cho",
            "Diman Zad Tootaghaj",
            "Lianjie Cao",
            "Puneet Sharma"
        ],
        "paper_abstract": "The current design of Serverless computing frameworks assumes that all the requests and underlying compute hardware are homogeneous. This homogeneity assumption causes two challenges in running ML workloads like Deep Neural Network (DNN) inference services on these frameworks. Such workloads can have various request types and might require heterogeneous accelerators. First, existing serverless frameworks are threshold-based and use simple query per second or CPU utilization as autoscaling rules, thus ignoring heterogeneous requests and accelerators, resulting in sub-optimal performance. Second, ignoring infrastructure heterogeneity for workload scheduling and inference request distribution can lead to further performance inefficiencies. To address these challenges, we propose SLA-aware ML Inference Framework, which is a novel application and hardware-aware serverless computing framework to manage ML (\\eg, DNN) inference applications in a heterogeneous infrastructure. Our framework designs an intelligent autoscaling strategy by leveraging rich, precise workload-specific metrics and heterogeneous GPU compute capability. We schedule functions on the suitable GPU accelerators and proportionally distribute inference requests to the deployed functions based on the autoscaling decision. In addition, our framework enables efficient shares of GPU accelerators with multiple functions to increase resource efficiency with minimal overhead. Unlike prior works, we use application-specific SLA metrics to make scheduling/autoscaling decisions. We implement a prototype of our framework based on the Knative serverless framework and evaluate its performance with various DNN models.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "FROTE: Feedback Rule-Driven Oversampling for Editing Models",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/bea4cb243dfb49a6309c58cf5531a6c6-Abstract.html",
        "paper_authors": [
            "Oznur Alkan",
            "Dennis Wei",
            "Massimiliano Mattetti",
            "Rahul Nair",
            "Elizabeth Daly",
            "Diptikalyan Saha"
        ],
        "paper_abstract": "Machine learning (ML) models may involve decision boundaries that change over time due to updates to rules and regulations, such as in loan approvals or claims management. However, in such scenarios, it may take time for sufficient training data to accumulate in order to retrain the model to reflect the new decision boundaries.  While work has been done to reinforce existing decision boundaries, very little has been done to cover these scenarios where decision boundaries of the ML models should change in order to reflect new rules. In this paper, we focus on user-provided feedback rules as a way to expedite the ML models' update process, and we formally introduce the problem of pre-processing training data to edit an ML model in response to feedback rules such that once the model is retrained on the pre-processed data, its decision boundaries align more closely with the rules. To solve this problem, we propose a novel data augmentation method, the Feedback Rule-Based Oversampling Technique (FROTE). Extensive experiments using different ML models and  real world datasets demonstrate the effectiveness of the method, in particular the benefit of augmentation and the ability to handle many feedback rules.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ML-EXray: Visibility into ML Deployment on the Edge",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/c04f25be56ab86371563568dce31808f-Abstract.html",
        "paper_authors": [
            "Hang Qiu",
            "Ioanna Vavelidou",
            "Jian Li",
            "Evgenya Pergament",
            "Pete Warden",
            "Sandeep Chinchali",
            "Zain Asgar",
            "Sachin Katti"
        ],
        "paper_abstract": "Benefited from expanding cloud infrastructure, today's neural networks have increasingly high performance trained on the cloud.  Model researchers spent months of sweat competing for an extra few percent of model accuracy. However, when these models are actually deployed on edge devices in practice, very often, the performance is dropping over 10% all of a sudden without obvious reasons. The key challenge is that there is not much visibility to ML inference execution on edge devices, and very little awareness of potential issues during the edge deployment process. ML-EXray provides visibility into layer-level details of the ML execution, helps developers analyze and debug cloud-to-edge deployment issues. More often than not, the reason does not only lie in the model itself, but every operation throughout the data flow and the deployment process. Evaluations show that ML-EXray can effectively catch deployment issues, such as pre-processing bugs, quantization issues, suboptimal kernels; using ML-EXray, users need to write less than 15 line of code to fully examine the edge deployment pipeline; eradicating these issues, ML-EXray can correct model performance by up to 30%, pinpoint error-prune layers, guide users to optimize kernel execution latency by two orders of magnitude. Code and APIs will be released as a multi-lingual instrumentation library and a Python deployment validation library.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "TorchSparse: Efficient Point Cloud Inference Engine",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/c48e820389ae2420c1ad9d5856e1e41c-Abstract.html",
        "paper_authors": [
            "Haotian Tang",
            "Zhijian Liu",
            "Xiuyu Li",
            "Yujun Lin",
            "Song Han"
        ],
        "paper_abstract": "Deep learning on point clouds has received increased attention thanks to its wide applications in AR/VR and autonomous driving. These applications require low latency and high accuracy to provide real-time user experience and ensure user safety. Unlike conventional dense workloads, the sparse and irregular nature of point clouds poses severe challenges to running sparse CNNs efficiently on the general-purpose hardware, and existing sparse acceleration techniques for 2D images do not translate to 3D point clouds. In this paper, we introduce TorchSparse, a high-performance point cloud inference engine that accelerates the sparse convolution computation on GPUs. TorchSparse directly optimizes the two bottlenecks of sparse convolution: irregular computation and data movement. It adopts adaptive MM grouping to trade computation for better regularity, achieving 1.4-1.5x speedup for matrix multiplication. It also optimizes the data movement by adopting vectorized, quantized and fused locality-aware memory access, reducing the memory movement cost by 2.7x. Evaluated on seven representative models across three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Improving Model Training with Multi-fidelity Hyperparameter Evaluation",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/c4d41d9619462c534b7b61d1f772385e-Abstract.html",
        "paper_authors": [
            "Yimin Huang",
            "Yujun Li",
            "Hanrong Ye",
            "Zhenguo Li",
            "Zhihua Zhang"
        ],
        "paper_abstract": "The evaluation of hyperparameters, neural architectures, or data augmentation policies becomes a critical problem in advanced deep model training with a large hyperparameter search space. In this paper, we propose an efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the scenario of hyperparameter search evaluation and its modified version for high GPU usage. It evaluates the potential of hyperparameters by the sub-samples of observations and is theoretically proved to be optimal under the criterion of cumulative regret. We further combine SS with Bayesian Optimization and develop a novel hyperparameter optimization algorithm called BOSS. Empirical studies validate our theoretical arguments of SS and demonstrate the superior performance of BOSS on a number of applications, including Neural Architecture Search (NAS), Data Augmentation (DA), Object Detection (OD), and Reinforcement Learning (RL).",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Plumber: Diagnosing and Removing Performance Bottlenecks in Machine Learning Data Pipelines",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/d0e90e9a9310570dfa643aa3b2da6e89-Abstract.html",
        "paper_authors": [
            "Michael Kuchnik",
            "Ana Klimovic",
            "Jiri Simsa",
            "Virginia Smith",
            "George Amvrosiadis"
        ],
        "paper_abstract": "Input pipelines, which ingest and transform input data, are an essential part of training Machine Learning (ML) models. However, it is challenging to implement efficient input pipelines, as it requires reasoning about parallelism, asynchrony, and variability in fine-grained profiling information. Our analysis of over two million ML jobs in Google datacenters reveals that a significant fraction of model training jobs could benefit from faster input data pipelines. At the same time, our analysis indicates that most jobs do not saturate host hardware, pointing in the direction of software-based bottlenecks. Motivated by these findings, we propose Plumber, a tool for finding bottlenecks in ML input pipelines. Plumber uses an extensible and interpretable operational analysis analytical model to automatically tune parallelism, prefetching, and caching under host resource constraints. Across five representative ML pipelines, Plumber obtains speedups of up to 47x for misconfigured pipelines. By automating caching, Plumber obtains end-to-end speedups of over 50% compared to state-of-the-art tuners.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "mmSampler: Efficient Frame Sampler for Multimodal Video Retrieval",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/d59a1dc497cf2773637256f50f492723-Abstract.html",
        "paper_authors": [
            "Zhiming Hu",
            "Ning Ye",
            "Iqbal Mohomed"
        ],
        "paper_abstract": "We study the problem of natural language-based video retrieval, the task of finding relevant videos given natural language search queries. Most recent state-of-the-art (SOTA) approaches would embed the video and query separately and map the video and query embeddings into a joint latent space to calculate a similarity score between them. To learn a video representation, existing solutions generally use all the frames or sample a subset of frames from the video using uniform sampling. The former solution could be computationally prohibitive while the latter may inject noise from uninformative frames into the final video representation. To this end, we propose mmSampler, a learning-based sampler, to adaptively select salient frames to represent the videos for multimodal video retrieval. mmSampler can greatly reduce the computational overhead for video representation without affecting the retrieval performance. We learn a lightweight policy network to decide whether to further process or discard a frame. By adopting the Gumbel-Softmax trick, we train the sampler jointly with the video retrieval model end-to-end in an efficient manner. Experimental results on benchmark datasets such as ActivityNet, DiDeMo and MSRVTT demonstrate that mmSampler achieves improved retrieval performance while saving as much as 43% GFLOPs per video.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "ULPPACK: Fast Sub-8-bit Matrix Multiply on Commodity SIMD Hardware",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/e09d45e14e9ece7142217550ddd3c4d0-Abstract.html",
        "paper_authors": [
            "Jaeyeon Won",
            "Jeyeon Si",
            "Sam Son",
            "Tae Jun Ham",
            "Jae W. Lee"
        ],
        "paper_abstract": "Recent progress in quantization techniques has demonstrated the feasibility of sub-8-bit quantization with a negligible end-to-end accuracy drop. However, today\u2019s commodity hardware such as CPUs and GPUs is still suboptimal in executing these sub-8-bit quantized networks as its SIMD instructions only support the granularity of 8 bits or wider. This paper presents ULPPACK, a software technique to accelerate those ultra low-precision networks via effective operand packing. The key idea of ULPPACK is to pack multiple low-precision (<8 bits) operands densely into a single wide (16 bits) register and perform multiple narrow multiply-accumulate (MAC) operations with a single wide multiply. We introduce two effective packing schemes with different tradeoffs as well as optimizations to amortize the overhead of shifting and masking the output partial sum. Our evaluation of ULPPACK with a 512x512x512 GEMM kernel demonstrates substantial performance gains over state-of-the-art low-precision linear algebra libraries with a speedup of 2.1x, 1.8x, and 2.7x\u2002for 3-bit weights/activations (W3A3) over Google\u2019s GEMMLOWP, Facebook\u2019s QNNPACK, and an optimized bit-serial implementation, respectively. For end-to-end evaluation on PyTorch with seven 3-bit quantized convolutional neural networks (CNNs), ULPPACK achieves geomean speedups of 3.9x and 1.5x over the baseline 32-bit floating-point (FP32) and QNNPACK, respectively.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Apollo: Automatic Partition-based Operator Fusion through Layer by Layer Optimization",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/e175e8a86d28d935be4f43719651f86d-Abstract.html",
        "paper_authors": [
            "Jie Zhao",
            "Xiong Gao",
            "Ruijie Xia",
            "Zhaochuang Zhang",
            "Deshi Chen",
            "Lei  Chen",
            "Renwei Zhang",
            "Zhen Geng",
            "Bin Cheng",
            "Xuefeng Jin"
        ],
        "paper_abstract": "We study fusion for deep neural networks (DNNs) in a just-in-time (JIT) compilation framework Apollo. It considers both memory- and compute-bound tensor operators for fusion, and integrates graph-level node grouping and operator-level loop fusion closely, widening the fusion search space. Apollo enables the upward feedback from the downstream loop optimizer, enforcing the graph engine to regenerate partition patterns amenable to the downstream pass and thus resolving the scalability issue. Besides data locality, Apollo also exploits the parallelism between independent tensor operators, further improving the performance of DNN workloads. Experimental results on training workloads show that Apollo outperforms TensorFlow and XLA by 1.86\u00d7 and 1.37\u00d7 on a single GPU, and 1.96\u00d7 and 1.18\u00d7 on multiple GPUs. Apollo also improves the performance of a vendor-provided DNN framework by 19.7% on a domain-specific accelerator. In addition, the results of inference workloads demonstrate the general applicability of our fusion framework.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/f0f9e98bc2e2f0abc3e315eaa0d808fc-Abstract.html",
        "paper_authors": [
            "Ningning Xie",
            "Tamara Norman",
            "Dominik Grewe",
            "Dimitrios Vytiniotis"
        ],
        "paper_abstract": "We present a novel characterization of the mapping of multiple parallelism forms(e.g. data and model parallelism) onto hierarchical accelerator systems that ishierarchy-aware and greatly reduces the space of software-to-hardware mapping.We experimentally verify the substantial effect of these mappings on all-reduceperformance (up to 448x). We offer a novel syntax-guided programsynthesis framework that is able to decompose reductions over one or moreparallelism axes to sequences of collectives in a hierarchy- and mapping-awareway. For 69% of parallelism placements and user requested reductions, ourframework synthesizes programs that outperform the default all-reduceimplementation when evaluated on different GPU hierarchies (max 2.04x,average 1.27x). We complement our synthesis tool with a simulatorexceeding 90% top-10 accuracy, which therefore reduces the need for massiveevaluations of synthesis results to determine a small set of optimal programsand mappings.",
        "paper_code": "#",
        "paper_cite": -1
    },
    {
        "paper_name": "DietCode: Automatic Optimization for Dynamic Tensor Programs",
        "paper_url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/f89b79c9a28d4cae22ef9e557d9fa191-Abstract.html",
        "paper_authors": [
            "Bojian Zheng",
            "Ziheng Jiang",
            "Cody Hao Yu",
            "Haichen Shen",
            "Joshua Fromm",
            "Yizhi Liu",
            "Yida Wang",
            "Luis Ceze",
            "Tianqi Chen",
            "Gennady Pekhimenko"
        ],
        "paper_abstract": "Achieving high performance for compute-intensive operators in machine learning (ML) workloads is a crucial but challenging task. Many ML and system practitioners rely on vendor libraries or auto-schedulers to do the job. While the former requires large engineering efforts, the latter only supports static-shape workloads in existing works. It is difficult, if not impractical, to apply existing auto-schedulers directly to dynamic-shape workloads, as this leads to extremely long auto-scheduling time.We observe that the key challenge faced by existing auto-schedulers when handling a dynamic-shape workload is that they cannot construct a unified search space for all the possible shapes of the workload, because their search space is shape-dependent. To address this, we propose DietCode, a new auto-scheduler framework that efficiently supports dynamic-shape workloads by constructing a shape-generic search space and cost model. Under this construction, all shapes jointly search within the same space and update the same cost model when auto-scheduling, which is therefore more efficient compared with existing auto-schedulers.We evaluate DietCode using state-of-the-art machine learning workloads on a modern GPU. Our evaluation shows that DietCode has the following key strengths when auto-scheduling an entire model end-to-end: (1) reduces the auto-scheduling time by up to 5.88x less than the state-of-the-art auto-scheduler on the uniformly sampled dynamic shapes (94.1x estimated if all possible shapes are included), (2) improves performance by up to 69.5% better than the auto-scheduler and 18.6% better than the vendor library. All these advantages make DietCode an efficient and practical solution for dynamic-shape workloads.",
        "paper_code": "#",
        "paper_cite": -1
    }
]